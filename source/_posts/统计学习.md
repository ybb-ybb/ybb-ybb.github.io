---
date: 2020-05-1 15:13:00
description: 统计学习
title: 统计学习
author: 鱼摆摆
comments: true
tags: 
 - 统计学习
photos: https://w.wallhaven.cc/full/ym/wallhaven-ympdrg.png
categories: 学习
---



# 基本概念

假设空间可定义为条件概率的集合
$$
\mathcal{F}=\{P|P(Y|X)\}
$$
通常是由一个参数向量决定的条件概率分布族
$$
\mathcal{F}=\left\{P\left|P_{\theta}(Y | X), \theta \in \mathbf{R}^{n}\right\}\right.
$$


风险函数(risk function)/期望损失(expected loss):
$$
R_{\exp }(f)=E_{P}[L(Y, f(X))]=\int_{x \times y} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y
$$

学习的目标是选择期望风险最小的模型，由于联合分布$P(X,Y)$ 未知，$R_{\exp}(f)$ 不能直接计算。所以监督学习是一个病态问题(ill-formed problem)。

经验风险：
$$
R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$


正则化:防止过拟合
$$
\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{t=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)
$$
交叉验证：一般选用S折交叉验证。

首先随机地将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型.

# 泛化误差

利用学习到的模型$\hat{f}$ ，对未知数据预测的误差:
$$
R_{\exp }(\hat{f})=E_{P}[L(Y, \hat{f}(X))]=\int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) P(x, y) \mathrm{d} x \mathrm{d} y
$$
泛化误差上界通常具有以下性质：它是样本容量的函数，当样本容量增加时，泛化上界趋于0；它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

**定理：**对于二分类问题，关于$f$ 的期望风险和经验风险分别是
$$
R(f)=E[L(Y, f(X))]
$$

$$
\hat{R}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$
当假设空间是有限个函数的集合$\mathcal{F}=\{f_1,f_2,\cdots,f_d\}$ 时,对任意一个函数$f \in \mathcal{F}$ ，至少以概率$1-\delta$ ,以下不等式成立：

$$
R(f) \leqslant \hat{R}(f)+\varepsilon(d, N, \delta)
$$

其中
$$
\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}
$$

不等式左端$R(f)$是泛化误差，右端即为泛化误差上界。在泛化误差上界中，第一项是训练误差，训练误差越小，泛化误差也越小，第2项是$N$的单调递减函数，$\log d$的单调递增函数，假设空间F包含的函数越多，其值越大。

**证明：**

证明中需要用到 Hoeffding 不等式：

设$S_n=\sum_{i=1}^{n}X_i$ 是独立随机变量$X_1,X_2,\cdots,X_n$ 之和，$X_i \in [a_i,b_i]$ ，则对任意$t>0$ ,以下不等式成立：
$$
P\left(S_{n}-E S_{n} \geqslant t\right) \leqslant \exp \left(\frac{-2 t^{2}}{\sum_{i=1}^{n}\left(b_{i}-a_{i}\right)^{2}}\right)
$$

$$
P\left(E S_{n}-S_{n} \geqslant t\right) \leqslant \exp \left(\frac{-2 t^{2}}{\sum_{i=1}^{n}\left(b_{i}-a_{i}\right)^{2}}\right)
$$

证明：对任意函数$f\in \mathcal{F}$， $\hat{R}(f)$ 是N个独立随机变量$L(Y,f(X))$ 的样本取值，$R(f)$ 是随机变量$L(Y,f(X))$ 的期望值。如果损失函数取值于区间$[0,1]$ ，即对所有$i$ ,$[a_i,b_i]=[0,1]$， 那么由Hoeffding不等式：对$\varepsilon > 0$，以下不等式成立：
$$
P(R(f)-\hat{R}(f) \geqslant \varepsilon) \leqslant \exp \left(-2 N \varepsilon^{2}\right)
$$

$\mathcal{F}=\left\{f_{1}, f_{2}, \cdots, f_{d}\right\}$ 是一个有限集合：
$$
\begin{aligned}
P(\exists f \in \mathcal{F}: R(f)-\hat{R}(f) \geqslant \varepsilon) &=P\left(\bigcup_{\mathscr{G}^{\prime} F}\{R(f)-\hat{R}(f) \geqslant \varepsilon\}\right) \\
& \leqslant \sum_{f \in \mathcal{F}} P(R(f)-\hat{R}(f) \geqslant \varepsilon) \\
& \leqslant d \exp \left(-2 N \varepsilon^{2}\right)
\end{aligned}
$$
即
$$
P(R(f)-\hat{R}(f)<\varepsilon) \geqslant 1-d \exp \left(-2 N \varepsilon^{2}\right)
$$
令$\delta=d \exp \left(-2 N \varepsilon^{2}\right)$
$$
P(R(f)<\hat{R}(f)+\varepsilon) \geqslant 1-\delta
$$

# 二分类问题：

许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络，Winnow等。

- TP——将正类预测为正类数；

- FN——将正类预测为负类数；

- FP——将负类预测为正类数；
- TN——将负类预测为负类数.

精确率：
$$
P=\frac{TP}{TP+FP}
$$
召回率：
$$
R=\frac{TP}{TP+FN}
$$
$F_1$ 值：
$$
\frac{2}{F_{1}}=\frac{1}{P}+\frac{1}{R}
$$
$$
F_{1}=\frac{2 T P}{2 T P+F P+F N}
$$