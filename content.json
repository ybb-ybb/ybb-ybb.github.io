{"meta":{"title":"鱼摆摆的blog","subtitle":null,"description":"自童年起，我就独自一人，照顾着历代星辰","author":"鱼摆摆","url":""},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"client/index.html","permalink":"/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"comment/index.html","permalink":"/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"donate/index.html","permalink":"/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"lab/index.html","permalink":"/lab/index.html","excerpt":"","text":"sakura主题 balabala","keywords":"Lab实验室"},{"title":"music","date":"2020-01-11T15:14:28.000Z","updated":"2020-03-09T12:35:33.243Z","comments":false,"path":"music/index.html","permalink":"/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-03-09T12:29:18.823Z","comments":true,"path":"links/index.html","permalink":"/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"tensorboardX","slug":"tensorboardX","date":"2020-03-09T13:31:11.839Z","updated":"2020-03-09T13:35:02.659Z","comments":true,"path":"2020/03/09/tensorboardX/","link":"","permalink":"/2020/03/09/tensorboardX/","excerpt":"","text":"创建 TensorBoardX的GitHub地址：传送门 首先创建一个 SummaryWriter 的示例 ： from tensorboardX import SummaryWriter # Creates writer1 object. # The log will be saved in 'runs/exp' writer1 = SummaryWriter('runs/exp') # Creates writer2 object with auto generated file name # The log directory will be something like 'runs/Aug20-17-20-33' writer2 = SummaryWriter() # Creates writer3 object with auto generated file name, the comment will be appended to the filename. # The log directory will be something like 'runs/Aug20-17-20-33-resnet' writer3 = SummaryWriter(comment='resnet') 以上展示了三种初始化 SummaryWriter 的方法： 提供一个路径，将使用该路径来保存日志 无参数，默认将使用 runs/日期时间 路径来保存日志 提供一个 comment 参数，将使用 runs/日期时间-comment 路径来保存日志 在浏览器中查看这些可视化数据： tensorboard --logdir=&lt;your_log_dir&gt; 用各种add方法记录数据 数字(scalar) add_scalar(tag, scalar_value, global_step=None, walltime=None) 参数: tag (string): 数据名称，不同名称的数据使用不同曲线展示 scalar_value (float): 数字常量值 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() 需要注意，这里的 scalar_value 一定是 float 类型，如果是 PyTorch scalar tensor，则需要调用 .item() 方法获取其数值。我们一般会使用 add_scalar 方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，直观地监控训练过程。 示例： from tensorboardX import SummaryWriter writer = SummaryWriter('runs/scalar_example') for i in range(10): writer.add_scalar('quadratic', i**2, global_step=i) writer.add_scalar('exponential', 2**i, global_step=i) 图片(image) 需要pillow库的支持 用add_image记录单个图像数据，用add_images记录多个图像数据 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') CHW为channel*hight*width 示例： from tensorboardX import SummaryWriter import cv2 as cv writer = SummaryWriter('runs/image_example') for i in range(1, 6): writer.add_image('countdown', cv.cvtColor(cv.imread('{}.jpg'.format(i)), cv.COLOR_BGR2RGB), global_step=i, dataformats='HWC') 直方图(histogram) add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 示例： from tensorboardX import SummaryWriter import numpy as np writer = SummaryWriter('runs/embedding_example') writer.add_histogram('normal_centered', np.random.normal(0, 1, 1000), global_step=1) writer.add_histogram('normal_centered', np.random.normal(0, 2, 1000), global_step=50) writer.add_histogram('normal_centered', np.random.normal(0, 3, 1000), global_step=100) &quot;DISTRIBUTIONS&quot;和&quot;HISTOGRAMS&quot;两栏都是用来观察数据分布的。其中在&quot;HISTOGRAMS&quot;中，同一数据不同 step 时候的直方图可以上下错位排布 (OFFSET) 也可重叠排布 (OVERLAY)。 运行图(graph) add_graph(model, input_to_model=None, verbose=False, **kwargs) 可以可视化神经网络的结构，参考Github官方样例 嵌入张量(embedding) 使用 add_embedding 方法可以在二维或三维空间可视化 embedding 向量。 add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) 参数： mat (torch.Tensor or numpy.array): 一个MxN矩阵，每行代表特征空间的一个数据点 metadata (list or torch.Tensor or numpy.array, optional): 一个一维列表N，mat 中每行数据的 label，大小应和 mat 行数相同 label_img (torch.Tensor, optional): 一个形如 NxCxHxW 的张量，对应 mat 每一行数据显示出的图像，N 应和 mat 行数相同 global_step (int, optional): 训练的 step tag (string, optional): 数据名称，不同名称的数据将分别展示 示例： from tensorboardX import SummaryWriter import torchvision writer = SummaryWriter('runs/embedding_example') mnist = torchvision.datasets.MNIST('mnist', download=True) writer.add_embedding( mnist.train_data.reshape((-1, 28 * 28))[:100,:], #直接将mnist前100个数据展开成一维向量作为embedding metadata=mnist.train_labels[:100], #每个embedding的label label_img = mnist.train_data[:100,:,:].reshape((-1, 1, 28, 28)).float() / 255, #每个图像 global_step=0 )","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2020-03-09T12:16:01.000Z","updated":"2020-03-09T13:38:13.627Z","comments":true,"path":"2020/03/09/Hexo-Theme-Sakura/","link":"","permalink":"/2020/03/09/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakura主题 English document 基于WordPress主题Sakura修改成Hexo的主题。 demo预览 正在开发中… 交流群 若你是使用者，加群QQ: 801511924 若你是创作者，加群QQ: 194472590 主题特性 首页大屏视频 首页随机封面 图片懒加载 valine评论 fancy-box相册 pjax支持，音乐不间断 aplayer音乐播放器 多级导航菜单（按现在大部分hexo主题来说，这也算是个特性了） 赞赏作者 如果喜欢hexo-theme-sakura主题，可以考虑资助一下哦~非常感激！ paypal | Alipay 支付宝 | WeChat Pay 微信支付 未完善的使用教程 那啥？老实说我目前也不是很有条理233333333~ 1、主题下载安装 hexo-theme-sakura建议下载压缩包格式，因为除了主题内容还有些source的配置对新手来说比较太麻烦，直接下载解压就省去这些麻烦咯。 下载好后解压到博客根目录（不是主题目录哦，重复的选择替换）。接着在命令行（cmd、bash）运行npm i安装依赖。 2、主题配置 博客根目录下的_config配置 站点 # Site title: 你的站点名 subtitle: description: 站点简介 keywords: author: 作者名 language: zh-cn timezone: 部署 deploy: type: git repo: github: 你的github仓库地址 # coding: 你的coding仓库地址 branch: master 备份 （使用hexo b发布备份到远程仓库） backup: type: git message: backup my blog of https://honjun.github.io/ repository: # 你的github仓库地址,备份分支名 （建议新建backup分支） github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup 主题目录下的_config配置 其中标明【改】的是需要修改部门，标明【选】是可改可不改，标明【非】是不用改的部分 # site name # 站点名 【改】 prefixName: さくら荘その siteName: hojun # favicon and site master avatar # 站点的favicon和头像 输入图片路径（下面的配置是都是cdn的相对路径，没有cdn请填写完整路径，建议使用jsdeliver搭建一个cdn啦，先去下载我的cdn替换下图片就行了，简单方便~）【改】 favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # 站点url 【改】 url: https://sakura.hojun.cn # 站点介绍（或者说是个人签名）【改】 description: Live your life with passion! With some drive! # 站点cdn，没有就为空 【改】 若是cdn为空，一些图片地址就要填完整地址了，比如之前avatar就要填https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # 开启pjax 【选】 pjax: 1 # 站点首页的公告信息 【改】 notice: hexo-Sakura主题已经开源，目前正在开发中... # 懒加载的加载中图片 【选】 lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # 站点菜单配置 【选】 menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer first，1: older first. 【非】 homePageSortType: -1 # Home page article shown number) 【非】 homeArticleShown: 10 # 背景图片 【选】 bgn: 8 # startdash面板 url, title, desc img 【改】 startdash: - {url: /theme-sakura/, title: Sakura, desc: 本站 hexo 主题, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: 博主的b站视频, img: /img/startdash/bilibili.jpg} - {url: /, title: hojun的万事屋, desc: 技术服务, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # 你的站点建立日期 【改】 siteBuildingTime: 07/17/2018 # 社交按钮(social) url, img PC端配置 【改】 social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # 社交按钮(msocial) url, img 移动端配置 【改】 msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # 赞赏二维码（其中wechatSQ是赞赏单页面的赞赏码图片）【改】 donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # 首页视频地址为https://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4，配置如下 【改】 movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # 多个视频用逗号隔开，随机获取。支持的格式目前已知MP4,Flv。其他的可以试下，不保证有用 name: Unbroken.mp4 # 左下角aplayer播放器配置 主要改id和server这两项，修改详见[aplayer文档] 【改】 aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valine评论配置【改】 valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ 分类页和标签页配置 分类页 标签页 配置项在\\themes\\Sakura\\languages\\zh-cn.yml里。新增一个分类或标签最好加下哦，当然嫌麻烦可以直接使用一张默认图片（可以改主题或者直接把404图片替换下，征求下意见要不要给这个在配置文件中加个开关，可以issue或群里提出来），现在是没设置的话会使用那种倒立小狗404哦。 #category # 按分类名创建 技术: #中文标题 zh: 野生技术协会 # 英文标题 en: Geek – Only for Love # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg 生活: zh: 生活 en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # 标签名即是标题 悦读: # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg 单页面封面配置 如留言板页面页面，位于source下的comment下，打开index.md如下： --- title: comment date: 2018-12-20 23:13:48 keywords: 留言板 description: comments: true # 在这里配置单页面头部图片，自定义替换哦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- 单页面配置 番组计划页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # 番组图片 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # 番组名 title: 朝花夕誓——于离别之朝束起约定之花 # 追番状态 （追番ing/已追完） status: 已追完 # 追番进度 progress: 100 # 番剧日文名称 jp: さよならの朝に約束の花をかざろう # 放送时间 time: 放送时间: 2018-02-24 SUN. # 番剧介绍 desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: 朝花夕誓——于离别之朝束起约定之花 status: 已追完 progress: 50 jp: さよならの朝に約束の花をかざろう time: 放送时间: 2018-02-24 SUN. desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 --- 友链页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: links title: links # 创建日期，可以改下 date: 2018-12-19 23:11:06 # 图片上的标题，自定义修改 keywords: 友人帐 description: # true/false 开启/关闭评论 comments: true # 页面头部图片，自定义修改 photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # 友链配置 links: # 类型分组 - group: 个人项目 # 类型简介 desc: 充分说明这家伙是条咸鱼 &lt; (￣︶￣)&gt; items: # 友链链接 - url: https://shino.cc/fgvf # 友链头像 img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # 友链站点名 name: Google # 友链介绍 下面雷同 desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 # 类型分组... - group: 小伙伴们 desc: 欢迎交换友链 ꉂ(ˊᗜˋ) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 --- 写文章配置 主题集成了个人插件hexo-tag-bili和hexo-tag-fancybox_img。其中hexo-tag-bili用来在文章或单页面中插入B站外链视频，使用语法如下： 详细使用教程详见hexo-tag-bili。 hexo-tag-fancybox_img用来在文章或单页面中图片，使用语法如下： 详细使用教程详见hexo-tag-fancybox_img 还有啥，一时想不起来… To be continued…","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"web","slug":"web","permalink":"/tags/web/"},{"name":"悦读","slug":"悦读","permalink":"/tags/悦读/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]},{"title":"neural ode","slug":"Neural ODE","date":"2020-03-04T03:38:00.000Z","updated":"2020-03-09T13:33:54.527Z","comments":true,"path":"2020/03/04/Neural ODE/","link":"","permalink":"/2020/03/04/Neural ODE/","excerpt":"","text":"原文：Neural ordinary differential equations NIPS2018最佳论文 简介 简单复习一下ResNet： 通过残差块解决反向传播过程中的梯度消失问题 ResNet、RNN、Normalizing flow 等模型都是这种形式： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) 如果采用更多的层数和更小的步长，可以优化为一个常微分方程： dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta) dtdh(t)​=f(h(t),t,θ) 这就是ODE Net的核心idea了……下面进行具体的分析 给定常微分方程，数学理论上可以对其进行解析法求解，但通常我们只关心数值解：在已知h(t0)h(t_0)h(t0​) 的情况下，求出h(t1)h(t_1)h(t1​) 。这在神经网络里对应的是正向传播。用ResNet对比一下： ResNet的正向传播： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) ODE网络的正向传播： dh(t)dt=f(h(t),t,θ)∫t0t1dh(t)=∫t0t1f(h(t),t,θ)dth(t1)=h(t0)+∫t0t1f(h(t),t,θ)dt\\begin{array}{c} \\frac{d h(t)}{d t}=f(h(t), t, \\theta) \\\\ \\int_{t_{0}}^{t_{1}} d h(t)=\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\\\ h\\left(t_{1}\\right)=h\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\end{array} dtdh(t)​=f(h(t),t,θ)∫t0​t1​​dh(t)=∫t0​t1​​f(h(t),t,θ)dth(t1​)=h(t0​)+∫t0​t1​​f(h(t),t,θ)dt​ 求解这个常微分方程数值解的方法有很多，最原始的是欧拉法：固定Δt\\Delta tΔt ,通过逐步迭代来求解： h(t+Δt)=h(t)+Δt∗f(h(t),t,θ)h(t+\\Delta t)=h(t)+\\Delta t * f(h(t),t,\\theta) h(t+Δt)=h(t)+Δt∗f(h(t),t,θ) 我们看到，如果令Δt=1\\Delta t=1Δt=1 ,离散化的欧拉法就退化成残差模块的表达式，也就是说ResNet可以看成是ODENet的特殊情况。 但欧拉法只是解常微分方程最基础的解法，它每走一步都会产生误差，并且误差会层层累积起来。近百年来，在数学和物理学领域已经有更成熟的ODE Solve方法，它们不仅能保证收敛到真实解，而且能够控制误差，本文在不涉及ODE Solve内部结构的前提下(将ODE Solve作为一个黑盒来使用)，研究如何用ODE Solve帮助机器学习。 这篇文章使用了一种适应性的ODE solver，它不像ResNet那样固定步长，而是根据给定的误差容忍度自动调整步长，黑色的评估位置可以视作神经元，他的位置也会根据误差容忍度自动调整： 使用ODENet的几个好处（和原文不完全一致，详细可看原文）： 一般的神经网络利用链式法则，将梯度从最外层的函数逐层向内传播，并更新每一层的参数θ\\thetaθ ,这就需要在前向传播中需要保留所有层的激活值，并在沿计算路径反传梯度时利用这些激活值。这对内存的占用非常大，层数越多，占用的内存也越大，这限制了深度模型的训练过程。 本文给出的用ODENet反向传播的方法不存储任何中间过程，因而不管层数如何加深，只需要常数级的内存成本。 自适应的计算。传统的欧拉法会有误差逐层累积的缺陷，而ODENet可以在训练过程中实时的监测误差水平，并可以调整精度来控制模型的成本。例如：在训练时我们可以使用较高的精度使训练的模型尽可能准确，而在测试时可以使用较低的精度，减少测试成本。 应用在流模型上会极大简化变分公式的计算，在下文中详细讲解 在时间上的连续性，好理解不展开 反向传播 在训练连续神经网络的过程中，正向传播可以使用ODE slove。但对ODE solve求导来进行反向传播求解梯度是很困难的，本篇文章使用Pontryagin的伴随方法(adjoint method) 来求解梯度，该方法不仅在计算和内存上有更大优势，同时还能够精确地控制数值误差。 具体而言，对于： L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))(1)\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right)\\tag{1} L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ))(1) 为优化LLL ,我们需要计算他对于参数z(t0),t0,t1z(t_0),t_0,t_1z(t0​),t0​,t1​ 和θ\\thetaθ 的梯度。 第一步是确定loss的梯度如何取决于隐藏状态z(t)z(t)z(t) 的变化，这在文章中被称作伴随a(t)a(t)a(t) (adjointa(t)adjoint \\quad a(t)adjointa(t) ) a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 这个a(t)a(t)a(t) 可以由另一个ODE给定(证明补充在后面)： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 在传统的基于链式法则的反向传播过程中，我们将后一层对前一层进行求导以传递梯度(∂L/∂z(t0)=∂L/∂z(t1)∗∂z(t1)/∂z(t0)\\partial L/\\partial z(t_0)=\\partial L/\\partial z(t_1) * \\partial z(t_1) / \\partial z(t_0)∂L/∂z(t0​)=∂L/∂z(t1​)∗∂z(t1​)/∂z(t0​))，而在ODENet中，可以再次调用ODESolve计算∂L/∂z(t0)\\partial L/\\partial z(t_0)∂L/∂z(t0​)。 对于计算相对于参数θ\\thetaθ 的梯度，公式类似： dLdθ=∫t1t0a(t)⊤∂f(z(t),t,θ)∂θdt(3)\\frac{d L}{d \\theta}=\\int_{t_{1}}^{t_{0}} a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t\\tag{3} dθdL​=∫t1​t0​​a(t)⊤∂θ∂f(z(t),t,θ)​dt(3) 这三个积分(带标号的三个)可以在同一个ODE solver过程中进行计算： emmm……文章写的是三个，我感觉算法里是四个…… 也有可能三个是说三个梯度吧=_= 简单解释如何理解上面这个算法： 以前向传播为例，ODESolve(h(t0),f,t0,t1,θ)ODESolve(h(t_0),f,t_0,t_1,\\theta)ODESolve(h(t0​),f,t0​,t1​,θ) 表示求解常微分方程dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta)dtdh(t)​=f(h(t),t,θ) 的数值解h(t1)h(t_1)h(t1​) 。 将积分串联在一起以使用一次ODESolver解出所有量。 如果Loss不仅仅取决于最终状态，那么在用ODENet进行反向传播时，需要在这些状态中进行一系列的单独求解，每次都要调整算法中的 adjoint a(t)a(t)a(t) 。 用ODE网络替代ResNet 对图像进行两次下采样，然后分别应用六个残差块和一个ODENet进行对比： RK-Net是用Runge-Kutta积分器，直接进行反向误差的传播 LLL 表示ResNet的隐藏层数，L~\\tilde{L}L~ 表示调用ODESolve的次数。 误差控制，前向传播和反向传播的求值次数，网络深度表现在下图： a：求值次数和精度成反比 b：求值次数与时间成正比 c：求值次数与反向传播时间成正比，并且反向传播的时间大概是正向传播的一半 d：网络深度，由于ODENet是一个连续网络，没有隐藏层，因此将评估点的数量作为深度，可以看到在训练过程中网络深度逐渐增加 连续的归一化流模型 流模型的解读在前一篇博客中 流模型使用一个可逆函数fff 进行两个分布之间的映射，变换前后的两个分布满足变量代换定理： z1=f(z0)⟹log⁡p(z1)=log⁡p(z0)−log⁡∣det⁡∂f∂z0∣\\mathrm{z}_{1}=f\\left(\\mathrm{z}_{0}\\right) \\Longrightarrow \\log p\\left(\\mathrm{z}_{1}\\right)=\\log p\\left(\\mathrm{z}_{0}\\right)-\\log \\left|\\operatorname{det} \\frac{\\partial f}{\\partial \\mathrm{z}_{0}}\\right| z1​=f(z0​)⟹logp(z1​)=logp(z0​)−log∣∣∣∣​det∂z0​∂f​∣∣∣∣​ 平面归一化流(NICE之后的一篇流模型的文章，这篇论文没看……)使用的变换： z(t+1)=z(t)+uh(w⊤z(t)+b),log⁡p(z(t+1))=log⁡p(z(t))−log⁡∣1+u⊤∂h∂z∣\\mathbf{z}(t+1)=\\mathbf{z}(t)+u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\log p(\\mathbf{z}(t+1))=\\log p(\\mathbf{z}(t))-\\log \\left|1+u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}}\\right| z(t+1)=z(t)+uh(w⊤z(t)+b),logp(z(t+1))=logp(z(t))−log∣∣∣∣​1+u⊤∂z∂h​∣∣∣∣​ 在流模型中，为使∂f/∂z\\partial f/\\partial z∂f/∂z 的雅可比行列式易于计算，通常是通过精心构建函数fff 来实现。并且fff 还需要是可逆的。而在这篇文章里发现，将离散的流模型换成连续流模型，可以极大的简化计算：不需要去计算∂f/∂z\\partial f/ \\partial z∂f/∂z 的行列式，只需要计算迹，并且不需要构建fff 可逆：fff 可以是任意函数，它是天然可逆的(常微分方程决定的函数只要满足唯一性，就一定是双射的)，因此fff 理论上可以是任何网络。 核心定理（梯度变元定理）： 证明过程见附录 于是我们将平面归一化流模型连续化： dz(t)dt=uh(w⊤z(t)+b),∂log⁡p(z(t))∂t=−u⊤∂h∂z(t)\\frac{d \\mathbf{z}(t)}{d t}=u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}(t)} dtdz(t)​=uh(w⊤z(t)+b),∂t∂logp(z(t))​=−u⊤∂z(t)∂h​ 不同于求行列式的值，求迹还是一个连续函数，因此如果常微分方程dz/dtdz/dtdz/dt 是由一组函数的和给出的，那么对数概率密度也可以直接用迹的和表示： dz(t)dt=∑n=1Mfn(z(t)),dlog⁡p(z(t))dt=∑n=1Mtr⁡(∂fn∂z)\\frac{d \\mathbf{z}(t)}{d t}=\\sum_{n=1}^{M} f_{n}(\\mathbf{z}(t)), \\quad \\frac{d \\log p(\\mathbf{z}(t))}{d t}=\\sum_{n=1}^{M} \\operatorname{tr}\\left(\\frac{\\partial f_{n}}{\\partial \\mathbf{z}}\\right) dtdz(t)​=n=1∑M​fn​(z(t)),dtdlogp(z(t))​=n=1∑M​tr(∂z∂fn​​) 因此对于有M个隐藏状态的连续流模型来说，计算成本仅仅是O(M)\\mathcal{O}\\left(M\\right)O(M),而平面归一化流的计算成本是O(M3)\\mathcal{O}\\left(M^{3}\\right)O(M3) 。 NF和CNF的比较： 通过ODE对时间序列建模 zt0∼p(zt0)z_{t_{0}} \\sim p\\left(z_{t_{0}}\\right) zt0​​∼p(zt0​​) zt1,zt2,…,ztN=ODESolve(zt0,f,θf,t0,…,tN)z_{t_{1}}, z_{t_{2}}, \\ldots, z_{t_{N}} =ODESolve(z_{t_0},f,\\theta_f,t_0,\\ldots,t_N) zt1​​,zt2​​,…,ztN​​=ODESolve(zt0​​,f,θf​,t0​,…,tN​) eachxti∼p(x∣zti,θX)each \\quad x_{t_i} \\sim p(x|z_{t_i},\\theta_X) eachxti​​∼p(x∣zti​​,θX​) 具体而言，在给定初始状态 z0z_0z0​ 和观测时间 t0,…tNt_0,\\ldots t_Nt0​,…tN​ 的情况下，该模型计算潜在状态 zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 和输出 xt1…xtNx_{t_1} \\ldots x_{t_N}xt1​​…xtN​​。在实验部分，初始状态z0z_0z0​由RNN编码产生，潜在状态zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 由ODESolve产生，其中的fff 用神经网络训练，然后利用VAE的方式从潜在状态中生成数据。 实验：从采样点进行螺旋线重建 均方差比较： 附录 伴随法的证明 对于z(t)z(t)z(t) 给定常微分方程： dz(t)d(t)=f(z(t),t,θ)\\frac{dz(t)}{d(t)}=f(z(t),t,\\theta) d(t)dz(t)​=f(z(t),t,θ) L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right) L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ)) 定义便随状态： a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 则： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 证明： z(t+ε)=∫tt+εf(z(t),t,θ)dt+z(t)=Tε(z(t),t)\\mathbf{z}(t+\\varepsilon)=\\int_{t}^{t+\\varepsilon} f(\\mathbf{z}(t), t, \\theta) d t+\\mathbf{z}(t)=T_{\\varepsilon}(\\mathbf{z}(t), t) z(t+ε)=∫tt+ε​f(z(t),t,θ)dt+z(t)=Tε​(z(t),t) 然后应用链式法则，有： dL∂z(t)=dLdz(t+ε)dz(t+ε)dz(t) or a(t)=a(t+ε)∂Tε(z(t),t)∂z(t)\\frac{d L}{\\partial \\mathbf{z}(t)}=\\frac{d L}{d \\mathbf{z}(t+\\varepsilon)} \\frac{d \\mathbf{z}(t+\\varepsilon)}{d \\mathbf{z}(t)} \\quad \\text { or } \\quad \\mathbf{a}(t)=\\mathbf{a}(t+\\varepsilon) \\frac{\\partial T_{\\varepsilon}(\\mathbf{z}(t), t)}{\\partial \\mathbf{z}(t)} ∂z(t)dL​=dz(t+ε)dL​dz(t)dz(t+ε)​ or a(t)=a(t+ε)∂z(t)∂Tε​(z(t),t)​ 利用导数定义，并进行泰勒展开进行化简计算： da(t)dt=lim⁡ε→0+a(t+ε)−a(t)ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)Tε(z(t))ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)(z(t)+εf(z(t),t,θ)+O(ε2))ε=lim⁡ε→0+a(t+ε)−a(t+ε)(I+ε∂f(z(t),t,θ)θz(t)+O(ε2))ε=lim⁡ε→0+−εa(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε2)ε=lim⁡ε→0+−a(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε)=−a(t)∂f(z(t),t,θ)∂z(t)\\begin{aligned} \\frac{d \\mathbf{a}(t)}{d t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)} T_{\\varepsilon}(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)}\\left(\\mathbf{z}(t)+\\varepsilon f(\\mathbf{z}(t), t, \\theta)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon)\\left(I+\\varepsilon \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\theta \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{-\\varepsilon \\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}}-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}(\\varepsilon) \\\\ &amp;=-\\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)} \\end{aligned} dtda(t)​​=ε→0+lim​εa(t+ε)−a(t)​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​Tε​(z(t))​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​(z(t)+εf(z(t),t,θ)+O(ε2))​=ε→0+lim​εa(t+ε)−a(t+ε)(I+εθz(t)∂f(z(t),t,θ)​+O(ε2))​=ε→0+lim​ε−εa(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε2)​=ε→0+lim​−a(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε)=−a(t)∂z(t)∂f(z(t),t,θ)​​ 对于θ\\thetaθ 和ttt 定义增强状态： ddt[zθt](t)=faug⁡([z,θ,t]):=[f([z,θ,t])01],aaug:=[aaθat],aθ(t):=dLdθ(t),at(t):=dLdt(t)\\frac{d}{d t}\\left[\\begin{array}{l} \\mathrm{z} \\\\ \\theta \\\\ t \\end{array}\\right](t)=f_{\\operatorname{aug}}([\\mathrm{z}, \\theta, t]):=\\left[\\begin{array}{c} f([\\mathrm{z}, \\theta, t]) \\\\ 0 \\\\ 1 \\end{array}\\right], \\mathbf{a}_{a u g}:=\\left[\\begin{array}{l} \\mathrm{a} \\\\ \\mathrm{a}_{\\theta} \\\\ \\mathrm{a}_{t} \\end{array}\\right], \\mathrm{a}_{\\theta}(t):=\\frac{d L}{d \\theta(t)}, \\mathrm{a}_{t}(t):=\\frac{d L}{d t(t)} dtd​⎣⎡​zθt​⎦⎤​(t)=faug​([z,θ,t]):=⎣⎡​f([z,θ,t])01​⎦⎤​,aaug​:=⎣⎡​aaθ​at​​⎦⎤​,aθ​(t):=dθ(t)dL​,at​(t):=dt(t)dL​ 其中θ\\thetaθ 和ttt 无关，即dθ(t)/dt=0,dt(t)/dt=1d\\theta(t) /dt=0,dt(t)/dt=1dθ(t)/dt=0,dt(t)/dt=1 计算雅可比行列式： ∂faug∂[z,θ,t]=[∂f∂z∂f∂θ∂f∂t000000]\\frac{\\partial f_{a u g}}{\\partial[\\mathbf{z}, \\theta, t]}=\\left[\\begin{array}{ccc} \\frac{\\partial f}{\\partial z} &amp; \\frac{\\partial f}{\\partial \\theta} &amp; \\frac{\\partial f}{\\partial t} \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right] ∂[z,θ,t]∂faug​​=⎣⎡​∂z∂f​00​∂θ∂f​00​∂t∂f​00​⎦⎤​ 直接将faugf_{aug}faug​ 和aauga_{aug}aaug​ 代入上一小节的伴随法公式： daaug(t)dt=−[a(t)aθ(t)at(t)]∂faug∂[z,θ,t](t)=−[a∂f∂za∂f∂θa∂f∂t](t)\\frac{d \\mathbf{a}_{a u g}(t)}{d t}=-\\left[\\begin{array}{lllll} \\mathbf{a}(t) &amp; \\mathbf{a}_{\\theta}(t) &amp; \\mathbf{a}_{t}(t) \\end{array}\\right] \\frac{\\partial f_{\\text {aug}}}{\\partial[\\mathbf{z}, \\theta, t]}(t)=-\\left[\\begin{array}{lll} \\mathbf{a} \\frac{\\partial f}{\\partial \\mathbf{z}} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial \\theta} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial t} \\end{array}\\right](t) dtdaaug​(t)​=−[a(t)​aθ​(t)​at​(t)​]∂[z,θ,t]∂faug​​(t)=−[a∂z∂f​​a∂θ∂f​​a∂t∂f​​](t) 于是得到了最终的结论： dLdθ=∫tNt0a(t)∂f(z(t),t,θ)∂θdt\\frac{d L}{d \\theta}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t dθdL​=∫tN​t0​​a(t)∂θ∂f(z(t),t,θ)​dt dLdtN=−a(tN)∂f(z(tN),tN,θ)∂tNdLdt0=∫tNt0a(t)∂f(z(t),t,θ)∂tdt\\frac{d L}{d t_{N}}=-\\mathbf{a}\\left(t_{N}\\right) \\frac{\\partial f\\left(\\mathbf{z}\\left(t_{N}\\right), t_{N}, \\theta\\right)}{\\partial t_{N}} \\quad \\frac{d L}{d t_{0}}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial t} d t dtN​dL​=−a(tN​)∂tN​∂f(z(tN​),tN​,θ)​dt0​dL​=∫tN​t0​​a(t)∂t∂f(z(t),t,θ)​dt 梯度变元定理的证明 给定常微分方程： dz(t)d(t)=f(z(t),t)\\frac{dz(t)}{d(t)}=f(z(t),t) d(t)dz(t)​=f(z(t),t) fff 要求对zzz Lipschitz连续，对ttt 连续。则对数概率密度满足： ∂log⁡p(z(t))∂t=−tr⁡(dfdz(t))\\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-\\operatorname{tr}\\left(\\frac{d f}{d \\mathbf{z}}(t)\\right) ∂t∂logp(z(t))​=−tr(dzdf​(t)) 证明： 首先类似上面伴随法证明的过程，将z(t+ε)z(t+\\varepsilon)z(t+ε) 表示为Tε(z(t))T_{\\varepsilon}(\\mathbf{z}(t))Tε​(z(t)) fff 要求对zzz Lipschitz连续，对ttt 连续。这是为了使方程满足Picard存在定理，使得解存在且唯一。 首先是要推导出 ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ϵ∂∂zTε(z(t)))\\frac{\\partial \\log p(z(t))}{\\partial t}=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\epsilon} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(z(t))\\right) ∂t∂logp(z(t))​=−tr(ε→0+lim​∂ϵ∂​∂z∂​Tε​(z(t))) 过程： ∂log⁡p(z(t))∂t=lim⁡ε→0+log⁡p(z(t))−log⁡∣det⁡∂∂zTε(z(t))∣−log⁡p(z(t))ε=−lim⁡ε→0+log⁡∣det⁡∂∂zTε(z(t))∣ε=−lim⁡ε→0+∂∂εlog⁡∣det⁡∂∂zTε(z(t))∣∂∂εε=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣∣det⁡∂∂zTε(z(t))∣(∂log⁡(z)∂z∣z=1=1)=−(lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣)⏟bounded (lim⁡ε→0+1∣det⁡∂∂zTε(z(t))∣)=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log p(\\mathbf{z}(t))-\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|-\\log p(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon} \\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\frac{\\partial}{\\partial \\varepsilon} \\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|} \\qquad \\qquad \\quad \\left(\\left.\\frac{\\partial \\log (\\mathbf{z})}{\\partial \\mathbf{z}}\\right|_{\\mathbf{z}=1}=1\\right)\\\\ &amp;=-\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0+} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|\\right)}_{\\text {bounded }}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{1}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}\\right)\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right| \\end{aligned} ∂t∂logp(z(t))​​=ε→0+lim​εlogp(z(t))−log∣∣​det∂z∂​Tε​(z(t))∣∣​−logp(z(t))​=−ε→0+lim​εlog∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∂ε∂​ε∂ε∂​log∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​∂ε∂​∣∣​det∂z∂​Tε​(z(t))∣∣​​(∂z∂log(z)​∣∣∣∣​z=1​=1)=−bounded (ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​)​​(ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​1​)=−ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​​ 第一步用到的是流模型中的公式(本质是概率密度上的雅可比公式)，后面仅用到洛必达法则、链式法则等简单技巧。 然后应用雅可比公式： ∂log⁡p(z(t))∂t=−lim⁡ε→0+tr⁡(adj⁡(∂∂zTε(z(t)))∂∂ε∂∂zTε(z(t)))=−tr⁡((lim⁡ε→0+adj⁡(∂∂zTε(z(t))))⏟=I(lim⁡ε→0+∂∂ε∂∂zTε(z(t))))=−tr⁡(lim⁡ε→0+∂∂ε∂∂zTε(z(t)))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{tr}\\left(\\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right)}_{=I}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\end{aligned} ∂t∂logp(z(t))​​=−ε→0+lim​tr(adj(∂z∂​Tε​(z(t)))∂ε∂​∂z∂​Tε​(z(t)))=−tr⎝⎜⎜⎛​=I(ε→0+lim​adj(∂z∂​Tε​(z(t))))​​(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))⎠⎟⎟⎞​=−tr(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))​ 雅可比公式是： ddtdet(A(t))=tr(adj(A(t))ddtA(t))\\frac{d}{dt}det(A(t))=tr(adj(A(t))\\frac{d}{dt}A(t)) dtd​det(A(t))=tr(adj(A(t))dtd​A(t)) 最后进行泰勒展开即可： ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ε∂∂z(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+∂∂ε(I+∂∂zεf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+(∂∂zf(z(t),t)+O(ε)+O(ε2)+…))=−tr⁡(∂∂zf(z(t),t))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}}\\left(\\mathbf{z}+\\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left(I+\\frac{\\partial}{\\partial \\mathbf{z}} \\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)+\\mathcal{O}(\\varepsilon)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)\\right) \\end{aligned} ∂t∂logp(z(t))​​=−tr(ε→0+lim​∂ε∂​∂z∂​(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​∂ε∂​(I+∂z∂​εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​(∂z∂​f(z(t),t)+O(ε)+O(ε2)+…))=−tr(∂z∂​f(z(t),t))​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"gan,vae和flow","slug":"GAN VAE and Flow","date":"2020-02-15T13:19:00.000Z","updated":"2020-03-09T13:36:05.975Z","comments":true,"path":"2020/02/15/GAN VAE and Flow/","link":"","permalink":"/2020/02/15/GAN VAE and Flow/","excerpt":"","text":"前言 GAN，VAE和FLOW的目标是一致的——希望构建一个从隐变量ZZZ生成目标数据XXX的模型，其中先验分布P(z)P(z)P(z)通常被设置为高斯分布。我们希望找到一个变换函数f(x)f(x)f(x)，他能建立一个从zzz到xxx的映射：f:z→xf:z\\to xf:z→x，然后在P(Z)P(Z)P(Z)中随机采样一个点z′z&#x27;z′，通过映射fff，就可以找到一个新的样本点x′x&#x27;x′。 举个栗子： 如何将均匀分布U[0,1]U[0,1]U[0,1]映射成正态分布N(0,1)N(0,1)N(0,1)？ 将X∼U[0,1]X \\sim U[0,1]X∼U[0,1]经过函数Y=f(x)Y = f(x)Y=f(x)映射之后，就有Y∼N(0,1)Y\\sim N(0,1)Y∼N(0,1)了那么[x,x+dx][x,x+dx][x,x+dx]和[y,y+dy][y,y+dy][y,y+dy]两个区间上的概率应该相等，即： ρ(x)dx=12πexp⁡(−y22)dy\\rho(x) d x=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{y^{2}}{2}\\right) d y ρ(x)dx=2π​1​exp(−2y2​)dy 对其进行积分，有： ∫0xρ(t)dt=∫−∞y12πexp⁡(−t22)dt=Φ(y)\\int_{0}^{x} \\rho(t) d t=\\int_{-\\infty}^{y} \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{t^{2}}{2}\\right) d t=\\Phi(y) ∫0x​ρ(t)dt=∫−∞y​2π​1​exp(−2t2​)dt=Φ(y) y=Φ−1(∫0xρ(t)dt)=f(x)y=\\Phi^{-1}\\left(\\int_{0}^{x} \\rho(t) d t\\right)=f(x) y=Φ−1(∫0x​ρ(t)dt)=f(x) 可以看到Y=f(X)Y = f(X)Y=f(X)的解是存在的，但很复杂，无法用初等函数进行显示的表示，因此在大多数情况下，我们都是通过神经网络来拟合这个函数。 假设我们现在已经有一个映射fff，我们如何衡量映射fff构造出来的数据集f(z1),f(z2),...,f(zn)f(z_1),f(z_2),...,f(z_n)f(z1​),f(z2​),...,f(zn​)，是否和目标数据XXX分布相同？(注：KL和JS距离根据两个概率分布的表达式计算分布的相似度，而我们现在只有从构造的分布采样的数据和真实分布采样的数据，而离散化的KL和JS距离因为图像维度问题，计算量非常大)。在这里GAN采用了一个暴力的办法：训练一个判别器作为两者相似性的度量，而VAE(变分自编码器)和FLOW(流模型)在最大化最大似然。 VAE(变分自编码器) VAE的基本思路 对于连续随机变量，概率分布PPP和QQQ，KL散度(又称相对熵)的定义为： DKL(P∥Q)=∫−∞∞p(x)ln⁡p(x)q(x)dx=Ex∼P(x)[logP(x)−logQ(x)]D_{\\mathrm{KL}}(P \\| Q)=\\int_{-\\infty}^{\\infty} p(x) \\ln \\frac{p(x)}{q(x)} \\mathrm{d} x=E_{x \\sim P(x)}[logP(x)-logQ(x)] DKL​(P∥Q)=∫−∞∞​p(x)lnq(x)p(x)​dx=Ex∼P(x)​[logP(x)−logQ(x)] 给定一个概率分布DDD,已知其概率密度函数(连续分布)或概率质量函数()离散分布为fDf_DfD​，以及一个分布参数θ\\thetaθ，我们可以从这个分布中抽出一个具有nnn个值的采样X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，利用fDf_DfD​计算出其似然函数： L(θ∣x1,…,xn)=fθ(x1,…,xn)\\mathbf{L}\\left(\\theta | x_{1}, \\ldots, x_{n}\\right)=f_{\\theta}\\left(x_{1}, \\ldots, x_{n}\\right) L(θ∣x1​,…,xn​)=fθ​(x1​,…,xn​) 若DDD是离散分布，fθf_{\\theta}fθ​即是在参数为θ\\thetaθ时观测到这一采样的概率。若其是连续分布，fθf_{\\theta}fθ​则为X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​联合分布的概率密度函数在观测值处的取值。一旦我们获得X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，我们就能求得一个关于θ\\thetaθ的估计。最大似然估计会寻找关于的最可能的值（即，在所有可能的取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的值即称为的最大似然估计。由定义，最大似然估计是样本的函数。 VAE做最大似然估计，也就是要最大化概率： P(X)=∑iP(X∣zi;θ)P(zi)P(X)=\\sum_{i} P\\left(X | z_{i} ; \\theta\\right) P\\left(z_{i}\\right) P(X)=i∑​P(X∣zi​;θ)P(zi​) 这里可以理解为使用积分创造更多的分布，一般选择P(Z)P(Z)P(Z)服从一个高斯分布，而p(X∣z)p(X|z)p(X∣z)可以是任意分布，例如条件高斯分布或狄拉克分布，理论上讲，这个积分形式的分布可以拟合任意分布。 但是这里的P(X)P(X)P(X)是积分形式的，很难进行计算。VAE从让人望而生畏的变分和贝叶斯理论出发，推导出了一个很接地气的公式： log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)](1)\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] \\tag{1} logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)](1) VAE并没有选择直接去优化P(X)P(X)P(X)，而是选择去优化他的一个变分下界（公式1右端）。 而VAE的自编码器性质也从这个公式里开始体现出来：我们可以将D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]视作编码器的优化，使由真实数据编码出的隐变量分布Q(z∣X)Q(z|X)Q(z∣X)去尽量近似P(z)P(z)P(z)（标准高斯分布），而将Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]视作解码器的优化，使得服从分布QQQ的隐变量zzz解码出的xxx尽可能地服从真是数据分布，而将D[Q(z∣X)∥P(z∣X)]\\mathcal{D}[Q(z | X) \\| P(z | X)]D[Q(z∣X)∥P(z∣X)]视作误差项。 但VAE也因为它并没有直接去优化P(X)P(X)P(X)，而选择去优化它的变分下界，使得他只是一个近似模型，无法保证良好的生成效果。 VAE的优化过程 首先要确定概率密度Q(z∣X)Q(z|X)Q(z∣X)的形式，一般选择正态分布，即N(μ,σ2)\\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)N(μ,σ2)，其中μ(X;θμ),σ2(X;θσ)\\mu\\left(X ; \\theta_{\\mu}\\right) , \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)μ(X;θμ​),σ2(X;θσ​)通过两个神经网络(编码器)训练出来。公式中的D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]变为D[N(μ(X;θμ),σ2(X;θσ))∥N(0,I)]D\\left[\\mathcal{N}\\left(\\mu\\left(X ; \\theta_{\\mu}\\right), \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)\\right) \\| \\mathcal{N}(0, I)\\right]D[N(μ(X;θμ​),σ2(X;θσ​))∥N(0,I)]，这个时候就可以通过两个正态分布的KL散度的计算公式来计算这一项。 对于第一项Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]，对于一个batch来说，可以在QQQ中采样，然后将单个样本的log⁡P(X∣z)\\log P(X|z)logP(X∣z)求和取平均数作为期望的估计。但这样出现一个问题：把Q(z∣X)Q(z|X)Q(z∣X)弄丢了，也就是每次训练的时候梯度不传进QQQ里，论文里采用了一个称为重参数化技巧(reparamenterization trick)的方法，如图： 至此，整个VAE网络就可以训练了。 公式推导部分 D[Q(z∣X)∣∣P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)−log⁡P(X)]+log⁡P(X)\\begin{aligned} \\mathcal{D}[Q(z|X)||P(z|X)] &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X)] \\\\ &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X) - \\log P(X)] + \\log P(X) \\end{aligned} D[Q(z∣X)∣∣P(z∣X)]​=Ez∼Q​[logQ(z∣X)−logP(z∣X)]=Ez∼Q​[logQ(z∣X)−logP(z∣X)−logP(X)]+logP(X)​ 移项得 log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)]\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)] GAN 模型构建 由于大家都对GAN比较熟悉，本文直接从变分推断的角度去理解GAN。 不同于VAE将P(X∣z)P(X|z)P(X∣z)选为高斯分布，GAN的选择是： P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dzP(x | z)=\\delta(x-G(z)), \\quad P(x)=\\int P(x | z) P(z) d z P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dz 其中δ(x)\\delta (x)δ(x)是狄拉克函数，G(z)G(z)G(z)为生成器网络。 在VAE中z被当作是一个隐变量，但在GAN中，狄拉克函数意味着单点分布，即x和z为一一对应的关系。于是在GAN中z没有被当作隐变量处理(不需要考虑后验分布P(z∣x)P(z|x)P(z∣x)) 判别器的理解： 在GAN中引入了一个二元的隐变量y来构成联合分布，其中p~(x)\\tilde{p}(x)p~​(x) 为真实样本的分布： q(x,y)={p~(x)p1,y=1p(x)p0,y=0q(x, y)=\\left\\{\\begin{array}{l} {\\tilde{p}(x) p_{1}, y=1} \\\\ {p(x) p_{0}, y=0} \\end{array}\\right. q(x,y)={p~​(x)p1​,y=1p(x)p0​,y=0​ 这里y是图像的真实标签，当图片为真实图片时，y=1，当图片是生成图片时，y=0。 其中p1+p0=1p_1+p_0=1p1​+p0​=1描述了一个二元概率分布，比如：从真实样本采集m个样本，从生成样本中采集m个样本，同时传入判别器，则p0=p1=1/2p_0=p_1=1/2p0​=p1​=1/2。在下面讨论中我们直接取p0=p1=1/2p_0=p_1=1/2p0​=p1​=1/2 另一方面，我们需要使判别器的判别结果尽可能真实，设p(x,y)=p(y∣x)p~(x)p(x,y)=p(y|x)\\tilde{p}(x)p(x,y)=p(y∣x)p~​(x)，p(y∣x)p(y|x)p(y∣x)为一个条件伯努利分布(判别器的判别结果)。优化目标是KL(q(x,y)∣∣p(x,y))KL(q(x,y)||p(x,y))KL(q(x,y)∣∣p(x,y))： KL(q(x,y)∥p(x,y))=∫p~(x)p1log⁡p~(x)p1p(1∣x)p~(x)dx+∫p(x)p0log⁡p(x)p0p(0∣x)p~(x)dx∼∫p~(x)log⁡12p(1∣x)dx+∫p(x)log⁡p(x)2p(0∣x)p~(x)dx=−Ex∼p~(x)[log⁡2p(1∣x)]−Ex∼p(x)[log⁡2p(0∣x)]+KL(p(x)∣∣p~(x))\\begin{aligned} K L(q(x, y) \\| p(x, y)) &amp;=\\int \\tilde{p}(x) p_{1} \\log \\frac{\\tilde{p}(x) p_{1}}{p(1 | x) \\tilde{p}(x)} d x+\\int p(x) p_{0} \\log \\frac{p(x) p_{0}}{p(0 | x) \\tilde{p}(x)} d x \\\\ &amp; \\sim \\int \\tilde{p}(x) \\log \\frac{1}{2p(1 | x)} d x+\\int p(x) \\log \\frac{p(x)}{2p(0 | x) \\tilde{p}(x)} d x\\\\ &amp; = -E_{x \\sim \\tilde{p}(x)}[\\log 2p(1|x)]-E_{x \\sim p(x)}[\\log 2p(0|x)]+KL(p(x)||\\tilde{p}(x)) \\end{aligned} KL(q(x,y)∥p(x,y))​=∫p~​(x)p1​logp(1∣x)p~​(x)p~​(x)p1​​dx+∫p(x)p0​logp(0∣x)p~​(x)p(x)p0​​dx∼∫p~​(x)log2p(1∣x)1​dx+∫p(x)log2p(0∣x)p~​(x)p(x)​dx=−Ex∼p~​(x)​[log2p(1∣x)]−Ex∼p(x)​[log2p(0∣x)]+KL(p(x)∣∣p~​(x))​ 一旦成功优化，就有q(x,y)→p(x,y)q(x,y)\\to p(x,y)q(x,y)→p(x,y)，对于x求边缘概率分布，有： 12p~(x)+12p(x)→p(1∣x)p~(x)+p(0∣x)p~(x)=p~(x)\\frac{1}{2}\\tilde{p}(x)+\\frac{1}{2}p(x)\\to p(1|x)\\tilde{p}(x)+p(0|x)\\tilde{p}(x)=\\tilde{p}(x) 21​p~​(x)+21​p(x)→p(1∣x)p~​(x)+p(0∣x)p~​(x)=p~​(x) 即： p(x)→p~(x)p(x)\\to \\tilde{p}(x) p(x)→p~​(x) 这就完成了对模型的构建。 目标优化 现在我们有优化目标：p(1∣x)p(1|x)p(1∣x)和G(z)G(z)G(z)，分别是判别器(p(y∣x)p(y|x)p(y∣x)服从条件伯努利分布，可以直接由p(1∣x)p(1|x)p(1∣x)确定)和生成器(p(x)p(x)p(x)由G(z)G(z)G(z)决定)。类似EM算法，我们进行交替优化：先固定G(z)G(z)G(z),这也意味着p(x)p(x)p(x)固定了，然后优化p(y∣x)p(y|x)p(y∣x)，优化目标为： D=arg⁡min⁡D{−Ex∼p~(x)[log⁡2D(x)]−Ex∼p(x)[log⁡2(1−D(x))]}D=\\underset{D}{\\arg \\min }\\{-E_{x \\sim \\tilde{p}(x)}[\\log 2D(x)]-\\mathbb{E}_{x \\sim p(x)}[\\log 2(1-D(x))]\\} D=Dargmin​{−Ex∼p~​(x)​[log2D(x)]−Ex∼p(x)​[log2(1−D(x))]} 然后固定D(x)D(x)D(x)来优化G(x)G(x)G(x)，相关loss为： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dxG=\\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x G=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx 假设D(x)D(x)D(x)有足够的拟合能力，注意到当D(x)=p~(x)p~(x)+p0(x)D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}D(x)=p~​(x)+p0(x)p~​(x)​时，有 KL(q(x,y)∥p0(x,y))=∫p~(x)log⁡12D(x)dx+∫p0(x)log⁡p0(x)2(1−D(x))p~(x)dx=∫p~(x)log⁡p~(x)+p0(x)2p~(x)+p0(x)log⁡p~(x)+p0(x)2p~(x)=KL(p~(x)+p0(x)∣∣2p~(x))\\begin{aligned} K L(q(x, y) \\| p^0(x, y)) &amp;= \\int \\tilde{p}(x) \\log \\frac{1}{2D(x)} d x+\\int p^0(x) \\log \\frac{p^0(x)}{2(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\int\\tilde{p}(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}+p^0(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}\\\\ &amp;= KL(\\tilde{p}(x) +p^0(x)||2\\tilde{p}(x) ) \\end{aligned} KL(q(x,y)∥p0(x,y))​=∫p~​(x)log2D(x)1​dx+∫p0(x)log2(1−D(x))p~​(x)p0(x)​dx=∫p~​(x)log2p~​(x)p~​(x)+p0(x)​+p0(x)log2p~​(x)p~​(x)+p0(x)​=KL(p~​(x)+p0(x)∣∣2p~​(x))​ 不严格的说法：由于现在对p0(x)p^0(x)p0(x) 和 p~(x)\\tilde{p}(x)p~​(x)没有约束，可以直接p0(x)=p~(x)p^0(x)=\\tilde{p}(x)p0(x)=p~​(x)使得loss等于0，也就是说D(x)=p~(x)p~(x)+p0(x)D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}D(x)=p~​(x)+p0(x)p~​(x)​为理论最优解。在优化判别器时，p0(x)p^0(x)p0(x)应该为上一阶段生成器优化的p(x)p(x)p(x) 。将这个D(x)D(x)D(x)代入生成器的相关loss： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dx=arg⁡min⁡G∫p(x)log⁡p(x)2D(x)p0(x)dx=arg⁡min⁡G[−Ex∼p(x)2D(x)+KL(p(x)∣∣p0(x))]=arg⁡min⁡G[−Ex∼p(x)2D(G(z))+KL(p(x)∣∣p0(x))]\\begin{aligned} G &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{ p(x)}{2D(x) p^0(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(x)+KL(p(x)||p^0(x))]\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(G(z))+KL(p(x)||p^0(x))] \\end{aligned} G​=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx=Gargmin​∫p(x)log2D(x)p0(x)p(x)​dx=Gargmin​[−Ex∼p(x)​2D(x)+KL(p(x)∣∣p0(x))]=Gargmin​[−Ex∼p(x)​2D(G(z))+KL(p(x)∣∣p0(x))]​ 可以看到，此时的第一项−Ex∼p(x)2D(G(z))-E_{x \\sim p(x)}2D(G(z))−Ex∼p(x)​2D(G(z))就是标准的GAN所采用的loss之一。而我们知道，目前标准的GAN生成器的loss都不包含KL(p(x)∣∣p0(x))KL(p(x)||p^0(x))KL(p(x)∣∣p0(x))，这实际上造成了loss的不完备。 顺便提一句，VAE中也有类似GAN中交替优化的方法，称为EM算法。 第二个loss是在限制要求新的生成器跟旧的生成器生成结果不能差别太大 ，也就是生成器不能剧烈变化。在loss不完备的情况下，假设有一个优化算法总能找到G(z)G(z)G(z)的理论最优解、并且G(z)G(z)G(z)具有无限的拟合能力，那么G(z)G(z)G(z)只需要生成唯一一个使得D(x)D(x)D(x)最大的样本（不管输入的zzz是什么），这就是模型坍缩。模型塌缩的视频(需要梯子)。 然后对第二项进行估算，得到一个可以在实验中使用的正则项： 记po(x)=qθ−Δθ(x),p(x)=qθ(x)p^{o}(x)=q_{\\theta-\\Delta \\theta}(x), \\quad p(x)=q_{\\theta}(x)po(x)=qθ−Δθ​(x),p(x)=qθ​(x)，其中Δθ\\Delta \\thetaΔθ为生成器的参数变化，对qo(x)=qθ−Δθ(x)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)qo(x)=qθ−Δθ​(x)做泰勒展开，有： qo(x)=qθ−Δθ(x)=qθ(x)−Δθ⋅∇θqθ(x)+O((Δθ)2)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)=q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)+O\\left((\\Delta \\theta)^{2}\\right) qo(x)=qθ−Δθ​(x)=qθ​(x)−Δθ⋅∇θ​qθ​(x)+O((Δθ)2) KL(q(x)∥qo(x))≈∫qθ(x)log⁡qθ(x)qθ(x)−Δθ⋅∇θqθ(x)dx=−∫qθ(x)log⁡[1−Δθ⋅∇θqθ(x)qθ(x)]dx≈−∫qθ(x)[−Δθ⋅∇θqθ(x)qθ(x)−(Δθ⋅∇θqθ(x)qθ(x))2]dx=Δθ⋅∇θ∫qθ(x)dx+(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx=(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx≈(Δθ⋅c)2\\begin{aligned} K L\\left(q(x) \\| q^{o}(x)\\right) &amp; \\approx \\int q_{\\theta}(x) \\log \\frac{q_{\\theta}(x)}{q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)} d x \\\\ &amp;=-\\int q_{\\theta}(x) \\log \\left[1-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right] d x \\\\ &amp; \\approx-\\int q_{\\theta}(x)\\left[-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}-\\left(\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right)^{2}\\right] d x \\\\ &amp;=\\Delta \\theta \\cdot \\nabla_{\\theta} \\int q_{\\theta}(x) d x+(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp;=(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp; \\approx(\\Delta \\theta \\cdot c)^{2} \\end{aligned} KL(q(x)∥qo(x))​≈∫qθ​(x)logqθ​(x)−Δθ⋅∇θ​qθ​(x)qθ​(x)​dx=−∫qθ​(x)log[1−qθ​(x)Δθ⋅∇θ​qθ​(x)​]dx≈−∫qθ​(x)[−qθ​(x)Δθ⋅∇θ​qθ​(x)​−(qθ​(x)Δθ⋅∇θ​qθ​(x)​)2]dx=Δθ⋅∇θ​∫qθ​(x)dx+(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx=(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx≈(Δθ⋅c)2​ 上式中应用了log⁡(1+x)\\log(1+x)log(1+x)的泰勒展开式以及求导和积分可互换、可积分的假设。上面的粗略估计表明，生成器的参数不能变化太大。而我们用的是基于梯度下降的优化算法，所以Δθ\\Delta \\thetaΔθ正比于梯度，因此标准GAN训练时的很多trick，比如梯度裁剪、用Adam优化器、用BN，都可以解释得通了，它们都是为了稳定梯度，使得Δθ\\Delta \\thetaΔθ不至于过大，同时，G(z)G(z)G(z)的迭代次数也不能过多，因为过多同样会导致Δθ\\Delta \\thetaΔθ过大。 正则项 考虑如何添加正则项以改进GAN的稳定性： 直接对KL(q(x)∥qo(x))K L\\left(q(x) \\| q^{o}(x)\\right)KL(q(x)∥qo(x))进行估算是很困难的，但是我们上面提到q(z∣x)q(z|x)q(z∣x)和qo(z∣x)q^o(z|x)qo(z∣x)是狄拉克分布，而狄拉克分布可以看作方差为0的高斯分布，于是考虑用KL(q(x,z)∥qo(x,z))K L\\left(q(x,z) \\| q^{o}(x,z)\\right)KL(q(x,z)∥qo(x,z))进行估算： KL(q(x,z)∥q~(x,z))=∬q(x∣z)q(z)log⁡q(x∣z)q(z)q~(x∣z)q(z)dxdz=∬δ(x−G(z))q(z)log⁡δ(x−G(z))δ(x−Go(z))dxdz=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\iint q(x | z) q(z) \\log \\frac{q(x | z) q(z)}{\\tilde{q}(x | z) q(z)} d x d z \\\\ &amp;=\\iint \\delta(x-G(z)) q(z) \\log \\frac{\\delta(x-G(z))}{\\delta\\left(x-G^{o}(z)\\right)} d x d z \\\\ &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∬q(x∣z)q(z)logq~​(x∣z)q(z)q(x∣z)q(z)​dxdz=∬δ(x−G(z))q(z)logδ(x−Go(z))δ(x−G(z))​dxdz=∫q(z)logδ(G(z)−Go(z))δ(0)​dz​ 将狄拉克分布可以看作方差为0的高斯分布,并代入： δ(x)=lim⁡σ→01(2πσ2)d/2exp⁡(−x22σ2)\\delta(x)=\\lim _{\\sigma \\rightarrow 0} \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{d / 2}} \\exp \\left(-\\frac{x^{2}}{2 \\sigma^{2}}\\right) δ(x)=σ→0lim​(2πσ2)d/21​exp(−2σ2x2​) KL(q(x,z)∥q~(x,z))=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz=lim⁡σ→0∫q(x)log⁡[1/exp⁡(−(G(z)−G0(z))22σ2)]dx=lim⁡σ→0∫q(x)(−(G(z)−G0(z))22σ2)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\log \\left[ 1/{\\exp \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)} \\right]dx \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)dx \\\\ &amp; \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∫q(z)logδ(G(z)−Go(z))δ(0)​dz=σ→0lim​∫q(x)log[1/exp(−2σ2(G(z)−G0(z))2​)]dx=σ→0lim​∫q(x)(−2σ2(G(z)−G0(z))2​)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz​ 于是有 KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dzK L\\left(q(x) \\| q^{o}(x)\\right) \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dz 从而完整的生成器loss可以选择为 Ez∼q(z)[−log⁡D(G(z))+λ∥G(z)−Go(z)∥2]\\mathbb{E}_{z \\sim q(z)}\\left[-\\log D(G(z))+\\lambda\\left\\|G(z)-G^{o}(z)\\right\\|^{2}\\right] Ez∼q(z)​[−logD(G(z))+λ∥G(z)−Go(z)∥2] 实验结果 FLOW 基本思路：直接硬算积分式 ∫zp(x∣z)p(z)dz\\int_{z} p(x | z) p(z) d z ∫z​p(x∣z)p(z)dz 流模型有一个非常与众不同的特点是，它的转换通常是可逆的。也就是说，流模型不 仅能找到从 A 分布变化到 B 分布的网络通路，并且该通路也能让 B 变化到 A，简言之流模 型找到的是一条 A、B 分布间的双工通路。当然，这样的可逆性是具有代价的——A、B 的 数据维度必须是一致的。 A、B 分布间的转换并不是轻易能做到的，流模型为实现这一点经历了三个步骤：最初 的 NICE 实现了从 A 分布到高斯分布的可逆求解；后来 RealNVP 实现了从 A 分布到条件非 高斯分布的可逆求解；而最新的 GLOW，实现了从 A 分布到 B 分布的可逆求解，其中 B 分 布可以是与 A 分布同样复杂的分布，这意味着给定两堆图片，GLOW 能够实现这两堆图片 间的任意转换。 NICE 两个一维分布之间的转化参考前言中的栗子，下面考虑高维分布： 类似一维分布，两个分布在映射前后的相同区域应该有相同的概率。 p(x′)∣det⁡(Jf)∣=π(z′)p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|=\\pi\\left(z^{\\prime}\\right) p(x′)∣det(Jf​)∣=π(z′) 其中JfJ_fJf​为雅可比行列式，函数fff将zzz上的分布变换到xxx上的分布。 根据雅可比行列式的逆运算，同样有： p(x′)=π(z′)∣det⁡(Jf−1)∣p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right| p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​ 至此，我们得到了一个比较重要的结论：如果 zzz 与 xxx 分别满足两种分布，并且 zzz 通过 函数 fff 能够转变为 xxx，那么 zzz 与 xxx 中的任意一组对应采样点 𝑧′𝑧′z′ 与 𝑥′𝑥′x′ 之间的关系为： {π(z′)=p(x′)∣det⁡(Jf)∣p(x′)=π(z′)∣det⁡(Jf−1)∣\\left\\{\\begin{array}{c} {\\pi\\left(z^{\\prime}\\right)=p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|} \\\\ {p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right|} \\end{array}\\right. {π(z′)=p(x′)∣det(Jf​)∣p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​​ 从这个公式引入了Flow_based_model 的基本思路：设计一个神经网络，将分布 xxx 映射到分布 zzz ，具体来说，流模型选择 q(z)q(z)q(z) 为高斯分布，q(x∣z)q(x|z)q(x∣z) 为狄拉克分布 δ(x−g(z)\\delta(x-g(z)δ(x−g(z) ，其中ggg 是可逆的： x=g(z)⇔z=f(x)x=g(z) \\Leftrightarrow z=f(x) x=g(z)⇔z=f(x) 要从理论上实现可逆，需要 xxx 和 zzz 的维数相同，将 zzz 的分布代入，则有： q(z)=1(2π)D/2exp⁡(−12∥z∥2)q(z)=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|z\\|^{2}\\right) q(z)=(2π)D/21​exp(−21​∥z∥2) q(x)=1(2π)D/2exp⁡(−12∥f(x)∥2)∣det⁡[∂f∂x]∣(2)q(\\boldsymbol{x})=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|\\boldsymbol{f}(\\boldsymbol{x})\\|^{2}\\right)\\left|\\operatorname{det}\\left[\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}\\right]\\right|\\tag{2} q(x)=(2π)D/21​exp(−21​∥f(x)∥2)∣∣∣∣​det[∂x∂f​]∣∣∣∣​(2) 公式(2)(2)(2)对 fff 提出了三个基本要求： 可逆，且逆函数容易计算。 对应的雅可比行列式方便计算 拟合能力强 这样的话，就有 log⁡q(x)=−D2log⁡(2π)−12∥f(x)∥2+log⁡∣det⁡[∂f∂x]∣\\log q(x)=-\\frac{D}{2} \\log (2 \\pi)-\\frac{1}{2}\\|f(x)\\|^{2}+\\log \\left|\\operatorname{det}\\left[\\frac{\\partial f}{\\partial x}\\right]\\right| logq(x)=−2D​log(2π)−21​∥f(x)∥2+log∣∣∣∣​det[∂x∂f​]∣∣∣∣​ 这个优化目标是可计算的，并且因为 fff 可逆，那么我们在zzz 中取样，就可以生成相应的 xxx x=f−1(z)=g(z)x=f^{-1}(z)=g(z) x=f−1(z)=g(z) 为了满足这三个条件，NICE和REAL NVP、GLOW都采用了模块化思想，将 fff 设计成一组函数的复合，其中每个函数都满足要求一和要求二，经过复合之后函数也容易满足要求三。 f=fL∘…∘f2∘f1f=f_{L} \\circ \\ldots \\circ f_{2} \\circ f_{1} f=fL​∘…∘f2​∘f1​ 相对而言，雅可比行列式的计算要比函数求逆更加复杂，考虑第二个要求，我们知道三角行列式最容易计算，所以我们要想办法让变换 fff 的雅可比矩阵为三角阵。NICE的做法是：将 DDD 的 xxx 分为两部分 x1,x2x_1,x_2x1​,x2​，然后取下述变换： h1=x1h2=x2+m(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{x}_{2}+\\boldsymbol{m}\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=x2​+m(x1​)​ 其中 mmm 为任意函数，这个变换称为“加性耦合层” ，这个变换的雅可比矩阵 [∂h∂x][\\frac{\\partial h}{\\partial x}][∂x∂h​] 是一个三角阵，且对角线元素全部为1，用分块矩阵表示为： [∂h∂x]=(IdO∂m∂x1ID−d)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathrm{I}_{d}} &amp; {\\mathrm{O}} \\\\ \\frac{\\partial m}{\\partial x_1} &amp; I_{D-d} \\end{array}\\right) [∂x∂h​]=(Id​∂x1​∂m​​OID−d​​) 同时这个变换也是可逆的，其逆变换为 x1=xhx2=h2−m(h1)\\begin{array}{l} {\\boldsymbol{x}_{1}=\\boldsymbol{x}_{h}} \\\\ {\\boldsymbol{x}_{2}=\\boldsymbol{h}_{2}-\\boldsymbol{m}\\left(\\boldsymbol{h}_{1}\\right)} \\end{array} x1​=xh​x2​=h2​−m(h1​)​ 满足了要求一和要求二，同时这个雅可比行列式的值为1，行列式的值的物理含义是体积，所以这个变换暗含了变换前后的体积不变性。我们注意到：该变换的第一部分是平凡的（恒等变换），因此需要对调I1和I2两组维度，再输入加和耦合层，并将这个过程重复若干次， 以达到信息充分混合的目的，如图： 因为该变换需要满足 zzz 和 xxx 的维度相同，这会产生很严重的唯独浪费问题，NICE在最后一层里引入了一个尺度变换对维度进行缩放： z=s⊗h(n)z=s \\otimes h^{(n)} z=s⊗h(n) 其中s=(s1,s2,...,sD)s=(s_1,s_2,...,s_D)s=(s1​,s2​,...,sD​)也是一个要优化的参数向量，这个 sss 向量能够识别每个维度的重要程度， sss 越小，这个维度越不重要，起到压缩流形的作用。这个尺度变换层的雅可比行列式就不是一了，而是： [∂z∂h(n)]=diag⁡(s)\\left[\\frac{\\partial z}{\\partial \\boldsymbol{h}^{(n)}}\\right]=\\operatorname{diag}(\\boldsymbol{s}) [∂h(n)∂z​]=diag(s) 他的行列式的值为 ∏isi\\prod_{i} s_{i}∏i​si​,于是最后的对数似然为： log⁡q(x)∼−12∥s⊗f(x)∥2+∑ilog⁡si\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2}\\|\\boldsymbol{s} \\otimes \\boldsymbol{f}(\\boldsymbol{x})\\|^{2}+\\sum_{i} \\log \\boldsymbol{s}_{i} logq(x)∼−21​∥s⊗f(x)∥2+i∑​logsi​ 这个尺度变换实际上是将先验分布 q(z)q(z)q(z) 的方差也作为训练参数，方差越小，说明这个维度的“弥散”越小，若方差为0，这一维的特征就恒为均值，于是流行减小一维。 我们写出带方差的正态分布： q(z)=1(2π)D/2∏i=1Dσiexp⁡(−12∑i=1Dzi2σi2)q(z)=\\frac{1}{(2 \\pi)^{D / 2} \\prod_{i=1}^{D} \\sigma_{i}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{z_{i}^{2}}{\\sigma_{i}^{2}}\\right) q(z)=(2π)D/2∏i=1D​σi​1​exp(−21​i=1∑D​σi2​zi2​​) 将 z=f(x)z=f(x)z=f(x) 代入，并取对数，类似得： log⁡q(x)∼−12∑i=1Dfi2(x)σi2−∑i=1Dlog⁡σi\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{\\boldsymbol{f}_{i}^{2}(\\boldsymbol{x})}{\\boldsymbol{\\sigma}_{i}^{2}}-\\sum_{i=1}^{D} \\log \\boldsymbol{\\sigma}_{i} logq(x)∼−21​i=1∑D​σi2​fi2​(x)​−i=1∑D​logσi​ 与之前那个公式对比，就有 si=1/σis_i=1/\\sigma_isi​=1/σi​ ，所以尺度变换层等价于将先验分布的方差作为训练参数，若方差足够小，则维度减一，暗含了降维的可能。 REALNVP NICE构思巧妙，但在实验部分只是采取了简单的加性耦合层和将全连接层进行简单的堆叠，并没有使用卷积。REALNVP一般化了耦合曾，并在耦合模型中引入了卷积层，使得模型可以更好地处理图像问题。论文里还引入了一个多尺度结构来处理维度浪费问题。 将加性耦合层换成仿射耦合层： h1=x1h2=s(x1)⊗x2+t(x1)(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{s}\\left(\\boldsymbol{x}_{1}\\right) \\otimes \\boldsymbol{x}_{2}+t\\left(\\boldsymbol{x}_{1}\\right)\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=s(x1​)⊗x2​+t(x1​)(x1​)​ 仿射耦合层的雅可比行列式仍然是一个对角阵 [∂h∂x]=(IdO[∂m∂x1]s)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathbb{I}_{d}} &amp; {\\mathbb{O}} \\\\ {\\left[\\frac{\\partial m}{\\partial x_{1}}\\right]} &amp; {s} \\end{array}\\right) [∂x∂h​]=(Id​[∂x1​∂m​]​Os​) 雅可比行列式的值不再是1，没有保持变换前后的体积不变。 在NICE中，通过交错的方式来混合信息流(直接反转原来的向量)，在REALNVP中发现：随机打乱维度可以使信息混合的更加充分。 引入卷积层：使用卷积的条件是具有局部相关性，因此指定向量的打乱和重排都是在channel维度上进行，在height和width维度上进行卷积。对通道的分割论文里还提出棋盘式分割的策略，但较为复杂，对模型的提升也不大，因此在GLOW中被舍弃了。 一般的图像通道数只有三层，MNIST等灰度图只有一层，因此REALNVP引入了squeeze操作来增加通道数。 其思想很简单：直接 reshape，但 reshape 时局部地进行。具体来说，假设原来图像为 h×w×c 大小，前两个轴是空间维度，然后沿着空间维度分为一个个 2×2×c 的块（ 2 可以自定义），然后将每个块直接 reshape 为 1×1×4c，最后变成了 h/2×w/2×4c。 REALNVP中还引入了一个多尺度结构： 最终的输出 z1,z3,z5z_1,z_3,z_5z1​,z3​,z5​ 怎么取？ p(z1,z3,z5)=p(z1∣z3,z5)p(z3∣z5)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{3}, z_{5}\\right) p\\left(z_{3} | z_{5}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z3​,z5​)p(z3​∣z5​)p(z5​) 由于 z3,z5z_3,z_5z3​,z5​ 是由 z2z_2z2​ 完全决定的，z5z_5z5​ 也是由 z4z_4z4​ 完全决定的，因此条件部分可以改为： p(z1,z3,z5)=p(z1∣z2)p(z3∣z4)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{2}\\right) p\\left(z_{3} | z_{4}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z2​)p(z3​∣z4​)p(z5​) RealNVP 和 Glow 假设右端三个概率分布都是正态分布，类似VAE， p(z1∣z2)p(z_1|z_2)p(z1​∣z2​) 的均值方差由 z2z_2z2​ 算出来，p(z3∣z4)p(z_3|z_4)p(z3​∣z4​) 的均值方差由 z4z_4z4​ 算出来，p(z5)p(z_5)p(z5​) 的均值方差直接学习出来。这相当于做了变量代换： z^1=z1−μ(z2)σ(z2),z^3=z3−μ(z4)σ(z4),z^5=z5−μσ\\hat{z}_{1}=\\frac{z_{1}-\\mu\\left(z_{2}\\right)}{\\sigma\\left(z_{2}\\right)}, \\quad \\hat{z}_{3}=\\frac{z_{3}-\\mu\\left(z_{4}\\right)}{\\sigma\\left(z_{4}\\right)}, \\quad \\hat{z}_{5}=\\frac{z_{5}-\\mu}{\\sigma} z^1​=σ(z2​)z1​−μ(z2​)​,z^3​=σ(z4​)z3​−μ(z4​)​,z^5​=σz5​−μ​ 然后认为 [z^1,z^3,z^5][\\hat{z}_1,\\hat{z}_3,\\hat{z}_5][z^1​,z^3​,z^5​]服从标准正态分布。类似NICE，这三个变换会导致一个非1的雅可比行列式，也就是往loss中加入 Σi=1Dlog⁡σi\\Sigma_{i=1}^{D} \\log \\sigma_{i}Σi=1D​logσi​ 这一项。 多尺度结构相当于抛弃了 p(z)p(z)p(z) 是标准正态分布的直接假设，而采用了一个组合式的条件分布，这样尽管输入输出的总维度依然一样，但是不同层次的输出地位已经不对等了，模型可以通过控制每个条件分布的方差来抑制维度浪费问题（极端情况下，方差为 0，那么高斯分布坍缩为狄拉克分布，维度就降低 1），条件分布相比于独立分布具有更大的灵活性。而如果单纯从 loss 的角度看，多尺度结构为模型提供了一个强有力的正则项。 GLOW 效果好的令人惊叹的生成模型： 改变图像属性 采样展示 潜在空间的插值 总体来说，GLOW引入1*1可逆卷积来代替通道维度的打乱和重排操作，并对REALNVP的原始模型做了简化和规范。 向量之间的元素置换操作可以用简单的行变换矩阵来操作： (badc)=(0100100000010010)(abcd)\\left(\\begin{array}{l} {b} \\\\ {a} \\\\ {d} \\\\ {c} \\end{array}\\right)=\\left(\\begin{array}{llll} {0} &amp; {1} &amp; {0} &amp; {0} \\\\ {1} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {1} \\\\ {0} &amp; {0} &amp; {1} &amp; {0} \\end{array}\\right)\\left(\\begin{array}{l} {a} \\\\ {b} \\\\ {c} \\\\ {d} \\end{array}\\right) ⎝⎜⎜⎛​badc​⎠⎟⎟⎞​=⎝⎜⎜⎛​0100​1000​0001​0010​⎠⎟⎟⎞​⎝⎜⎜⎛​abcd​⎠⎟⎟⎞​ GLOW中用一个更一般的矩阵 WWW 来代替这个置换矩阵 h=xWh=xW h=xW 这个变换的雅可比矩阵就是det(W)det(W)det(W)，因此需要将 −log∣det(W)∣-log|det(W)|−log∣det(W)∣ 加入到loss中，WWW 的初始选择要求可逆，不引入loss，因此选为随即正交阵。 这个变换引入了 det(W)det(W)det(W) 的计算问题，GLOW中逆用LU分解克服了这个问题，若 W=PLUW=PLUW=PLU (其中P是一个置换矩阵),则 log⁡∣det⁡W∣=∑log⁡∣diag⁡(U)∣\\log |\\operatorname{det} W|=\\sum \\log |\\operatorname{diag}(U)| log∣detW∣=∑log∣diag(U)∣ 这就是GLOW中给出的技巧：先随机生成一个正交矩阵，然后做 LULULU 分解，得到 P,L,UP,L,UP,L,U，固定 P，也固定 U 的对角线的正负号，然后约束 L 为对角线全 1 的下三角阵，U 为上三角阵，优化训练 L,U 的其余参数。 整个GLOW模型如下： 对比 比较反转、打乱和1*1逆卷积的loss： 缺点 模型庞大，参数量极大，NICE模型在MNIST数据集上的训练参数就大概有两千万个。 再贴两个Glow模型在Gayhub Github上的issue感受下： 256*256的高清人脸生成，用一块GPU训练的话，大概要一年…… 一图对比GAN，VAE和FLOW 参考文献 Variational Inference: A Unified Framework of Generative Models and Some Revelations Tutorial on Variational Autoencoders 用变分推断统一理解生成模型（VAE、GAN、AAE、ALI） NICE: Non-linear Independent Components Estimation NOTE_FLOW Glow: Generative Flow with Invertible 1×1 Convolutions 细水长flow之NICE：流模型的基本概念与实现 RealNVP与Glow：流模型的传承与升华 Density estimation using Real NVP","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"gan","slug":"gan","permalink":"/tags/gan/"},{"name":"vae","slug":"vae","permalink":"/tags/vae/"},{"name":"flow","slug":"flow","permalink":"/tags/flow/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"shell脚本","slug":"shell脚本","date":"2020-02-10T05:32:00.000Z","updated":"2020-03-09T13:27:53.839Z","comments":true,"path":"2020/02/10/shell脚本/","link":"","permalink":"/2020/02/10/shell脚本/","excerpt":"","text":"# 指定解释器 #！/bin/bash # 向窗口输出文本 echo \"Hello world!\" printf \"Hello world!\" # for循环示例,使用变量要加$符号 for file in `ls /etc` do echo \"${file}\" done # 双引号和单引号 # 双引号中可以有变量，单引号中的变量是无效的 # if-else语句 if condition1 then command1 elif condition2 then commed2 else command3 fi","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"shell","slug":"shell","permalink":"/tags/shell/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"emacs基本操作","slug":"Emacs笔记","date":"2020-02-10T03:16:00.000Z","updated":"2020-03-09T13:27:39.923Z","comments":true,"path":"2020/02/10/Emacs笔记/","link":"","permalink":"/2020/02/10/Emacs笔记/","excerpt":"","text":"Emacs基本操作 C = Ctrl, M = Alt 光标移动 C-v 向下翻页 M-v 向上翻页 C-b 向左(back) C-f 向右(forward) C-n 向下(next) C-p 向上(previous) M-b 上一个单词 M-f 下一个单词 C-a 行首 C-e 行尾 M-a 句首 M-e 句尾 M-&lt; 文件头 M-&gt; 文件尾 M-g g 跳到某一行 选择区域 C-@ 标记 删除剪切复制粘贴 C-d 向后删除(delele) C-k 删掉光标后至行尾 M-w 复制区域 C-w 剪切/删除区域 C-y 粘贴 M-y 滚动选择粘贴内容 查找替换 C-s 向前查找 C-s C-r 向后查找 M-% 替换 文件操作 C-x C-f 打开文件(find) C-x C-s 保存文件(save) C-x C-w 另存为(write) C-k 关闭文件 窗口操作 C-x b 切换文件 C-x 1 关闭其它窗口 C-x 2/C-x 3 打开其它窗口 C-x o 跳到另一个窗口(other) 其它 C-/ 撤销 M-$ 拼写检查 M-x 输入命令 C-g 取消命令","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"emacs","slug":"emacs","permalink":"/tags/emacs/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"科学上网","slug":"科学上网","date":"2020-02-09T03:19:00.000Z","updated":"2020-03-09T13:39:13.379Z","comments":true,"path":"2020/02/09/科学上网/","link":"","permalink":"/2020/02/09/科学上网/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Please enter the password to read the blog. Incorrect Password! No content to display! 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","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"},{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"},{"name":"科学上网","slug":"科学上网","permalink":"/tags/科学上网/"},{"name":"翻墙","slug":"翻墙","permalink":"/tags/翻墙/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用tensorboard和visdom","slug":"服务器使用tensorboard和visdom","date":"2020-01-28T03:27:21.351Z","updated":"2020-03-09T13:25:17.135Z","comments":true,"path":"2020/01/28/服务器使用tensorboard和visdom/","link":"","permalink":"/2020/01/28/服务器使用tensorboard和visdom/","excerpt":"","text":"服务器使用tensorboard和visdom 以tensorboard为例： 创建容器时开放6006端口 # 运行容器时将服务器docker容器的6006端口暴漏到自己主机ip下的16006端口(可自己指定) $ docker run -p &lt;ip&gt;:16006:6006 -it -v /data:/workspace/data --runtime=nvidia --net=host --name=temp /bin/bash 或者 # 在连接ssh时，将docker容器中的6006端口重新定向到自己机器上 $ ssh -p 1001 -L 16006:&lt;ip&gt;:6006 root@10.7.60.40","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"},{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用详细教程","slug":"服务器使用说明","date":"2020-01-24T03:38:00.000Z","updated":"2020-03-09T13:19:27.967Z","comments":true,"path":"2020/01/24/服务器使用说明/","link":"","permalink":"/2020/01/24/服务器使用说明/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Please enter the password to read the blog. Incorrect Password! No content to display! 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","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"docker基本使用","slug":"docker使用","date":"2020-01-19T12:48:00.000Z","updated":"2020-03-09T13:17:56.659Z","comments":true,"path":"2020/01/19/docker使用/","link":"","permalink":"/2020/01/19/docker使用/","excerpt":"","text":"Docker基本使用 docker和nvidia-docker 安装 略 安装后查看安装信息和测试docker是否安装正确 $ sudo docker --version $ sudo docker run hello-world # 验证nvidia-docker $ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi docker hub 链接:dockerhub 在dockerhub上搜索自己需要的镜像 选择自己要用的镜像，点进去，点Tags会出现不同的版本 复制命令，在终端中执行，就可以将镜像下载到本地 docker基本命令 # 查看主机下有多少镜像 $ sudo docker images # 删除镜像 $ sudo docker rmi &lt;image_id&gt; # 查看主机下的容器,不加-a表示查看运行中的容器 $ sudo docker ps -a # 启动、重启和停止容器 $ sudo docker start/restart/stop &lt;container_id&gt; # 进入容器 $ sudo docker attach &lt;container_id&gt; # 删除容器(需要先停止容器) $ sudo docker rm &lt;container_id&gt; ctrl+d退出并停止容器，ctrl+p+q退出但不停止容器 根据镜像新建并启动容器 $ sudo docker run -it [options] &lt;image_id&gt; bash 说明：-it为为容器分配一个输入终端，以交互式模式运行,bash为调用镜像里的bash 可选选项： --name 为容器指定名字 -p 端口映射 -v 给容器挂载存储卷 --net 指定容器网络 --runtime=nvidia 可调用gpu 例如： $ sudo docker run -it -p 8022:22 --name=tensorflow --runtime=nvidia -v /home/yu/code:/home --net=host ufoym/deepo bash 删除所有镜像和容器 $ sudo docker rmi `sudo docker images -q` $ sudo docker rm `sudo docker ps -a -q` 导出容器和保存加载镜像 容器导出为镜像 # 容器导出为镜像 $ docker commit &lt;continer_name&gt; &lt;image_name&gt; # 镜像导出为文件 $ docker save image_name&gt; /dir/name.tar # 文件导入为镜像 $ docker load &lt; /dir/filename docker和宿主机的文件拷贝 $ docker cp &lt;container_name&gt;:/dir/filename /hostdir/ $ docker cp /hostdir/filename &lt;container_name&gt;:/dir 不管容器有没有启动，拷贝命令都会生效","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"docker","slug":"docker","permalink":"/tags/docker/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"OpenPAI安装记录","slug":"OpenPai安装记录","date":"2020-01-16T02:48:00.000Z","updated":"2020-03-09T13:40:39.587Z","comments":true,"path":"2020/01/16/OpenPai安装记录/","link":"","permalink":"/2020/01/16/OpenPai安装记录/","excerpt":"","text":"OpenPAI安装记录 环境准备 一台master主机和多台worker主机，一台维护机 所有节点不要安装CUDA驱动，具有统一的登录账户和密码 开启ssh功能和ntp功能(互相访问，时间同步) 部署过程 安装docker-ce $ sudo apt-get -y install docker.io $ sudo docker pull docker.io/openpai/dev-box:v0.14.0 运行dev-box $ sudo docker run -itd \\ -e COLUMNS=$COLUMNS -e LINES=$LINES -e TERM=$TERM \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /pathConfiguration:/cluster-configuration \\ -v /hadoop-binary:/hadoop-binary \\ --pid=host \\ --privileged=true \\ --net=host \\ --name=dev-box \\ docker.io/openpai/dev-box:v0.14.0 登录dev-box $ sudo docker exec -it dev-box /bin/bash $ cd /pai/deployment/quick-start/ 修改配置信息 $ cp quick-start-example.yaml quick-start.yaml $ vim quick-start.yaml 修改内容： machines: - &lt;ip-of-master&gt; - &lt;ip-of-worker1&gt; - &lt;ip-of-worder2&gt; ssh-username: &lt;username&gt; ssh-password: &lt;password&gt; 生成OepnPai配置文件 $ cd /pai $ python paictl.py config generate -i /pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f $ cd ~/pai-config/ 修改kubernetes-configuration.yaml 将docker-registry替换为国内镜像库 docker-registry: docker.io/mirrorgooglecontainers 修改layout.yaml 修改自己机器的配置信息 machine-sku: GENERIC: mem: 256G gpu: type: TITAN V count: 1 cpu: vcore: 4 os: ubuntu16.04 Worker1: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 Worker2: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 修改services-configuration.yaml 解除common和data-path两个字段的注释，将data-path赋值到真实位置，作为服务数据存储路径 cluster: common: # cluster-id: pai-example # # # HDFS, zookeeper data path on your cluster machine. data-path: \"/data\" tag字段修改为真实版本 v014.0 可修改cluster-id,后面会用到 修改rest-server下的用户名和密码，作为登录平台的账户密码 指定显卡驱动版本，不指定的话默认安装384.11，这个驱动是不支持图灵核心显卡的，安装到后面会出现’nvidia-drm’ not found 错误，驱动版本只能从注释里的版本选择 drivers: set-nvidia-runtime: false # You can set drivers version here. If this value is miss, default value will be 384.111 # Current supported version list # 384.111 # 390.25 # 410.73 version: \"410.73\" 部署Kubernetes http::9090查看进度 $ cd /pai $ python paictl.py cluster k8s-bootup -p ~/pai-config 更改配置文件到kubernetes $ cd /pai python paictl.py config push -p ~/pai-config/ -c ~/.kube/config 若报错，卸载openpai组件和ks组件，检查之前的配置文件，重新安装 $ python paictl.py service [delete|start|stop] -c ~/.kube/config [-n name] # 卸载openpai组件 $ python paictl.py service delete -c ~/.kube/config # 卸载k8s组件 $ python paictl.py cluster k8s-clean -p ~/pai-config/ 启动Openpai $ python paictl.py service start -c ~/.kube/config 界面 http://&lt;master-ip&gt;:9090 http://&lt;master-ip&gt;:80 Reference https://github.com/kangapp/openPAI https://github.com/microsoft/pai https://zhuanlan.zhihu.com/p/64061072","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"openpai","slug":"openpai","permalink":"/tags/openpai/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"NVIDIA,CUDA,CUDNN,Anaconda","slug":"Ubuntu命令记录","date":"2020-01-11T12:32:00.000Z","updated":"2020-03-09T13:16:03.363Z","comments":true,"path":"2020/01/11/Ubuntu命令记录/","link":"","permalink":"/2020/01/11/Ubuntu命令记录/","excerpt":"","text":"NVIDIA,CUDA,CUDNN ppa安装NVIDIA驱动 $ sudo add-apt-repository ppa:graphics-drivers/ppa $ sudo apt-get update $ ubuntu-drivers devices $ sudo apt-get install nvidia-driver-xxx 自动安装NVIDIA驱动 # 卸载残余驱动 sudo apt-get --purge remove \"*nvidia*\" # 查看推荐驱动版本 ubuntu-drivers devices # 自动安装 sudo ubuntu-drivers autoinstall .deb安装CUDA 下载deb文件 $ sudo dpkg -i cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb $ sudo apt-get update $ sudo apt-get install cuda 安装CUDNN 下载符合自己cuda版本的cudnn 安装cudnn 安装过程实际是将cudnn的头文件复制到CUDA的头文件目录里 $ sudo cp cuda/include/* /usr/local/cuda-10.0/include/ $ sudo cp cuda/lib64/* /usr/local/cuda-10.0/lib64/ # 添加可执行权限 $ sudo chmod +x /usr/local/cuda-10.0/include/cudnn.h $ sudo chmod +x /usr/local/cuda-10.0/lib64/libcudnn* 检验 $ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 指定运行程序使用的GPU 在程序中添加 import os os.environ['CUDA_VISIBLE_DEVICES]='0' 或者在终端中 $ CUDA_VISIBLE_DEVICES=0 python main.py 命令行安装Anaconda $ wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh $ bash Anaconda3-5.0.1-Linux-x86_64.sh # 添加环境变量，可选 $ echo 'export PATH=\"~/anaconda3/bin:$PATH\"' &gt;&gt; ~/.bashrc $ source .bashrc","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"nvidia","slug":"nvidia","permalink":"/tags/nvidia/"},{"name":"cuda","slug":"cuda","permalink":"/tags/cuda/"},{"name":"cudnn","slug":"cudnn","permalink":"/tags/cudnn/"},{"name":"anaconda","slug":"anaconda","permalink":"/tags/anaconda/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"安利","slug":"安利区","date":"2020-01-11T02:48:00.000Z","updated":"2020-03-09T13:05:16.903Z","comments":true,"path":"2020/01/11/安利区/","link":"","permalink":"/2020/01/11/安利区/","excerpt":"","text":"一些软件 softdownloader: 一款布局清爽的下载工具 Fences: 桌面管理工具 copytranslator: 一款翻译软件 天若OCR文字识别 Snipaste: 一款简洁的截图和贴图软件 Mathpix: 每个月50次识别次数，快速识别图片中的公式，转化为 LaTeX\\LaTeXLATE​X 格式 Windows 的内置 Liunx 系统，支持 Linux 命令 TexStudio + Texlive 编辑LaTeX\\LaTeXLATE​X公式 Axmath: 个人感觉优于mathtype, Office中的公式编辑器 亿寻:百度云破解限速 Google浏览器插件 momentum: 浏览器壁纸标签页 LastPass: 密码记录插件 SwitchyOmega: 浏览器代理插件 OneTab: 标签页管理 一键管理扩展: 管理所有插件 AdblockPlus: 强烈安利，拦截广告 Read Viewer: 网页阅读模式 划词翻译 Tampermonkey:传说中的油猴","categories":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}],"tags":[{"name":"安利","slug":"安利","permalink":"/tags/安利/"}],"keywords":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-01-11T02:16:02.000Z","updated":"2020-03-09T13:37:22.679Z","comments":true,"path":"2020/01/11/hello-world/","link":"","permalink":"/2020/01/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]}]}