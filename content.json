{"meta":{"title":"鱼摆摆的blog","subtitle":null,"description":"自童年起，我就独自一人，照顾着历代星辰","author":"鱼摆摆","url":""},"pages":[{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"client/index.html","permalink":"/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"donate/index.html","permalink":"/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"comment/index.html","permalink":"/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"lab/index.html","permalink":"/lab/index.html","excerpt":"","text":"sakura主题 balabala","keywords":"Lab实验室"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-03-09T12:29:18.823Z","comments":true,"path":"links/index.html","permalink":"/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2020-01-11T15:14:28.000Z","updated":"2020-03-09T12:35:33.243Z","comments":false,"path":"music/index.html","permalink":"/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"}],"posts":[{"title":"cProfile时间分析","slug":"Python 时间分析","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:45.775Z","comments":true,"path":"2021/02/22/Python 时间分析/","link":"","permalink":"/2021/02/22/Python 时间分析/","excerpt":"","text":"Python 时间分析 import cProfile import pstats import os # 性能分析装饰器定义 def do_cprofile(filename): def wrapper(func): def profiled_func(*args, **kwargs): # Flag for do profiling or not. DO_PROF = os.getenv(\"PROFILING\") if DO_PROF: profile = cProfile.Profile() profile.enable() result = func(*args, **kwargs) profile.disable() # Sort stat by internal time. sortby = \"tottime\" ps = pstats.Stats(profile).sort_stats(sortby) ps.dump_stats(filename) else: result = func(*args, **kwargs) return result return profiled_func return wrapper @do_cprofile('./main.prof') main() $ pip install gprof2dot $ gprof2dot -f pstats main.prof | dot -Tpng -o main.png gprof项目地址","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"java-学习笔记","slug":"java学习笔记","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:55.119Z","comments":true,"path":"2021/02/22/java学习笔记/","link":"","permalink":"/2021/02/22/java学习笔记/","excerpt":"","text":"Hello world public class HelloWorld { /* 第一个Java程序 * 它将输出字符串 Hello World */ public static void main(String[] args) { System.out.println(\"Hello World\"); // 输出 Hello World } } 参数说明： 注意注释方式 public：程序的访问权限，表示任何场合可以被引用 static：方法是静态的，不依赖于类的对象，是属于类的，在类加载的时候main()方法也会加载到内存中 void:main()：方法不需要返回值 String[] args： 从控制台接收参数，参数为数组形式 例如：需要传递参数时 public class Test { public static void main(String []args) { System.out.println(args[0]); System.out.println(args[1]); System.out.println(args[2]); } } $ javac Test.java $ java Test aaa bbb ccc aaa bbb ccc 基础语法 枚举 class FreshJuice { enum FreshJuiceSize{ SMALL, MEDIUM , LARGE } FreshJuiceSize size; } public class FreshJuiceTest { public static void main(String[] args){ FreshJuice juice = new FreshJuice(); juice.size = FreshJuice.FreshJuiceSize.MEDIUM ; } } 命名规范和注释规范 1、 项目名全部小写 2、 包名全部小写 3、 类名首字母大写，如果类名由多个单词组成，每个单词的首字母都要大写。如：public class MyFirstClass{} 4、 变量名、方法名首字母小写，如果名称由多个单词组成，每个单词的首字母都要大写。如： int index=0; public void toString(){} 5、 常量名全部大写 A 如： public static final String GAME_COLOR=&quot;RED&quot;; 6、所有命名规则必须遵循以下规则： 1)、名称只能由字母、数字、下划线、$符号组成 2)、不能以数字开头 3)、名称不能使用JAVA中的关键字。 4)、坚决不允许出现中文及拼音命名。 二、注释规范 1、类注释 在每个类前面必须加上类注释，注释模板如下： /** * Copyright (C), 2006-2010, ChengDu Lovo info. Co., Ltd. * FileName: Test.java * 类的详细说明 * * @author 类创建者姓名 * @Date 创建日期 * @version 1.00 */ 2、属性注释 在每个属性前面必须加上属性注释，注释模板如下： /** 提示信息 */ private String strMsg = null; 3、方法注释 在每个方法前面必须加上方法注释，注释模板如下： /** * 类方法的详细使用说明 * * @param 参数1 参数1的使用说明 * @return 返回结果的说明 * @throws 异常类型.错误代码 注明从此类方法中抛出异常的说明 */ 4、构造方法注释 在每个构造方法前面必须加上注释，注释模板如下： /** * 构造方法的详细使用说明 * * @param 参数1 参数1的使用说明 * @throws 异常类型.错误代码 注明从此类方法中抛出异常的说明 */ 5、方法内部注释 在方法内部使用单行或者多行注释，该注释根据实际情况添加。 如： //背景颜色 Color bgColor = Color.RED 对象、类、方法 类 public class Dog { \\\\类 String breed; \\\\变量(成员变量) int size; String colour; int age; void eat() { \\\\方法 } void run() { } void sleep(){ } void name(){ } } 一个类可以有一个或多个构造方法，在创建对象时将调用构造方法，构造方法必须与类同名 public class Puppy{ public Puppy(){ } public Puppy(String name){ // 这个构造器仅有一个参数：name } } 对象 创建对象需要 声明、实例化、初始化 public class Puppy{ public Puppy(String name){ //这个构造器仅有一个参数：name System.out.println(\"小狗的名字是 : \" + name ); } public static void main(String[] args){ // 下面的语句将创建一个Puppy对象 Puppy myPuppy = new Puppy( \"tommy\" ); //声明：Puppy类, 实例化：myPuppy， 初始化：new Puppy(\"tommy\") } } 访问实例变量和方法 public class Puppy{ int puppyAge; public Puppy(String name){ // 这个构造器仅有一个参数：name System.out.println(\"小狗的名字是 : \" + name ); } public void setAge( int age ){ \\\\方法，无返回值 puppyAge = age; } public int getAge( ){ //方法，返回值为整数 System.out.println(\"小狗的年龄为 : \" + puppyAge ); return puppyAge; } public static void main(String[] args){ /* 创建对象 */ Puppy myPuppy = new Puppy( \"tommy\" ); /* 通过方法来设定age */ myPuppy.setAge( 2 ); /* 调用另一个方法获取age */ myPuppy.getAge( ); /*你也可以像下面这样访问成员变量 */ System.out.println(\"变量值 : \" + myPuppy.puppyAge ); } } 结果： 小狗的名字是 : tommy 小狗的年龄为 : 2 变量值 : 2 小狗的名字是 : tommy 小狗的年龄为 : 2 变量值 : 2 源文件声明规则 一个源文件中只能有一个 public 类 一个源文件可以有多个非 public 类 源文件的名称应该和 public 类的类名保持一致。例如：源文件中 public 类的类名是 Employee，那么源文件应该命名为Employee.java。 如果一个类定义在某个包中，那么 package 语句应该在源文件的首行。 如果源文件包含 import 语句，那么应该放在 package 语句和类定义之间。如果没有 package 语句，那么 import 语句应该在源文件中最前面。 import 语句和 package 语句对源文件中定义的所有类都有效。在同一源文件中，不能给不同的类不同的包声明。 数据类型 数据类型 默认值 byte 0 short 0 int 0 long 0L float 0.0f double 0.0d char ‘u0000’ String (or any object) null boolean false 类型转换： String s = \"1\"; byte b = Byte.parseByte( s ); short t = Short.parseShort( s ); int i = Integer.parseInt( s ); long l = Long.parseLong( s ); Float f = Float.parseFloat( s ); Double d = Double.parseDouble( s );","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"java","slug":"java","permalink":"/tags/java/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"位运算","slug":"位运算","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:47.219Z","comments":true,"path":"2021/02/22/位运算/","link":"","permalink":"/2021/02/22/位运算/","excerpt":"","text":"运算技巧 乘2 a * 2 == a &lt;&lt; 1，正负通用 -1024 = 0b11111111111111111111110000000000 -2048 = 0b11111111111111111111100000000000 乘2加1 a * 2 + 1 == a &lt;&lt; 1 | 1，正负通用 -1024 = 0b11111111111111111111110000000000 -1023 = 0b11111111111111111111110000000001 除2 a / 2 == a &gt;&gt; 1，不能整除时向下取整 取相反数 ~a + 1，补码表示法 1024 = 0b00000000000000000000010000000000 -1024 = 0b11111111111111111111110000000000 ~1024 = 0b11111111111111111111101111111111 ~-1024 = 0b00000000000000000000001111111111 消去最低位1 a &amp; (a - 1) 0b00010110 -1 = 0b00010101 0b00011000 -1 = 0b00010111 0b11111000(-8) -1 = 0b11110111(-9) 获取最低位1 a &amp; (-a) = a &amp; (~a + 1) 22 = 0b00010110 ~22 = 0b11101001 +1 = 0b11101010(-22) &amp; = 0b00000010 判断 判断奇偶 a &amp; 1 判断是否是2的幂次方 a &amp; (a - 1) == 0 二进制枚举子集 n = 0b1011 sub = n while sub: print(bin(sub)) sub = (sub - 1) &amp; n 0b1011 0b1010 0b1001 0b1000 0b11 0b10 0b1 二进制连续1的个数 n = 0b11101111 ans = 0 while n: ans += 1 print(bin(n)) n = n &amp; n &gt;&gt; 1 0b11101111 0b1100111 0b100011 0b1","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python/java","slug":"python-java","permalink":"/tags/python-java/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"深度学习复习笔记","slug":"深度学习复习笔记","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:49.587Z","comments":true,"path":"2021/02/22/深度学习复习笔记/","link":"","permalink":"/2021/02/22/深度学习复习笔记/","excerpt":"","text":"指标 TP（true positive，真正）: 预测为正，实际为正 FP（false positive，假正）: 预测为正，实际为负 TN（true negative，真负）：预测为负，实际为负 FN（false negative，假负）: 预测为负，实际为正 ACC（accuracy，准确率）：ACC=(TP+TN)/(TP+TN+FN+FP)ACC = (TP+TN)/(TP+TN+FN+FP)ACC=(TP+TN)/(TP+TN+FN+FP) P（precision精确率、精准率、查准率P=TP/(TP+FP)P = TP/ (TP+FP)P=TP/(TP+FP) R（recall，召回率、查全率）：$ R = TP/ (TP+FN)$ TPR（true positive rate，，真正类率同召回率、查全率）：TPR=TP/(TP+FN)TPR = TP/ (TP+FN)TPR=TP/(TP+FN) 注：Recall = TPR FPR（false positive rate，假正类率）：FPR=FP/(FP+TN)FPR =FP/ (FP+TN)FPR=FP/(FP+TN) TNR (true negative rate，真负类率) ：TNR=TN/(FP+TN)TNR = TN/(FP+TN)TNR=TN/(FP+TN) F-Score: F−Score=(1+β2)×(P×R)/(β2×(P+R))=2×TP/(2×TP+FP+FN)F-Score = (1+β^2) \\times (P\\times R) / (β^2 \\times (P+R)) = 2 \\times TP/(2 \\times TP + FP + FN)F−Score=(1+β2)×(P×R)/(β2×(P+R))=2×TP/(2×TP+FP+FN) 当β=1是，F1−score=2×P×R/(P+R)F1-score = 2\\times P\\times R/(P+R)F1−score=2×P×R/(P+R) P-R曲线（precision-recall，查准率-查全率曲线） ROC曲线（receiver operating characteristic，接收者操作特征曲线）横轴：负正类率(false postive rate FPR) 纵轴：真正类率(true postive rate TPR) AUC（area under curve）值 正则项 https://www.cnblogs.com/maybe2030/p/9231231.html L1范数相当于加入了一个Laplacean先验； L2范数相当于加入了一个Gaussian先验。 DropOut 概率丢弃部分神经元 BN 把每神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，避免因为激活函数导致的梯度弥散问题。 归一化和标准化： 归一化 x′=x−min⁡(x)max⁡(x)−min⁡(x)x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} x′=max(x)−min(x)x−min(x)​ 标准化 x′=x−μσx^{\\prime}=\\frac{x-\\mu}{\\sigma} x′=σx−μ​ 提升模型精度：归一化后，不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。 加速模型收敛：标准化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。： 如何防止过拟合？ 数据增广（Data Augmentation） 正则化（L0正则、L1正则和L2正则），也叫限制权值Weight-decay Dropout Early Stopping 简化模型 增加噪声 Bagging 贝叶斯方法 决策树剪枝 集成方法，随机森林 Batch Normalization 如何防止欠拟合？ 添加新特征 添加多项式特征 减少正则化参数 增加网络复杂度 使用集成学习方法，如Bagging Attention 将注意力放到重点信息上 优势： 参数少 模型复杂度跟 CNN、RNN 相比，复杂度更小，参数也更少。所以对算力的要求也就更小 速度快 Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。 效果好 Attention引入之前存在的问题：长距离的信息会被弱化 原理 query 和 key 进行相似度计算，得到权值 将权值进行归一化，得到直接可用的权重 将权重和 value 进行加权求和 类型 计算区域 1）Soft Attention，这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量可能会比较大一些。 2）Hard Attention，这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的） 3）Local Attention，这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。 Self-Attention: Attention (Q,K,V)=softmax⁡(QKTdk)V\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V Attention (Q,K,V)=softmax(dk​​QKT​)V Multi-head: 有不同的Q,K,VQ,K,VQ,K,V 表示，最后将其结果结合起来 所用信息 假设我们要对一段原文计算Attention，这里原文指的是我们要做attention的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息。 1）General Attention，这种方式利用到了外部信息，常用于需要构建两段文本关系的任务，query一般包含了额外信息，根据外部query对原文进行对齐。 比如在阅读理解任务中，需要构建问题和文章的关联，假设现在baseline是，对问题计算出一个问题向量q，把这个q和所有的文章词向量拼接起来，输入到LSTM中进行建模。那么在这个模型中，文章所有词向量共享同一个问题向量，现在我们想让文章每一步的词向量都有一个不同的问题向量，也就是，在每一步使用文章在该步下的词向量对问题来算attention，这里问题属于原文，文章词向量就属于外部信息。 2）Local Attention，这种方式只使用内部信息，key和value以及query只和输入原文有关，在self attention中，key=value=query。既然没有外部信息，那么在原文中的每个词可以跟该句子中的所有词进行Attention计算，相当于寻找原文内部的关系。 还是举阅读理解任务的例子，上面的baseline中提到，对问题计算出一个向量q，那么这里也可以用上attention，只用问题自身的信息去做attention，而不引入文章信息。 结构层次 结构方面根据是否划分层次关系，分为单层attention，多层attention和多头attention： 1）单层Attention，这是比较普遍的做法，用一个query对一段原文进行一次attention。 2）多层Attention，一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。 3）多头Attention，这是Attention is All You Need中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention： 最后再把这些结果拼接起来： 相似度计算方式 在做attention的时候，我们需要计算query和某个key的分数（相似度），常用方法有： 1）点乘：最简单的方法， 2）矩阵相乘： 3）cos相似度： 4）串联方式：把q和k拼接起来， 5）用多层感知机也可以： BN,LN,IN,GN,SN BN、LN、IN和GN这四个归一化的计算流程几乎是一样的，可以分为四步： 1.计算出均值 2.计算出方差 3.归一化处理到均值为0，方差为1 4.变化重构，恢复出这一层网络所要学到的分布 μB←1m∑i=1mxiσB2←1m∑i=1m(xi−μB)2x^i←xi−μBσB2+ϵyi←γx^i+β≡BNγ,β(xi)\\begin{aligned} \\mu_{\\mathcal{B}} &amp; \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} &amp; \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} &amp; \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} &amp; \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned} μB​σB2​xi​yi​​←m1​i=1∑m​xi​←m1​i=1∑m​(xi​−μB​)2←σB2​+ϵ​xi​−μB​​←γxi​+β≡BNγ,β​(xi​)​ 训练的时候，是根据输入的每一批数据来计算均值和方差，那么测试的时候，平均值和方差是怎么来的？ 对于均值来说直接计算所有训练时batch 均值的平均值；然后对于标准偏差采用每个batch 方差的无偏估计 E[x]←EB[μB]Var⁡[x]←mm−1EB[σB2]\\begin{aligned} \\mathrm{E}[x] &amp; \\leftarrow \\mathrm{E}_{\\mathcal{B}}\\left[\\mu_{\\mathcal{B}}\\right] \\\\ \\operatorname{Var}[x] &amp; \\leftarrow \\frac{m}{m-1} \\mathrm{E}_{\\mathcal{B}}\\left[\\sigma_{\\mathcal{B}}^{2}\\right] \\end{aligned} E[x]Var[x]​←EB​[μB​]←m−1m​EB​[σB2​]​ Batch Normalization： 1.BN的计算就是把每个通道的NHW单独拿出来归一化处理 2.针对每个channel我们都有一组γ,β，所以可学习的参数为2*C 3.当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局 Layer Normalizaiton： 1.LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响 2.常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理 Instance Normalization 1.IN的计算就是把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响 2.常用在风格化迁移，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理 Group Normalizatio 1.GN的计算就是把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW 2.GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C Switchable Normalization 1.将 BN、LN、IN 结合，赋予权重，让网络自己去学习归一化层应该使用什么方法 2.集万千宠爱于一身，但训练复杂 鞍点的定义和特点？ 一阶导为0，但不是极大极小值的点 该点处的黑塞矩阵为不定矩阵 常见的损失函数 https://zhuanlan.zhihu.com/p/58883095","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"/tags/深度学习/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"机器学习复习笔记","slug":"机器学习复习笔记","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:47.763Z","comments":true,"path":"2021/02/22/机器学习复习笔记/","link":"","permalink":"/2021/02/22/机器学习复习笔记/","excerpt":"","text":"逻辑回归（LR） 基本原理 逻辑回归（Logistic Regression，LR）也称为&quot;对数几率回归&quot;，又称为&quot;逻辑斯谛&quot;回归。 知识点提炼 分类，经典的二分类算法！ 逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。 Logistic 回归实际上是一种分类方法，主要用于两分类问题 回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。 逻辑回归的本质：极大似然估计 逻辑回归的激活函数：Sigmoid 逻辑回归的代价函数：交叉熵 逻辑回归的优缺点 优点： 1）速度快，适合二分类问题 2）简单易于理解，直接看到各个特征的权重 3）能容易地更新模型吸收新的数据 缺点： 对数据和场景的适应能力有局限性，不如决策树算法适应性那么强 逻辑回归中最核心的概念是 Sigmoid 函数，Sigmoid函数可以看成逻辑回归的激活函数。 下图是逻辑回归网络： 对数几率函数（Sigmoid）：y=σ(z)=11+e−zy = \\sigma (z) = \\frac{1}{1+e^{-z}}y=σ(z)=1+e−z1​ 通过对数几率函数的作用，我们可以将输出的值限制在区间[0，1]上，p(x) 则可以用来表示概率 p(y=1|x)，即当一个x发生时，y被分到1那一组的概率。 Regression 常规步骤 寻找h函数（即预测函数） 构造J函数（损失函数） 想办法（迭代）使得J函数最小并求得回归参数（θ） 函数h(x)的值有特殊的含义，它表示结果取1的概率，于是可以看成类1的后验估计。因此对于输入x分类结果为类别1和类别0的概率分别为： P(y=1│x;θ)=hθ (x) P(y=0│x;θ)=1-hθ (x) 代价函数 逻辑回归一般使用交叉熵作为代价函数。 交叉熵代价函数如下所示： J(w)=−l(w)=−∑i=1ny(i)ln(ϕ(z(i)))+(1−y(i))ln(1−ϕ(z(i)))J(w)=-l(w)=-\\sum_{i = 1}^n y^{(i)}ln(\\phi(z^{(i)})) + (1 - y^{(i)})ln(1-\\phi(z^{(i)})) J(w)=−l(w)=−i=1∑n​y(i)ln(ϕ(z(i)))+(1−y(i))ln(1−ϕ(z(i))) J(ϕ(z),y;w)=−yln(ϕ(z))−(1−y)ln(1−ϕ(z))J(\\phi(z),y;w)=-yln(\\phi(z))-(1-y)ln(1-\\phi(z)) J(ϕ(z),y;w)=−yln(ϕ(z))−(1−y)ln(1−ϕ(z)) 注：为什么要使用交叉熵函数作为代价函数，而不是平方误差函数？请参考：逻辑回归算法之交叉熵函数理解 避免陷入局部最优解 逻辑回归伪代码 初始化线性函数参数为1 构造sigmoid函数 重复循环I次 计算数据集梯度 更新线性函数参数 确定最终的sigmoid函数 输入训练（测试）数据集 运用最终sigmoid函数求解分类 为什么 LR 要使用 sigmoid 函数？ 1.广义模型推导所得 2.满足统计的最大熵模型 3.性质优秀，方便使用（Sigmoid函数是平滑的，而且任意阶可导，一阶二阶导数可以直接由函数值得到不用进行求导，这在实现中很实用） f′(x)=f(x)[1−f(x)]=F(f(x))f&#x27;(x) = f(x)[1 - f(x)]=F(f(x)) f′(x)=f(x)[1−f(x)]=F(f(x)) f′′(x)=f(x)[1−f(x)][1−2f(x)]f&#x27;&#x27;(x) = f(x)[1-f(x)][1-2f(x)] f′′(x)=f(x)[1−f(x)][1−2f(x)] 参考资料 为什么逻辑回归 模型要使用 sigmoid 函数 LR 可以用核函数么？ 不能，loss没法转为简易求解 SVM的loss函数为 12∥w∥2−∑i=1N(αiyi(wx+b−1))\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{N}\\left(\\alpha_{i} y_{i}(w x+b-1)\\right) 21​∥w∥2−i=1∑N​(αi​yi​(wx+b−1)) 对W,bW,bW,b求偏导置为0： −12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)+∑i=1Nαi-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i} −21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)+i=1∑N​αi​ 如果加上核函数的话，式一中的xxx替换为ϕ(x)\\phi(x)ϕ(x)，式二中的xixjx_ix_jxi​xj​替换为ϕ(xi)Tϕ(xj)\\phi(x_i)T\\phi(x_j)ϕ(xi​)Tϕ(xj​) 因为核函数满足条件 κ(xi⋅kj)=φ(xi)T⋅φ(xj)κ(xi·kj)=φ(xi)T·φ(xj)κ(xi⋅kj)=φ(xi)T⋅φ(xj) 可以简化计算。 为什么 LR 用交叉熵损失而不是平方损失？ 请参考：逻辑回归算法之交叉熵函数理解 避免陷入局部最优解 LR 能否解决非线性分类问题？ TODO 参考资料 逻辑斯蒂回归能否解决非线性分类问题？ LR为什么要离散特征？ 稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。 逻辑回归是处理线性问题还是非线性问题的？ 逻辑回归以线性回归为理论支持的,本质上是线性回归模型， 支持向量机（SVM） 基本原理 支持向量机（supporr vector machine，SVM）是一种二类分类模型，该模型是定义在特征空间上的间隔最大的线性分类器。间隔最大使它有区别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的最小化问题。 知识点提炼： SVM核函数 多项式核函数 高斯核函数 字符串核函数 SMO SVM损失函数 支持向量机的学习算法是求解凸二次规划的最优化算法。 支持向量机学习方法包含构建由简至繁的模型： 线性可分支持向量机 线性支持向量机 非线性支持向量机（使用核函数） 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization）学习一个线性的分类器，即线性可分支持向量机，又成为硬间隔支持向量机； 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization）也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机； 当训练数据不可分时，通过核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。 注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）—学习的对偶问题—软间隔最大化（引入松弛变量）—非线性支持向量机（核技巧）。 SVM的主要特点 （1）非线性映射-理论基础 （2）最大化分类边界-方法核心 （3）支持向量-计算结果 （4）小样本学习方法 （5）最终的决策函数只有少量支持向量决定，避免了“维数灾难” （6）少数支持向量决定最终结果—-&gt;可“剔除”大量冗余样本+算法简单+具有鲁棒性（体现在3个方面） （7）学习问题可表示为凸优化问题—-&gt;全局最小值 （8）可自动通过最大化边界控制模型，但需要用户指定核函数类型和引入松弛变量 （9）适合于小样本，优秀泛化能力（因为结构风险最小） （10）泛化错误率低，分类速度快，结果易解释 SVM为什么采用间隔最大化？ 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。 感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。 线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。 然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—&gt;求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。 为什么要将求解SVM的原始问题转换为其对偶问题？ 对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。） 自然引入核函数，进而推广到非线性分类问题 为什么SVM要引入核函数？ 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。 SVM核函数有哪些？ 线性（Linear）核函数：主要用于线性可分的情形。参数少，速度快。 多项式核函数 高斯（RBF）核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。 Sigmoid核函数 拉普拉斯（Laplac）核函数 注：如果feature数量很大，跟样本数量差不多，建议使用LR或者Linear kernel的SVM。如果feature数量较少，样本数量一般，建议使用Gaussian Kernel的SVM。 SVM如何处理多分类问题？ 一般有两种做法： 直接法：直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。 间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。 一对多：对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。 一对一：针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。 SVM中硬间隔和软间隔 硬间隔分类即线性可分支持向量机，软间隔分类即线性不可分支持向量机，利用软间隔分类时是因为存在一些训练集样本不满足函数间隔（泛函间隔）大于等于1的条件，于是加入一个非负的参数 ζ （松弛变量），让得出的函数间隔加上 ζ 满足条件。于是软间隔分类法对应的拉格朗日方程对比于硬间隔分类法的方程就多了两个参数（一个ζ ，一个 β），但是当我们求出对偶问题的方程时惊奇的发现这两种情况下的方程是一致的。下面我说下自己对这个问题的理解。 我们可以先考虑软间隔分类法为什么会加入ζ 这个参数呢？硬间隔的分类法其结果容易受少数点的控制，这是很危险的，由于一定要满足函数间隔大于等于1的条件，而存在的少数离群点会让算法无法得到最优解，于是引入松弛变量，从字面就可以看出这个变量是为了缓和判定条件，所以当存在一些离群点时我们只要对应给他一个ζi，就可以在不变更最优分类超平面的情况下让这个离群点满足分类条件。 综上，我们可以看出来软间隔分类法加入ζ 参数，使得最优分类超平面不会受到离群点的影响，不会向离群点靠近或远离，相当于我们去求解排除了离群点之后，样本点已经线性可分的情况下的硬间隔分类问题，所以两者的对偶问题是一致的。 手撕SVM 参考链接 LR 与 SVM的区别和联系 相同点 第一，LR和SVM都是分类算法。 第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。 第三，LR和SVM都是监督学习算法。 第四，LR和SVM都是判别模型。 判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别（哈哈，废话是不是太多）。 不同点 第一，本质上是其损失函数（loss function）不同。 注：lr的损失函数是交叉熵损失 cross entropy loss， adaboost的损失函数是指数损失 expotional loss ,svm是铰链损失函数hinge loss，常见的回归模型通常用 均方误差 loss。 第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用） 第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。 第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！ SVM与LR的区别与联系 联系：（1）分类（二分类） （2）可加入正则化项 区别： （1）LR–参数模型；SVM–非参数模型？ （2）目标函数：LR—logistical loss；SVM–hinge loss （3）SVM–support vectors；LR–减少较远点的权重 （4）LR–模型简单，好理解，精度低，可能局部最优；SVM–理解、优化复杂，精度高，全局最优，转化为对偶问题—&gt;简化模型和计算 （5）LR可以做的SVM可以做（线性可分），SVM能做的LR不一定能做（线性不可分） 总结一下 Linear SVM和LR都是线性分类器 Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要对数据先做balancing。 Linear SVM依赖数据表打对距离测度，所以需要对数据先做normalization；LR不受影响 Linear SVM依赖penalty的系数，实验中需要做validation Linear SVM的LR的performance都会收到outlier的影响，就敏感程度而言，无法给出明确结论。 Boosting系列 Boosting提升算法：将弱学习算法提升为强学习算法的过程，主要涉及两个部分：加法模型和向前分步算法。 加法模型就是说强分类器由一系列弱分类器线性相加而成。一般组合形式如下： FM(x;P)=∑m=1nβmh(x;am)F_{M}(x ; P)=\\sum_{m=1}^{n} \\beta_{m} h\\left(x ; a_{m}\\right) FM​(x;P)=m=1∑n​βm​h(x;am​) 其中，h(𝑥;𝑎𝑚)ℎ(𝑥;𝑎_𝑚)h(x;am​) 就是一个个的弱分类器，ama_mam​是弱分类器学习到的最优参数，βm\\beta _ mβm​就是弱学习在强分类器中所占比重，PPP是所有ama_mam​和βm\\beta _ mβm​的组合。这些弱分类器线性相加组成强分类器。 前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。也就是可以写成这样的形式： Fm(x)=Fm−1(x)+βmhm(x;am)F_{m}(x)=F_{m-1}(x)+\\beta_{m} h_{m}\\left(x ; a_{m}\\right) Fm​(x)=Fm−1​(x)+βm​hm​(x;am​) 由于采用的损失函数不同，Boosting算法也因此有了不同的类型，AdaBoost就是损失函数为指数损失的Boosting算法。 AdaBoost 基于Boosting的理解，对于AdaBoost，我们要搞清楚两点： 每一次迭代的弱学习h(x;am)h\\left(x ; a_{m}\\right)h(x;am​)有何不一样，如何学习？ 弱分类器权值βm\\beta _ mβm​如何确定？ 对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。 对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。 梯度提升树（GBDT） 简单易学的机器学习算法——梯度提升决策树GBDT KNN 如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 1）计算测试数据与各个训练数据之间的距离； 2）按照距离的递增关系进行排序； 3）选取距离最小的K个点； 4）确定前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。 K-Means 选择K个点作为初始质心 repeat 将每个点指派到最近的质心，形成K个簇 重新计算每个簇的质心 until 簇不发生变化或达到最大迭代次数 PCA 设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵C=1mXX⊤C=\\frac{1}{m} X X^{\\top}C=m1​XX⊤ 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P 6）Y=PXY=P XY=PX即为降维到k维后的数据 本质为协方差矩阵(实对称矩阵)的对角化 线性判别分析LDA 每个数据集的样本有类别输出 投影后类内方差最小，类间方差最大 瑞利商： R(A,x)=xHAxxHxR(A, x)=\\frac{x^{H} A x}{x^{H} x} R(A,x)=xHxxHAx​ A为Hermitan矩阵，即共轭转置矩阵和自己相等，即AH=AA^H=AAH=A 满足： λmin⁡≤xHAxxHx≤λmax⁡\\lambda_{\\min } \\leq \\frac{x^{H} A x}{x^{H} x} \\leq \\lambda_{\\max } λmin​≤xHxxHAx​≤λmax​ 广义瑞利商： R(A,B，x)=xHAxxHBxR(A,B，x)=\\frac{x^{H} A x}{x^{H} B x} R(A,B，x)=xHBxxHAx​ 化简： R(A,B,x′)=x′HB−1/2AB−1/2x′x′Hx′R\\left(A, B, x^{\\prime}\\right)=\\frac{x^{\\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\\prime}}{x^{\\prime H} x^{\\prime}} R(A,B,x′)=x′Hx′x′HB−1/2AB−1/2x′​ 其中x=B−1/2x′x=B^{-1 / 2} x^{\\prime}x=B−1/2x′ 二类LDA: 类内散度矩阵： Sw=Σ0+Σ1=∑x∈X0(x−μ0)(x−μ0)T+∑x∈X1(x−μ1)(x−μ1)TS_{w}=\\Sigma_{0}+\\Sigma_{1}=\\sum_{x \\in X_{0}}\\left(x-\\mu_{0}\\right)\\left(x-\\mu_{0}\\right)^{T}+\\sum_{x \\in X_{1}}\\left(x-\\mu_{1}\\right)\\left(x-\\mu_{1}\\right)^{T} Sw​=Σ0​+Σ1​=x∈X0​∑​(x−μ0​)(x−μ0​)T+x∈X1​∑​(x−μ1​)(x−μ1​)T 类间散度矩阵： Sb=(μ0−μ1)(μ0−μ1)TS_{b}=\\left(\\mu_{0}-\\mu_{1}\\right)\\left(\\mu_{0}-\\mu_{1}\\right)^{T} Sb​=(μ0​−μ1​)(μ0​−μ1​)T 优化目标： arg⁡max⁡⏟wJ(w)=wTSbwwTSww\\underbrace{\\arg \\max }_{w} J(w)=\\frac{w^{T} S_{b} w}{w^{T} S_{w} w} wargmax​​J(w)=wTSw​wwTSb​w​ 奇异值分解SVD 假设我们的矩阵A是一个m×nm \\times nm×n的矩阵，那么我们定义矩阵A的SVD为： A=UΣVTA=U \\Sigma V^{T} A=UΣVT","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"/tags/机器学习/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"java-基本数据结构","slug":"java/string_api","date":"2021-02-22T13:00:00.000Z","updated":"2021-02-23T03:17:47.439Z","comments":true,"path":"2021/02/22/java/string_api/","link":"","permalink":"/2021/02/22/java/string_api/","excerpt":"","text":"String API java.lang.string • char charAt (int index) 返回给定位置的代码单元。除非对底层的代码单元感兴趣， 否则不需要调用这个方法。 • int codePointAt(int Index) 返回从给定位置开始的码点。 • int offsetByCodePoints(int startlndex, int cpCount) 返回从 startlndex 代码点开始，位移 cpCount 后的码点索引。 • int compareTo(String other) 按照字典顺序， 如果字符串位于 other 之前， 返回一个负数；如果字符串位于 other 之后，返回一个正数； 如果两个字符串相等，返回 0。 • IntStream codePoints() 将这个字符串的码点作为一个流返回。调用 toArray 将它们放在一个数组中。 • new String(int[] codePoints, int offset, int count) 用数组中从 offset 开始的 count 个码点构造一个字符串。 • boolean equals(0bject other) 如果字符串与 other 相等， 返回 true。 • boolean equalsIgnoreCase( String other ) 如果字符串与 other 相等 （忽略大小写，) 返回 true。 • boolean startsWith( String prefix ) • boolean endsWith( String suffix ) 如果字符串以 suffix 开头或结尾， 则返回 true。 • int indexOf ( String str ) • int indexOf( String str, int fromlndex ) • int indexOf ( int cp) • int indexOf( int cp, int fromlndex ) 返回与字符串 str 或代码点 cp 匹配的第一个子串的开始位置。这个位置从索引 0 或 fromlndex 开始计算。 如果在原始串中不存在 str， 返回 -1。 • int 1astIndexOf( String str ) • Int 1astIndexOf ( String str, int fromlndex ) • int lastindexOf( int cp) • int 1astindexOf( int cp, int fromlndex ) 返回与字符串 str 或代码点 cp 匹配的最后一个子串的开始位置。 这个位置从原始串尾端或 fromlndex 开始计算。 • int length( ) 返回字符串的长度。 • int codePointCount( int startlndex, int endlndex ) 5.0 返回 startlndex 和 endludex-l 之间的代码点数量。没有配成对的代用字符将计入代码点。 • String replace( CharSequence oldString,CharSequence newString) 返回一个新字符串。这个字符串用 newString 代替原始字符串中所有的 oldString。可以用 String 或 StringBuilder 对象作为 CharSequence 参数。 • String substring( int beginlndex ) • String substring(int beginlndex, int endlndex ) 返回一个新字符串。这个字符串包含原始字符串中从 beginlndex 到串尾或 endlndex-l的所有代码单元。 • String toLowerCase( ) • String toUpperCase( ) 返回一个新字符串。 这个字符串将原始字符串中的大写字母改为小写，或者将原始字符串中的所有小写字母改成了大写字母。 • String trim( ) 返回一个新字符串。这个字符串将删除了原始字符串头部和尾部的空格。 • String join(CharSequence delimiter, CharSequence... elements ) 8 返回一个新字符串， 用给定的定界符连接所有元素。 Examples: String greeting = \"Hello\"; String s = greeting.substring(0,3); //\"Hel\" String all = String.join(\"/\",\"S\",\"M\",\"L\",\"ML\"); // all is \"S/M/L/ML\" s.equals(t); //s和t是否相等 s.equalsIgnoreCase(t); //不区分大小写是否相等 int[] codePoints = str.codePoints().toArray(); //依次查看每一个码点 StringBuilder API • StringBuilder() 构造一个空的字符串构建器。 • int length() 返回构建器或缓冲器中的代码单元数量。 • StringBuilder append(String str) 追加一个字符串并返回 this. • StringBuilder append(char c) 追加一个代码单元并返回 this。 • StringBuilder appendCodePoint(int cp) 追加一个代码点，并将其转换为一个或两个代码单元并返回 this 。 • void setCharAt(int i ,char c) 将第 i 个代码单元设置为 c。 • StringBuilder insert(int offset,String str) 在 offset 位置插入一个字符串并返回 this。 • StringBuilder insert(int offset,Char c) 在 offset 位置插入一个代码单元并返回 this。 • StringBuilder delete(1 nt startindex,int endlndex) 删除偏移量从 startindex 到-endlndex-1 的代码单元并返回 this。 • String toString() 返回一个与构建器或缓冲器内容相同的字符串 StringBuilder builder = new StringBuilder(); builder.append(str); String completedString = builder.toString(); Scanner API java.util.Scanner • Scanner (InputStream in) 用给定的输入流创建一个Scanner对象，常用System.in、Path.of • String nextLine() 读取输入的下一行内容 • String next() 读取输入的下一个单词，以空格作为分隔符 • int nextInt() • double nextDouble() 读取并转换下一个表示整数或浮点数的字符序列 • boolean hasNext() 检测输入中是否还有其他单词 • boolean hasNextInt() • boolean hasNextDouble() 检测是否还有下一个表示整数或浮点数的字符序列 Examples: Scanner in = new Scanner(System.in); String name = in.nextLine(); 输出 文件读取、写入： Scanner in = new Scanner(Path.of(\"myfile.txt\", StandardCharsets.UTF_8)) String line = in.nextLine() PrintWriter out = new PrintWriter(\"file.txt\", StandardCharsets.UTF_8) out.printf(line) 格式化输出：格式说明符，参数替换 创建格式化字符串： String message = String.format(\"Hello, %s. Next year, you will be %d\", name, age) 当前启动目录位置： String dir = System.getProperty(\"user.dir\") MATH API import static java.lang.MATH.* 大数类型 java.math.BigInteger BigInteger a = BigInteger.valueOf(100); BigInteger reallyBig = new BigInteger(\"22222222333333444444455555555555555\") • BigInteger add(BigInteger other) • BigInteger subtract(BigInteger other) • BigInteger multiply(BigInteger other) • BigInteger divide(BigInteger other) • BigInteger mod(BigInteger other) 和、差、积、商、余数 • BigInteger sqrt() • int compare(BigInteger other) 相等返回0，小于返回负数，大于返回正数 • static BigInteger valueOf(long x) 返回值等于x的大整数 Java.math.BigDecimal 类似，略 Arrays java.util.Arrays • static String toString(xxx[] a) 返回包含a中元素的一个字符串，这些元素用中括号包围，并用逗号分隔 • static xxx[] copyOf(xxx[] a, int end) • static xxx[] copyOf(xxx[] a, int start, int end) 返回与a类型相同的一个数组 • static void sort(xxx[] a) 排序 • static int binarySearch(xxx[] a, xxx v) • static int binarySearch(xxx[] a, int start, int end, xxx v) 二分查找v，若找到返回相应下标，否则返回一个负数值r, -r-1为v应插入的位置 • static void fill(xxx[] a, xxx[] b) 将数组所有数据元素设置为v • static boolean equals(xxx[] a, xxx[] b) 是否相等 数据类型 基本数据类型 **整型：**int,short,long,byte 长整型数值有一个后缀L或l，十六进制数值有一个前缀0x或0X，八进制有一个前缀0，二进制前有一个0b或0B。 浮点类型： float和double float类型的数值后面有一个后缀F或f，没有后缀F的浮点数值总是默认为double类型。 特殊浮点数： 正无穷大 Double.POSITIVE_INFINITY 负无穷大 DOUBLE.MEGATIVE_INFINITY Nan DOUBLE.NaN 区分代码单元和替代区域 局部变量、常量 对于局部变量，如果可以从变量的初始值推断出它的类型，就不再需要声明类型，只需要使用关键字var var vacationDays = 12 利用关键字final指示常亮，常量名使用全大写 final double CM_PER_TNCH = 2.54 使用关键字static final设置一个类常量 枚举类型 enum Size {SMALL, MEDIUM, LARGE, EXTRA_LARGE} Size s = Size.MEDIUM 强制类型转换 double x = 0.007 int nx = (int) x 逻辑运算符和位运算符 &amp;&amp; || ! &amp; | ^ ~ 控制流程 条件语句：if &lt;condition&gt; statement 循环语句：while &lt;condition&gt; statement 或 do statement while &lt;condition&gt; for循环：for (int i = 0; i &lt; 10; i++) switch语句：switch (choice) {case 1: ... default} 数组 int[] a = new int[100] int[] smallPrimes = {2,3,4,5,6} array.length() // for each循环： for (variable : collection) statement // example for (int element: a) System.out.printlb(element)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"java","slug":"java","permalink":"/tags/java/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"python应用","slug":"python+图算法","date":"2021-01-06T13:00:00.000Z","updated":"2021-02-23T03:17:55.259Z","comments":true,"path":"2021/01/06/python+图算法/","link":"","permalink":"/2021/01/06/python+图算法/","excerpt":"","text":"深度优先搜索(DFS)模板: def dfs(): if (满足特定条件）{ // 返回结果 or 退出搜索空间 } for choice in all_choices(): record(choice) dfs() rollback(choice) } 广度优先搜索(BFS)模板： class Solution: def bfs(k): # 使用双端队列，而不是数组。因为数组从头部删除元素的时间复杂度为 N，双端队列的底层实现其实是链表。 queue = collections.deque([root]) # 记录层数 steps = 0 # 需要返回的节点 ans = [] # 队列不空，生命不止！ while queue: size = len(queue) # 遍历当前层的所有节点 for _ in range(size): node = queue.popleft() if (step == k) ans.append(node) if node.right: queue.append(node.right) if node.left: queue.append(node.left) # 遍历完当前层所有的节点后 steps + 1 steps += 1 return ans 典型建图代码： # 邻接表形式 from collections import defaultdict graph = defaultdict(list) for u, v, c in times: graph[u].append((v, c)) # 邻接矩阵形式 graph = [[float('inf') for _ in range(n)] for _ in range(n)] for u, v, c in items: graph[u][v] = c graph[v][u] = c Dijkstra算法(狄克斯拉特算法) 模板：借助堆的数据结构，每次在Ologn\\mathcal{O} lognOlogn 时间复杂度内找到cost最小的点。(注：只适用于有向无环图，且无负权边) 步骤： 找出最短时间内前往的节点(对于该节点来说，没有更短路径，直接将其加入到visited中) 对于该节点的每个邻居，更新开销 重复计算，直到每个节点的开销都被更新过 计算最终路径 import heapq def dijkstra(graph, start, end): # 堆里的数据都是 (cost, i) 的二元祖，其含义是“从 start 走到 i 的距离是 cost”。 heap = [(0, start)] visited = set() while heap: (cost, u) = heapq.heappop(heap) if u in visited: continue visited.add(u) if u == end: return cost for v, c in graph[u]: if v in visited: continue next = cost + c heapq.heappush(heap, (next, v)) return -1 Floyd_warshall算法： 基于动态规划，dist(i,j)=disti,k+dist(j,k)dist(i,j)=dist{i,k}+dist(j,k)dist(i,j)=disti,k+dist(j,k)，时间复杂度O(n3)\\mathcal{O}(n^3)O(n3)，空间复杂度O(n2)\\mathcal{O}(n^2)O(n2)，nnn 为图顶点个数。 # graph 是邻接矩阵，v 是顶点个数 def floyd_warshall(graph, v): dist = [[float(\"inf\") for _ in range(v)] for _ in range(v)] for i in range(v): for j in range(v): dist[i][j] = graph[i][j] # check vertex k against all other vertices (i, j) for k in range(v): # looping through rows of graph array for i in range(v): # looping through columns of graph array for j in range(v): if ( dist[i][k] != float(\"inf\") and dist[k][j] != float(\"inf\") and dist[i][k] + dist[k][j] &lt; dist[i][j] ): dist[i][j] = dist[i][k] + dist[k][j] return dist, v A星寻路算法 解决的问题是在二维表格中找出任意两点的最短路径，一般题目中包括障碍物。 f(n)=g(n)+h(n)f(n) = g(n) + h(n) f(n)=g(n)+h(n) f(n)f(n)f(n) 是从初始状态经由状态 nnn 到目标状态的估计代价， g(n)g(n)g(n) 是在状态空间中从初始状态到状态 nnn 的实际代价， h(n)h(n)h(n) 是从状态 nnn 到目标状态的最佳路径的估计代价。 估价算法：一般使用曼哈顿距离进行估价。 H(n)=D∗(abs(n.x–goal.x)+abs(n.y–goal.y))H(n) = D * (abs ( n.x – goal.x ) + abs ( n.y – goal.y ) ) H(n)=D∗(abs(n.x–goal.x)+abs(n.y–goal.y)) grid = [ [0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], # 0 are free path whereas 1's are obstacles [0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0], ] \"\"\" heuristic = [[9, 8, 7, 6, 5, 4], [8, 7, 6, 5, 4, 3], [7, 6, 5, 4, 3, 2], [6, 5, 4, 3, 2, 1], [5, 4, 3, 2, 1, 0]]\"\"\" init = [0, 0] goal = [len(grid) - 1, len(grid[0]) - 1] # all coordinates are given in format [y,x] cost = 1 # the cost map which pushes the path closer to the goal heuristic = [[0 for row in range(len(grid[0]))] for col in range(len(grid))] for i in range(len(grid)): for j in range(len(grid[0])): heuristic[i][j] = abs(i - goal[0]) + abs(j - goal[1]) if grid[i][j] == 1: heuristic[i][j] = 99 # added extra penalty in the heuristic map # the actions we can take delta = [[-1, 0], [0, -1], [1, 0], [0, 1]] # go up # go left # go down # go right # function to search the path def search(grid, init, goal, cost, heuristic): closed = [ [0 for col in range(len(grid[0]))] for row in range(len(grid)) ] # the reference grid closed[init[0]][init[1]] = 1 action = [ [0 for col in range(len(grid[0]))] for row in range(len(grid)) ] # the action grid x = init[0] y = init[1] g = 0 f = g + heuristic[init[0]][init[0]] cell = [[f, g, x, y]] found = False # flag that is set when search is complete resign = False # flag set if we can't find expand while not found and not resign: if len(cell) == 0: return \"FAIL\" else: # to choose the least costliest action so as to move closer to the goal cell.sort() cell.reverse() next = cell.pop() x = next[2] y = next[3] g = next[1] if x == goal[0] and y == goal[1]: found = True else: for i in range(len(delta)): # to try out different valid actions x2 = x + delta[i][0] y2 = y + delta[i][1] if x2 &gt;= 0 and x2 &lt; len(grid) and y2 &gt;= 0 and y2 &lt; len(grid[0]): if closed[x2][y2] == 0 and grid[x2][y2] == 0: g2 = g + cost f2 = g2 + heuristic[x2][y2] cell.append([f2, g2, x2, y2]) closed[x2][y2] = 1 action[x2][y2] = i invpath = [] x = goal[0] y = goal[1] invpath.append([x, y]) # we get the reverse path from here while x != init[0] or y != init[1]: x2 = x - delta[action[x][y]][0] y2 = y - delta[action[x][y]][1] x = x2 y = y2 invpath.append([x, y]) path = [] for i in range(len(invpath)): path.append(invpath[len(invpath) - 1 - i]) print(\"ACTION MAP\") for i in range(len(action)): print(action[i]) return path a = search(grid, init, goal, cost, heuristic) for i in range(len(a)): print(a[i]) 拓扑排序： 对于有向无环图进行排序，使得对于从顶点uuu 到顶点vvv 的每个有向边uvuvuv，uuu 在排序中都在vvv 之前。 Kahn算法： 假设 LLL 是存放结果的列表，先找到那些入度为零的节点，把这些节点放到 LLL 中，因为这些节点没有任何的父节点。然后把与这些节点相连的边从图中去掉，再寻找图中的入度为零的节点。对于新找到的这些入度为零的节点来说，他们的父节点已经都在LLL 中了，所以也可以放入 LLL。重复上述操作，直到找不到入度为零的节点。如果此时 LLL 中的元素个数和节点总数相同，说明排序完成；如果LLL 中的元素个数和节点总数不同，说明原图中存在环，无法进行拓扑排序。 def topologicalSort(graph): \"\"\" Kahn's Algorithm is used to find Topological ordering of Directed Acyclic Graph using BFS \"\"\" indegree = [0] * len(graph) queue = [] topo = [] cnt = 0 for key, values in graph.items(): for i in values: indegree[i] += 1 for i in range(len(indegree)): if indegree[i] == 0: queue.append(i) while queue: vertex = queue.pop(0) cnt += 1 topo.append(vertex) for x in graph[vertex]: indegree[x] -= 1 if indegree[x] == 0: queue.append(x) if cnt != len(graph): print(\"Cycle exists\") else: print(topo) # Adjacency List of Graph graph = {0: [1, 2], 1: [3], 2: [3], 3: [4, 5], 4: [], 5: []} topologicalSort(graph) 最小生成树： Krusal算法： 基本思想为从小到大加入边，是一种贪心算法。 其算法流程为： 将图G={V,E}G = \\{V, E\\}G={V,E} 中的所有边按照长度由小到大进行排序。 初始化图 G′G&#x27;G′ 为 {V,∅}\\{V,\\varnothing\\}{V,∅} ，从前向后扫描排序后的边，如果扫描到的边 eee 在 G′G&#x27;G′ 中连接了两个相异的连通块,则将它插入G′G&#x27;G′ 中。 最后得到的图 G′ G&#x27; G′就是图GGG 的最小生成树。 实际代码中，首先将这张完全图中的边全部提取到边集数组中，然后对所有边进行排序，从小到大进行枚举，每次贪心选边加入答案。使用并查集维护连通性，若当前边两端不连通即可选择这条边。 class DisjointSet: def __init__(self, element_num=None): self._father = {} self._rank = {} # 初始化时每个元素单独成为一个集合 if element_num is not None: for i in range(element_num): self.add(i) def add(self, x): # 添加新集合 # 如果已经存在则跳过 if x in self._father: return self._father[x] = x self._rank[x] = 0 def _query(self, x): # 如果father[x] == x，说明x是树根 if self._father[x] == x: return x self._father[x] = self._query(self._father[x]) return self._father[x] def merge(self, x, y): if x not in self._father: self.add(x) if y not in self._father: self.add(y) # 查找到两个元素的树根 x = self._query(x) y = self._query(y) # 如果相等，说明属于同一个集合 if x == y: return # 否则将树深小的合并到树根大的上 if self._rank[x] &lt; self._rank[y]: self._father[x] = y else: self._father[y] = x # 如果树深相等，合并之后树深+1 if self._rank[x] == self._rank[y]: self._rank[x] += 1 # 判断是否属于同一个集合 def same(self, x, y): return self._query(x) == self._query(y) # 构造数据 edges = [[1, 2, 7], [2, 3, 8], [2, 4, 9], [1, 4, 5], [3, 5, 5], [2, 5, 7], [4, 5, 15], [4, 6, 6], [5, 6, 8], [6, 7, 11], [5, 7, 9]] if __name__ == \"__main__\": disjoinset = DisjointSet(8) # 根据边长对边集排序 edges = sorted(edges, key=lambda x: x[2]) res = 0 for u, v, w in edges: if disjoinset.same(u ,v): continue disjoinset.merge(u, v) res += w print(res) Prim算法： 流程： 选择一个点u，当做已经覆盖 把u所有相连的边加入队列 循环 循环 从队列头部弹出边 如果边合法 弹出 跳出循环 获取边的两个端点 将未覆盖的端点所有边加入队列 直到所有点都已经覆盖 import heapq class PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): # 传入两个参数，一个是存放元素的数组，另一个是要存储的元素，这里是一个元组。 # heap内部默认从小到大排 heapq.heappush(self._queue, (priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] def empty(self): return len(self._queue) == 0 edges = [[1, 2, 7], [2, 3, 8], [2, 4, 9], [1, 4, 5], [3, 5, 5], [2, 5, 7], [4, 5, 15], [4, 6, 6], [5, 6, 8], [6, 7, 11], [5, 7, 9]] if __name__ == \"__main__\": # 记录点是否覆盖 visited = [False for _ in range(11)] visited[1] = True # 邻接表，可以理解成二维数组 adj_table = [[] for _ in range(11)] # u和v表示两个端点，w表示线段长度 # 我们把v和w放入下标u中 # 把u和w放入下标v中 for (u, v, w) in edges: adj_table[u].append([v, w]) adj_table[v].append([u, w]) que = PriorityQueue() # 我们选择1作为起始点 # 将与1相邻的所有边加入队列 for edge in adj_table[1]: que.push(edge, edge[1]) ret = 0 # 一共有7个点，我们需要加入6条边 for i in range(7): # 如果队列为空，说明无法构成树 while not que.empty(): u, w = que.pop() # 如果连通的端点已经被覆盖了，则跳过 if visited[u]: continue # 标记成已覆盖 visited[u] = True ret += w # 把与它相连的所有边加入队列 for edge in adj_table[u]: que.push(edge, edge[1]) break print(ret) import sys from collections import defaultdict def PrimsAlgorithm(l): # noqa: E741 nodePosition = [] def get_position(vertex): return nodePosition[vertex] def set_position(vertex, pos): nodePosition[vertex] = pos def top_to_bottom(heap, start, size, positions): if start &gt; size // 2 - 1: return else: if 2 * start + 2 &gt;= size: m = 2 * start + 1 else: if heap[2 * start + 1] &lt; heap[2 * start + 2]: m = 2 * start + 1 else: m = 2 * start + 2 if heap[m] &lt; heap[start]: temp, temp1 = heap[m], positions[m] heap[m], positions[m] = heap[start], positions[start] heap[start], positions[start] = temp, temp1 temp = get_position(positions[m]) set_position(positions[m], get_position(positions[start])) set_position(positions[start], temp) top_to_bottom(heap, m, size, positions) # Update function if value of any node in min-heap decreases def bottom_to_top(val, index, heap, position): temp = position[index] while index != 0: if index % 2 == 0: parent = int((index - 2) / 2) else: parent = int((index - 1) / 2) if val &lt; heap[parent]: heap[index] = heap[parent] position[index] = position[parent] set_position(position[parent], index) else: heap[index] = val position[index] = temp set_position(temp, index) break index = parent else: heap[0] = val position[0] = temp set_position(temp, 0) def heapify(heap, positions): start = len(heap) // 2 - 1 for i in range(start, -1, -1): top_to_bottom(heap, i, len(heap), positions) def deleteMinimum(heap, positions): temp = positions[0] heap[0] = sys.maxsize top_to_bottom(heap, 0, len(heap), positions) return temp visited = [0 for i in range(len(l))] Nbr_TV = [-1 for i in range(len(l))] # Neighboring Tree Vertex of selected vertex # Minimum Distance of explored vertex with neighboring vertex of partial tree # formed in graph Distance_TV = [] # Heap of Distance of vertices from their neighboring vertex Positions = [] for x in range(len(l)): p = sys.maxsize Distance_TV.append(p) Positions.append(x) nodePosition.append(x) TreeEdges = [] visited[0] = 1 Distance_TV[0] = sys.maxsize for x in l[0]: Nbr_TV[x[0]] = 0 Distance_TV[x[0]] = x[1] heapify(Distance_TV, Positions) for i in range(1, len(l)): vertex = deleteMinimum(Distance_TV, Positions) if visited[vertex] == 0: TreeEdges.append((Nbr_TV[vertex], vertex)) visited[vertex] = 1 for v in l[vertex]: if visited[v[0]] == 0 and v[1] &lt; Distance_TV[get_position(v[0])]: Distance_TV[get_position(v[0])] = v[1] bottom_to_top(v[1], get_position(v[0]), Distance_TV, Positions) Nbr_TV[v[0]] = vertex return TreeEdges if __name__ == \"__main__\": # pragma: no cover # &lt; --------- Prims Algorithm --------- &gt; n = int(input(\"Enter number of vertices: \").strip()) e = int(input(\"Enter number of edges: \").strip()) adjlist = defaultdict(list) for x in range(e): l = [int(x) for x in input().strip().split()] # noqa: E741 adjlist[l[0]].append([l[1], l[2]]) adjlist[l[1]].append([l[0], l[2]]) print(PrimsAlgorithm(adjlist)) 二分图： 参考leetcode 886，785 代码模板： from typing import List class Solution: def isBipartite(self, graph: List[List[int]]) -&gt; bool: ''' :param graph: 输入邻接表方式给出的图，例如： [[1,3], [0,2], [1,3], [0,2]] 图如下： 0----1 | | | | 3----2 :return: True or False ''' n = len(graph) colors = [0 for _ in range(n)] def dfs(i, color): colors[i] = color for j in graph[i]: if colors[j] == color: return False if colors[j] == 0 and not dfs(j, -1 * color): return False return True for i in range(n): if colors[i] == 0 and not dfs(i, 1): return False return True 并查集： 并查集 基础代码： class UnionFind: def __init__(self, n: int): self.parent = list(range(n)) def findset(self, x: int) -&gt; int: if self.parent[x] == x: return x self.parent[x] = self.findset(self.parent[x]) return self.parent[x] def unite(self, x: int, y: int) -&gt; bool: x, y = self.findset(x), self.findset(y) if x == y: return self.parent[y] = x def connected(self, x: int, y: int) -&gt; bool: x, y = self.findset(x), self.findset(y) return x == y 效率优化：永远将深度小的树合并到深度大的树上 class DisjointSetUnion: def __init__(self): self.f = dict() self.rank = dict() def find(self, x: int) -&gt; int: if x not in self.f: self.f[x] = x self.rank[x] = 1 return x if self.f[x] == x: return x self.f[x] = self.find(self.f[x]) return self.f[x] def unionSet(self, x: int, y: int): fx, fy = self.find(x), self.find(y) if fx == fy: return if self.rank[fx] &lt; self.rank[fy]: fx, fy = fy, fx self.rank[fx] += self.rank[fy] self.f[fy] = fx def numberOfConnectedComponent(self) -&gt; int: return sum(1 for x, fa in self.f.items() if x == fa) 效率优化：懒惰运算的思想，边查找边路径压缩 class UnionFind: def __init__(self): \"\"\" 记录每个节点的父节点 \"\"\" self.father = {} def find(self,x): \"\"\" 查找根节点 路径压缩 \"\"\" root = x while self.father[root] != None: root = self.father[root] # 路径压缩 while x != root: original_father = self.father[x] self.father[x] = root x = original_father return root def merge(self,x,y): \"\"\" 合并两个节点 \"\"\" root_x,root_y = self.find(x),self.find(y) if root_x != root_y: self.father[root_x] = root_y def is_connected(self,x,y): \"\"\" 判断两节点是否相连 \"\"\" return self.find(x) == self.find(y) def add(self,x): \"\"\" 添加新节点 \"\"\" if x not in self.father: self.father[x] = None 带权并查集 class UnionFind: def __init__(self): \"\"\" 记录每个节点的父节点 记录每个节点到根节点的权重 \"\"\" self.father = {} self.value = {} def find(self,x): \"\"\" 查找根节点 路径压缩 更新权重 \"\"\" root = x # 节点更新权重的时候要放大的倍数 base = 1 while self.father[root] != None: root = self.father[root] base *= self.value[root] while x != root: original_father = self.father[x] ##### 离根节点越远，放大的倍数越高 self.value[x] *= base base /= self.value[original_father] ##### self.father[x] = root x = original_father return root Trie class TrieNode: def __init__(self): self.children = collections.defaultdict(TrieNode) self.is_word = False class Trie: def __init__(self): self.root = TrieNode() def insert(self, word): curr = self.root for c in word: curr = curr.children[c] curr.is_word = True def search(self, word): current = self.root for letter in word: current = current.children.get(letter) if current is None: return False return current.is_word def startsWith(self, prefix): current = self.root for c in prefix: current = current.children.get(c) if current is None: return False return True","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"python应用","slug":"python应用","date":"2020-12-06T03:38:00.000Z","updated":"2021-02-23T03:17:49.023Z","comments":true,"path":"2020/12/06/python应用/","link":"","permalink":"/2020/12/06/python应用/","excerpt":"","text":"collections defaultdict defaultdict解决的是dict中常见的key为空的情况 from collections import defaultdict d = defaultdict(list) for k, v in data: d[k].append(v) 使用defaultdict之后，如果key不存在，会自动返回预先设置的默认值。defaultdict传入的默认值可以使一个类型或一个方法。 Counter 计数和排序功能 words = ['apple', 'apple', 'pear', 'watermelon', 'pear', 'peach'] from collections import Counter counter = Counter(words) &gt;&gt;&gt; print(counter) Counter({'apple': 2, 'pear': 2, 'watermelon': 1, 'peach': 1}) Counter也提供了most_common方法，可以筛选topk &gt;&gt;&gt; counter.most_common(1) [('apple', 2)] deque 双端队列，支持队首和队尾的元素插入和弹出 常用API：clear,copy,count,extend,append,pop,appendleft,popleft from collections import deque dque = deque(maxlen=10) # 假设我们想要从文件当中获取最后10条数据 for i in f.read(): dque.append(i) Orderdict python2中的字典类型为无序，但python3中为有序。 Orderdict是有序字典 Heapq 全称: heqp queue，即堆和队列 nlargest和nsmallest：筛选topk import heapq nums = [14, 20, 5, 28, 1, 21, 16, 22, 17, 28] heapq.nlargest(3, nums) # [28, 28, 22] heapq.nsmallest(3, nums) # [1, 5, 14] 可以通过关键词参数key自定义排序规则 laptops = [ {'name': 'ThinkPad', 'amount': 100, 'price': 91.1}, {'name': 'Mac', 'amount': 50, 'price': 543.22}, {'name': 'Surface', 'amount': 200, 'price': 21.09}, {'name': 'Alienware', 'amount': 35, 'price': 31.75}, {'name': 'Lenovo', 'amount': 45, 'price': 16.35}, {'name': 'Huawei', 'amount': 75, 'price': 115.65} ] cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price']) expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price']) 优先队列 heapq.heapify方法输入一个数组，返回的结果是这个数组生成的堆(等价于优先队列) heap = [5,8,0,3,6,7,9,1,4,2] hq.heapify(heap) &gt;&gt;&gt; heap [0, 1, 5, 3, 2, 7, 9, 8, 4, 6] 可以直接使用heapq的heappop方法和heappush方法维护这个堆。 heapq.push传入两个参数，一个是存储元素的数组，另一个是要存储的元素。 时间复杂度：nlogn sortedcontainers 自定义排序 字典排序 kids = [ {'name': 'xiaoming', 'score': 99, 'age': 12}, {'name': 'xiaohong', 'score': 75, 'age': 13}, {'name': 'xiaowang', 'score': 88, 'age': 15} ] # 按照score来排序 sorted(kids, key=lambda x: x['score']) # 多关键词排序 sorted(kids, key=lambda x: (x['score'], x['age'])) python中自带的库可以代替匿名函数 from operator import itemgetter sorted(kids, key=itemgetter('score')) sorted(kids, key=itemgetter('score', 'age')) 对象排序 class Kid: def __init__(self, name, score, age): self.name = name self.score = score self.age = age def __repr__(self): return 'Kid, name: {}, score: {}, age:{}'.format(self.name, self.score, self.age) # 为了方便观察打印结果，我们重载了__repr__方法，可以指定在print的时候的输出结果 from operator import attrgetter kids = [Kid('xiaoming', 99, 12), Kid('xiaohong', 75, 13), Kid('xiaowang', 88, 15)] sorted(kids, key=attrgetter('score')) sorted(kids, key=lambda x: x.score) 自定义排序 # 若x&lt;y,返回一个负数，若x&gt;y,返回一个正数，否则返回0 def cmp(kid1, kid2): if kid1.score == kid2.score: return kid1.age - kid2.age else: return kid1.score - kid2.score from functools import cmp_to_key sorted(kids, key=cmp_to_key(cmp)) 也可以在类中重载比较函数__lt__，更改其默认排序方式 lass Kid: def __init__(self, name, score, age): self.name = name self.score = score self.age = age def __repr__(self): return 'Kid, name: {}, score: {}, age:{}'.format(self.name, self.score, self.age) def __lt__(self, other): return self.score &gt; other.score or (self.score == other.score and self.age &lt; other.age) map,reduce和filter map map执行的是一个映射，会将一个序列通过一个函数映射到另一个序列，从而避免显式迭代。 def dis(point): return math.sqrt(point[0]**2 + point[1]**2) points = [[0, 1], [2, 4], [3, 2]] # map的输出值是一个迭代器，将其转换为list类型 print(list(map(dis, points))) # 用匿名函数简化 map(lambda x: math.sqrt(x[0]**2 + x[1] ** 2), points) reduce reduce是依次调用，将两个元素归并成一个结果 reduce(f, [a, b, c, d])的返回值为f(f(f(a, b), c), d) from functools import reduce def f(a, b): return a + b print(reduce(f, [1, 2, 3, 4])) 10 filter 过滤掉所有为False的元素 arr = [1, 3, 2, 4, 5, 8] list(filter(lambda x: x % 2 &gt; 0, arr)) compress 根据一个序列的条件过滤另一个序列 student = ['xiaoming', 'xiaohong', 'xiaoli', 'emily'] scores = [60, 70, 80, 40] from itemtools import compress &gt;&gt;&gt; pass = [i &gt; 60 for i in scores] &gt;&gt;&gt; print(pass) [False, True, True, False] &gt;&gt;&gt; list(compress(student, pass)) ['xiaohong', 'xiaoli'] 生成器和迭代器 容器迭代器 对于一个可迭代对象(python中的tuple,list,dict等都是可迭代对象)，可以用关键字iter获得一个相应的迭代器。 arr = [1, 3, 4, 5, 9] it = iter(arr) print(next(it)) print(next(it)) 当越界时会抛出StopIteration 的Error。 也可以使用for去进行迭代。 自己创建迭代器 在定义类时，添加__iter__和__next__方法，其中__iter__方法用来初始化并返回迭代器。(iterable 的 __iter__ 返回 iterator, iterator本身也是一个iterable对象) class PowTwo: \"\"\"Class to implement an iterator of powers of two\"\"\" def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n &lt;= self.max: result = 2 ** self.n self.n += 1 return result else: raise StopIteration &gt;&gt;&gt; a = PowTwo(4) &gt;&gt;&gt; i = iter(a) &gt;&gt;&gt; next(i) 1 &gt;&gt;&gt; next(i) 2 &gt;&gt;&gt; next(i) 4 &gt;&gt;&gt; next(i) 8 &gt;&gt;&gt; next(i) 16 &gt;&gt;&gt; next(i) Traceback (most recent call last): ... StopIteration &gt;&gt;&gt; for i in PowTwo(5): ... print(i) ... 1 2 4 8 16 32 生成器 括号创建法 g = (i * 3for i in range(10)) print(next(g)) 函数创建法 def test(): n = 0 whileTrue: if n &lt; 3: yield n n += 1 else: yield 10 if __name__ == '__main__': t = test() for i in range(10): print(next(t)) 二叉树的遍历： class Node: def __init__(self, key): self.key = key self.lchild = None self.rchild = None self.iterated = False self.father = None def iterate(self): if self.lchild isnotNone: yieldfrom self.lchild.iterate() yield self.key if self.rchild isnotNone: yieldfrom self.rchild.iterate() Itertools 跳过迭代器开头 # 跳过头部注释#的部分 from itertools import dropwhile with open('xxxx.txt') as f: for line in dropwhile(lambda line: line.startswith('#'), f): print(line) # 从第三行开始 from itertools import dropwhile with open('xxxx.txt') as f: for line in islice(f, 3, None): print(line) 排列组合 # 排列 items = ['a', 'b', 'c'] from itertools import permutations for p in permutations(items): print(p) # 只保留前两个元素 for p in permutations(items, 2): print(p) # 组合 from itertools import combindations for c in combinations(items): print(c) # 有放回的组合 for c in combinations_with_replacement(items, 3): print(c) 迭代的合并 from itertools import chain nums = [1, 2, 3] chars = ['a', 'b', 'c'] for i in chain(nums, chars): print(i) accumulate 将可迭代对象逐步进行某操作，默认为累加 itertools.accumulate(iterable[, func]) &gt;&gt;&gt; import operator &gt;&gt;&gt; c=accumulate(a,operator.mul) &gt;&gt;&gt; c &lt;itertools.accumulate object at 0x7f3e5c2f4f88&gt; &gt;&gt;&gt; list(c) [1, 2, 6, 24, 120] 打印对象 直接打印实例时，会返回一个内存地址： class point: def __init__(self, x, y): self.x = x self.y = y if __name__ == \"__main__\": p = point(3, 4) print(p) &lt;__main__.point object at 0x10a18c210&gt; 重置__str__方法： class point: def __init__(self, x, y): self.x = x self.y = y def __str__(self): return 'x: %s, y: %s' % (self.x, self.y) x: 3, y: 4 重置__repr__方法 class point: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return 'x: %s, y: %s' % (self.x, self.y) x: 3, y: 4 注：__str__ 侧重于展示，__repr__ 侧重于交互式中的报告 format方法 print('x:{x},y:{y}'.format(x=3,y=4)) 重载__format__函数： class point: def __init__(self, x, y): self.x = x self.y = y def __str__(self): return 'x: %s, y: %s' % (self.x, self.y) def __format__(self, code): return 'x: {x}, y: {y}'.format(x = self.x, y = self.y) print('The point is {}'.format(p)) The point is x: 3, y: 4 BiseCt模块进行二分查找 名称 功能 bisect_left() 查找目标元素左侧插入点 bisect_right() 查找目标元素右侧插入点 bisect() 同 bisect_right() insort_left() 查找目标元素左侧插入点，并保序地插入元素 insort_right() 查找目标元素右侧插入点，并保序地插入元素 insort() 同 insort_right() Any, All &gt;&gt;&gt; any([0,1]) True &gt;&gt;&gt; any([0,'0','']) True &gt;&gt;&gt; all(['a', 'b', 'c', 'd']) True &gt;&gt;&gt; all(['a', 'b', '', 'd']) False 正则表达式 字符匹配 在正则表达式中，\\d可以匹配一个数字，\\s可以匹配一个空格，\\w可以匹配一个字母或数字，.可以匹配任意字符，*可以匹配任意个字符(包括0个)，+表示至少一个字符，{n}表示n个字符，{n,m}表示n-m个字符。 例如：\\d{3}\\s+\\d{3,8}匹配 [三个数字+至少一个空格+3—8个数字] 精确匹配 要做更精确地匹配，可以用[]表示范围，比如： [0-9a-zA-Z\\_]可以匹配一个数字、字母或者下划线； [0-9a-zA-Z\\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如'a100'，'0_Z'，'Py3000'等等； [a-zA-Z\\_][0-9a-zA-Z\\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配'Python'或者'python'。 ^表示行的开头，^\\d表示必须以数字开头。 $表示行的结束，\\d$表示必须以数字结束。 re模块 先看看如何判断正则表达式是否匹配： &gt;&gt;&gt; import re &gt;&gt;&gt; re.match(r'^\\d{3}\\-\\d{3,8}$', '010-12345') &lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt; &gt;&gt;&gt; re.match(r'^\\d{3}\\-\\d{3,8}$', '010 12345') &gt;&gt;&gt; match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是： test = '用户输入的字符串' if re.match(r'正则表达式', test): print('ok') else: print('failed') 切分字符串： 用正则表达式切分字符串比用固定的字符更灵活 &gt;&gt;&gt; 'a b c'.split(' ') ['a', 'b', '', '', 'c'] &gt;&gt;&gt; re.split(r'\\s+', 'a b c') ['a', 'b', 'c'] &gt;&gt;&gt; re.split(r'[\\s\\,]+', 'a,b, c d') ['a', 'b', 'c', 'd'] &gt;&gt;&gt; re.split(r'[\\s\\,\\;]+', 'a,b;; c d') ['a', 'b', 'c', 'd'] 分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： &gt;&gt;&gt; m = re.match(r'^(\\d{3})-(\\d{3,8})$', '010-12345') &gt;&gt;&gt; m &lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt; &gt;&gt;&gt; m.group(0) '010-12345' &gt;&gt;&gt; m.group(1) '010' &gt;&gt;&gt; m.group(2) '12345' 如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。 注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。 &gt;&gt;&gt; t = '19:05:30' &gt;&gt;&gt; m = re.match(r'^(0[0-9]|1[0-9]|2[0-3]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$', t) &gt;&gt;&gt; m.groups() ('19', '05', '30') 这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期： '^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$' 对于'2-30'，'4-31'这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。 贪婪匹配 正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： &gt;&gt;&gt; re.match(r'^(\\d+)(0*)$', '102300').groups() ('102300', '') 由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配： &gt;&gt;&gt; re.match(r'^(\\d+?)(0*)$', '102300').groups() ('1023', '00') 编译 &gt;&gt;&gt; import re # 编译: &gt;&gt;&gt; re_telephone = re.compile(r'^(\\d{3})-(\\d{3,8})$') # 使用： &gt;&gt;&gt; re_telephone.match('010-12345').groups() ('010', '12345') &gt;&gt;&gt; re_telephone.match('010-8086').groups() ('010', '8086') 装饰器 装饰器实质上是一个高阶函数 from functools import wraps def wrapexp(func): def wrapper(*args, **kwargs): print('this is a wrapper') func(*args, **kwargs) return wrapper @wrapexp def exp(a, b, c='3', d='f'): print(a, b, c, d) &gt;&gt;&gt;args = [1, 3] &gt;&gt;&gt;dt = {'c': 4, 'd': 5} &gt;&gt;&gt;exp(*args, **dt) this is a wrapper 1 3 4 5 在这个例子当中，我们定义了一个wrapexp的装饰器。我们在其中的wrapper方法当中实现了装饰器的逻辑，wrapexp当中传入的参数func是一个函数，wrapper当中的参数则是func的参数。所以我们在wrapper当中调用func(*args, **kw)，就是调用打上了这个注解的函数本身。 利用装饰器计算函数耗时： import time from functools import wraps def timethis(func): def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper 调用装饰器后，函数的元信息会丢失： from functools import wraps def wrapexp(func): def wrapper(*args, **kwargs): print('this is a wrapper') func(*args, **kwargs) return wrapper @wrapexp def exp(a, b, c='3', d='f'): print(a, b, c, d) &gt;&gt;&gt; exp.__name__ 'wrapexp' Python当中为我们提供了一个专门的装饰器器用来保留函数的元信息，我们只需要在实现装饰器的wrapper函数当中加上一个注解wraps即可 def wrapexp(func): @wraps(func) def wrapper(*args, **kwargs): print('this is a wrapper') func(*args, **kwargs) return wrapper divmod 把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b)。 chr和ord ord() 函数主要用来返回对应字符的ascii码，chr() 主要用来表示ascii码对应的字符他的输入时数字。 位运算 缓存机制与lru_cache 进制转换 # 输出x的二进制 &gt;&gt;&gt; '{0:b}'.format(10) '1010' # 二进制转换回整数 &gt;&gt;&gt; int('1010',2) 10","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"实验室NAS挂载教程","slug":"实验室NAS设备挂载教程","date":"2020-11-21T12:22:00.000Z","updated":"2020-11-23T02:38:17.735Z","comments":true,"path":"2020/11/21/实验室NAS设备挂载教程/","link":"","permalink":"/2020/11/21/实验室NAS设备挂载教程/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Please enter the password to read the blog. Incorrect Password! No content to display! U2FsdGVkX18zLQJ7rfIgiS9SOHE/jdA7gmtE6mRqR1ITDKsAiUIs5QVV7ghU4ljv7ZmPmNBnS/2FI/cPOW4dqt67tbtLzVqdKIaPohN9mDAvGj7TXyVOxtE74x64S9tjeWffc8EjTM8HwaEjvVn9wPkPDSqc/UlpqLmQdxpkn6gLXGidgXWEAOaLVbeRBhB3nS0to01aR+4zXhgOPpfKnx72FQ819Sj0rCiOvsIaJNOPI2sjZJwAfem3JsqjWtKKEtmoPHHWoVBJ56sO+5H2rBgX93MkW0PW7YfrBfgPAInCjZ7Y0qyTbR4Lrt9Pp5xKBR6sIloIP8Nh+EgtnuqFp/R7kWHhWGDMl8PxH0YLWXWjNAq1i1DGcI1YbnXL3sd9xeEByM2+YAvCO7F5ANvkLvpj7y9LhM9s+r56TOO7L92GCkip9t0bJRHO03vKy/UVWC5SFL1mln0DssmrtUny/bJinR27/Wt1zg0fVwDrSWE6vySvcqkwFoJ0Q+2YOlqOdyR761fGm59tqnQOxJbnHHqH4DUX5I/haLB1i48jE6b+qoPikeXo23CCbS/tYLpR7KDNxFt7mxkrW3Mdf2oGxH5TGAKefSCHtZqIQouAC33OeCVCDU4gfkda+DPdjojew4PhJLASzfvSYG2ivywc1yysH3jb7BiDDC5bVg91krpaEFPE6wd+uKqa0UI6jRU6igw6dHLjRrYmPBEx8bRr7SLY1xc/gWRJM7EEvHkcJIiAYSEdiMI9DhFeFbvqAx45ueRXXzUh7DUzMO8Sof+cfZFHdkv4jAdb1gFmkc/n5S3haMf8Rn+3Q9eQWehOCsaKY0CSraOs46740o1X9mR69SsSwDGRWrwuVaIVIj+s6tedaQGIT0szHQ3ggjg+83BrSTE7Djz5SIjgzzW3H5yoynZzutcj0V5VmrKYwmpKpXVa8CcxhfDBtATXC+YjW3Ht8oAC9fiBr1hlqwQTt/zx6gbKpvQl6JhxWCNwehkiCzhev04EZAprDFeHNmDFW//SHGfA7q/fCB6cXNo+KHd4xjQ+Al5ZMH9A89C16rmEEHXX7vALGll5PFxLUvgQ+Pv17MB7/IEhKy7shJIrtFE84rwaTjovFSzGfgxp4Ua00oOd4y/ZhKIAqceZd6vcNaNu33a/CAwN0uqEEx8hiUBwpdwhgcM0CVPSUGr6cune2RcqbsFNhukqd38zWg3PmjK5ibUZRyp/J9viZjgHDT4SB8YWRbsk9d0zfCtyccaSYDB8IvqevP5TVSqjjD+YKh2BiXEtWKazyWAiajEQ9Vo4931WTsntunHryzRgcRTSptaNQabtxhs+RU2agWWtNNFgZQM7CwN05744CLBk62flCEUD7aSGtpv395ixCFewyFGYF66CAG9NPzhasRj3XN7fLQPa1OebhhgFpIKQnEK/H3R1gIzAgLBShX7LDRZu6Y9vSR6kgKMcps+gi/W2en7XIlSSjMj7obIrGC8okIGOgkafRsP01Kt0eVUTFJoICsSvNqctjGPYts9I9PdUHTYKi+Qa7mY+g2B7wmvE1OIjFFKVuQq5gS7FQ2ZWLPdKrKrf0GxzNfBKR9uwwaXv7dfd5PZvSbQsU8XNAvgFcOxUe5F4ktWVYeL3FDE18EUrWmVIv9AakbSKOHFdAOnJlictb5fZSKOqBoxmmO0qgvbCZXPyeKC7LUuQx5031EwImRnqPIUirLjTx69IT3O8GTVLd6ATvhmQd8AyV3XNxX9oDGwCa7YFYoP/zW3MMLVX+IHPyIbcQF6XAIzCinJfVMk4piclUr2uun6U+l+D0wWKxrJ3zHaSNwo/QWn1qA2l5v8RBrkkbnkKL6zpxLOgj9YDPVcB48WMZquAZiPQ3lKy+E7dzE/RwyEK8qLagKScpJimF0SHW/8ZC0eqLZwcAO66upbj87EvHiHcrMtdjSvHJkJUXUcmfoR3enAHqMnzSYJRFvq0S7r80DqqyaLscYRygvz3INx8SsbE57k5vS7T22vC9sUuBrkmdlo4748zSWlM5zPey1iHrD79ZWUUMrBbzEhvEGEcMJmSLQ06xAObfsfTaIPWWjKZ0gGs/CAqVcg3cXBJgEAI3BxFoE7SFkjhbf0hIYr6UEqVMgDxxB9xa/zjLgdzbjUW0r44EK2VSFdHo4/3pwM048lIQAFT2BDKkhHtJXEIIMxXJFUqhPsJpz4USVBkc8wSSVymeotVssj014JOELExYVfLiqjO47lMFWcWPIetiFDARY0VNg51Ycop4oAXCg2sCi2cCKKk/ibRJ+NTFm0Wsnem9pml3mQhvyxhOE68XpuNpjPgbFMegPGQcooXiZKz8GPK8A04f4eMVwPiOoE6loSJnxbxeAcMy2rQFMNJMLIT6qoleeXDDpbr62RRGVKQUqUx37jQBDoxzgS2YS2pvDfgsJNlD912eWLh2Iw9GvbilOnnEy7Sy0ZRZMSs0tPXfVBwst5r3aKqmYih5zSOtM905MyvWHRX0P1cU5dTX3ycOohwo5zY/6t1gypkirpX7nE7TV4zQMw/g+GfFqW/S8K+XYRae+DdfqXJDF1WemMrjaoAW2TeXdX2obL7a/TLavICDs2T/YGKRQX6uU9e1FpuhZDSC0a0oYILb7ONzM5U1USeGZw3IyUGRi4fTA3cvjJAGaQLcz5FSUJNN+Gkw4mK6bd+DEvbWMRrZuKBj4uywM5z28N9XQJdsmEev56sk1PHgFjCimU+KzUNhvM9Wv7NGmRSra9ex1Dw/d9kdI6tYq42yGcDM2BB3pUJvLunW6TvMKfqI1KNDtw3Xf9Ibzdrm8kv0Rj3jF4yo2ol3kbswhNruOnZZ0IJkuA5a7tBSAQNtzECgPhPbOw4i+Woduwuj24XKpw8StpA1p0XpcWlmLXM4iJ5vKsplVHfxbGmIx9YmnJE8AFSnXBMEVEdroSGX/iCHeU2RZW1epbvJ0It0e8U9K9kHgMb3+6kV2ET4eLNGNz/E7JBzdY4npQXJ6SmgX+DuEBGZiy/uDDdzXJV6V/W5Ui+JfUlLSWD5BnewTMM/LRj4kHiPbPYLTSzpizLjJIK6w2azw+WXGMuC87VfpmOfOigGMFGRBHhyzhaJeKe3f1Z8rU37Psif37pMI9Cf1oUmFUJPMFh2ZpqMNctYGmK0J2tAARIWfsWzQq/bo7ohvsYE6cuvaJNNqWyfiOsIpO8nQyRphIq/hIY4tzfdraJBjBOCoZ+ahnSX1J/7unuzLtJ7q9HtjFtJg1nyAxU1x1KYJ6YJI8GeCtlaH0xSC/K9gCXgLNWam19OJprfZMnxmn7kT+e+yE3BRbZzhOsG4szV/6IJ7jEcjrST6r3D6azm26AsTd7z2h5oFEXMA7rHG4mMCqJVUi+oMScjmYCrZPPs2v7JK3sBXBRCF4FZfH01Qymu2enuxE8jQg2FmiV6FeUfLJjSCLsh6YY7Y6tYiCXI1tJwM6HmPQiSL0tir7I6UngEMKZNX10nUU6bw3FsOtmzwbHBYeLbKK4Vu6myp6PX+6pHXZrAA4/9EiIO/6ECKeyi8DYILrJ1AOZo5i0keGrkdyTaUmpJUNlKWUL+d8ocB0dzbfn0kvy5rsk6qwd6kF1x2rDXluMAPrq+c34mY5+N37EdOF/WA9mEnhARIFPWYyJC+nVqM0Thh3lp1PDSsHEpdBz5TpMywxTYVtLddk9Um48B00PI6Kk67nhpeAzjgfYDiCw/raSMVOIapCLLsOWiH7YMudttOz5TQlqBT4AHzvY6tvyxAyBqrbVzpw4/YmKYoD6i3bKI/HmzT4qUbuSO+f7K3UFQyzGKhhuFMSrXCfVnviXNqiV1FMWaVKLOTx4iHisam7PMkMnbRsCaa+oMx/IxcsdbbumrXYizBJ1rAkBF0aEl0/5NNRGHikmp4dys7mxGziGA7hCK0u4cF1ydYupns9VPN6eW2vjMnd+xqHSq7FyL+ZOoiZUtlndQ74W4xFeklJEbbYRyjWCUpnSsyxax+PM1kyfwuERJU8jj+92hUtCZb3BbOkW7dLqZRtmxxUIWmZFAWLi5w+pkoOjN7d8UYK2dRxz/u5BerBCxKQfv9DEsKuykB9RJAc3VeuWFw7DyED6rYiV2B9xQYCy3YC7FkTaPSLUgXkPSVqi1VdtkEq85LDYfryUCADbqEpT","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"现代微分几何","slug":"现代微分几何","date":"2020-09-24T08:42:00.000Z","updated":"2020-09-24T11:24:43.862Z","comments":true,"path":"2020/09/24/现代微分几何/","link":"","permalink":"/2020/09/24/现代微分几何/","excerpt":"","text":"光滑流形的定义，会用定义证明某些拓扑空间是光滑流形。 设MMM 是mmm 维拓扑流形，A\\mathcal{A}A 是MMM 的一个CrC^rCr 微分结构，则称(M,A)(M,\\mathcal{A})(M,A) 是一个mmm 维CrC^rCr 微分流形。特别地，C∞C^{\\infty}C∞ 微分结构称为光滑结构， C∞C^{\\infty}C∞ 微分流形称为光滑流形。 证明某些拓扑空间是CrC^rCr 微分流形(或光滑流形)： (1). 证明MMM是Hausdorff空间(拓扑空间上的任意两点可由其邻域分离) (2). 证明MMM是mmm 维拓扑流形(每一点xxx 存在其邻域UUU, UUU是MMM上的开邻域，UUU同胚于RmR^mRm的开集) (3). 构造MMM的CrC^rCr 微分结构(或光滑结构)A\\mathcal{A}A (A={(Uα,φα)∣α∈I}\\mathcal{A}=\\{(U_{\\alpha},\\varphi_{\\alpha})|\\alpha \\in I\\}A={(Uα​,φα​)∣α∈I}，满足Uα∣α∈IU_{\\alpha}|\\alpha \\in IUα​∣α∈I 是M的开覆盖、$\\left(U_{\\alpha}, \\varphi_{\\alpha}\\right) $和 (Uβ,φβ)\\left(U_{\\beta}, \\varphi_{\\beta}\\right)(Uβ​,φβ​)相容、A\\mathcal{A}A是极大的) 参考例1.2和例1.6 例1.2 Rm+1\\mathcal{R}^{m+1}Rm+1中的单位球面Sm={(x1,⋯ ,xm+1)}∈Rm+1∣∑i=1m+1(xi)2=1S^m = \\{(x^1,\\cdots,x^{m+1})\\}\\in \\mathbb{R}^{m+1}|\\sum_{i=1}^{m+1}(x^i)^2=1Sm={(x1,⋯,xm+1)}∈Rm+1∣∑i=1m+1​(xi)2=1 (1).证明SmS^mSm是Hausdroff空间 取其拓扑为它在Rm+1\\mathcal{R}^{m+1}Rm+1中的相对拓扑，则其是一个具有可数基的Hausdroff空间。 (2)证明SmS^mSm是拓扑流形 (i)相对开集的选取： 记 Uˉi+={(x1,⋯ ,xm+1)∣xi&gt;0}Uˉi−={(x1,⋯ ,xm+1)∣xi&lt;0},i=1,⋯ ,m+1\\begin{array}{c} \\bar{U}_{i}^{+}=\\left\\{\\left(x^{1}, \\cdots, x^{m+1}\\right) \\mid x^{i}&gt;0\\right\\} \\\\ \\bar{U}_{i}^{-}=\\left\\{\\left(x^{1}, \\cdots, x^{m+1}\\right) \\mid x^{i}&lt;0\\right\\}, \\quad i=1, \\cdots, m+1 \\end{array} Uˉi+​={(x1,⋯,xm+1)∣xi&gt;0}Uˉi−​={(x1,⋯,xm+1)∣xi&lt;0},i=1,⋯,m+1​ 他们都是Rm+1\\mathcal{R}^{m+1}Rm+1 中的开集，则相对开集Ui±=Uˉi±∩Sm(i=1,⋯ ,m+1) U_{i}^{\\pm}=\\bar{U}_{i}^{\\pm} \\cap S^{m}(i=1, \\cdots, m+1) Ui±​=Uˉi±​∩Sm(i=1,⋯,m+1) 构成SmS^mSm 的开覆盖。 (ii)每一个相对开集与$ R^m$中的开集同胚 如下定义φi±:Ui±→Rm\\varphi_{i}^{\\pm}:U_{i}^{\\pm} \\to \\mathcal{R}^mφi±​:Ui±​→Rm φi±(x1,⋯ ,xm+1)=(x1,⋯ ,xi,⋯ ,xm+1)\\varphi_{i}^{\\pm}\\left(x_{1}, \\cdots, x^{m+1}\\right)=\\left(x_{1}, \\cdots, x^{i}, \\cdots, x^{m+1}\\right) φi±​(x1​,⋯,xm+1)=(x1​,⋯,xi,⋯,xm+1) 其中^表示 去掉该坐标，易见这些映射分别是Ui±U_{i}^{\\pm}Ui±​到Rm\\mathbb{R}^mRm的开集 Wi={(x1,⋯ ,x^i,⋯ ,xm+1)∈Rm∣(x1)2+⋯+(x^i)2+⋯+(xm+1)2&lt;1}W_{i}=\\left\\{\\left(x_{1}, \\cdots, \\hat{x}^{i}, \\cdots, x^{m+1}\\right) \\in \\mathbb{R}^{m} \\mid\\left(x^{1}\\right)^{2}+\\cdots+\\left(\\hat{x}^{i}\\right)^{2}+\\cdots+\\left(x^{m+1}\\right)^{2}&lt;1\\right\\} Wi​={(x1​,⋯,x^i,⋯,xm+1)∈Rm∣(x1)2+⋯+(x^i)2+⋯+(xm+1)2&lt;1} 的同胚，因此SmS^mSm是mmm维拓扑流形。 (3)证明光滑结构，开覆盖在(2)中满足，关键是证明坐标卡的相容性 考虑φ2−∘(φ1+)−1:φ1+(U2−∩U1+)→φ2−(U2−∩U1+)\\varphi_{2}^{-} \\circ\\left(\\varphi_{1}^{+}\\right)^{-1}: \\varphi_{1}^{+}\\left(U_{2}^{-} \\cap U_{1}^{+}\\right) \\rightarrow \\varphi_{2}^{-}\\left(U_{2}^{-} \\cap U_{1}^{+}\\right)φ2−​∘(φ1+​)−1:φ1+​(U2−​∩U1+​)→φ2−​(U2−​∩U1+​) φ2−∘(φ1+)−1(x2,⋯ ,xm+1)=φ2−([1−∑i=2m+1(xi)2]12,x2,⋯ ,xm+1)=([1−∑i=2m+1(xi)2]12,x3,⋯ ,xm+1)\\varphi_{2}^{-} \\circ\\left(\\varphi_{1}^{+}\\right)^{-1}\\left(x^{2}, \\cdots, x^{m+1}\\right)=\\varphi_{2}^{-}\\left(\\left[1-\\sum_{i=2}^{m+1}\\left(x^{i}\\right)^{2}\\right]^{\\frac{1}{2}}, x^{2}, \\cdots, x^{m+1}\\right)=\\left(\\left[1-\\sum_{i=2}^{m+1}\\left(x^{i}\\right)^{2}\\right]^{\\frac{1}{2}}, x^{3}, \\cdots, x^{m+1}\\right) φ2−​∘(φ1+​)−1(x2,⋯,xm+1)=φ2−​⎝⎛​[1−i=2∑m+1​(xi)2]21​,x2,⋯,xm+1⎠⎞​=⎝⎛​[1−i=2∑m+1​(xi)2]21​,x3,⋯,xm+1⎠⎞​ 若用(u1,⋯ ,um)(u^1,\\cdots,u^m)(u1,⋯,um)记U1+U^+_1U1+​ 上点的坐标，用(u1,⋯ ,um)(u^1,\\cdots,u^m)(u1,⋯,um)记U2−U^-_2U2−​上点的坐标，则φ2−∘(φ1+)−1\\varphi_{2}^{-} \\circ\\left(\\varphi_{1}^{+}\\right)^{-1}φ2−​∘(φ1+​)−1的坐标表达式为 v1=[1−∑i=1m(ui)2]12,vl=ul,l=2,⋯ ,mv^{1}=\\left[1-\\sum_{i=1}^{m}\\left(u^{i}\\right)^{2}\\right]^{\\frac{1}{2}}, \\quad v^{l}=u^{l}, l=2, \\cdots, m v1=[1−i=1∑m​(ui)2]21​,vl=ul,l=2,⋯,m 显然vi(i=1,⋯ ,m)v^i(i=1,\\cdots,m)vi(i=1,⋯,m)是uj(j=1,⋯ ,m)u^j(j=1,\\cdots,m)uj(j=1,⋯,m)的光滑函数，从而坐标卡(U1+,φ1+)(U^+_1,\\varphi^+_1)(U1+​,φ1+​) 和(U2−,φ2−)(U^-_2,\\varphi^-_2)(U2−​,φ2−​)是C∞C^\\inftyC∞相容的，同理可证，坐标卡(Ui±,φi±)(U^{\\pm}_i,\\varphi^{\\pm}_i)(Ui±​,φi±​)是 C∞C^\\inftyC∞相容的，SmS^mSm是mmm维光滑流形。 例1.6 mmm维实射影空间RPm\\mathbb{R}P^mRPm 用RPm\\mathbb{R}P^mRPm表示Rm+1\\mathbb{R}^{m+1}Rm+1中过原点的直线的集合，下面在RPm\\mathbb{R}P^mRPm上引入微分结构，使之成为一个mmm维光滑流形。 (1) 证明RPm\\mathbb{R}P^mRPm是拓扑流形 (i) 拓扑关系的构建 首先在X=Rm+1\\0X = \\mathbb{R}^{m+1} \\backslash {0}X=Rm+1\\0 上定义等价关系∼\\thicksim∼如下： 对任意x=(x1,⋯ ,xm+1),y=(y1,⋯ ,ym+1),x∼yx = (x^1,\\cdots,x^{m+1}),y = (y^1,\\cdots,y^{m+1}),x\\sim yx=(x1,⋯,xm+1),y=(y1,⋯,ym+1),x∼y 当且仅当存在非零实数λ\\lambdaλ，使得y=λxy = \\lambda xy=λx，易见RPm\\mathbb{R}P^mRPm 可以等同XXX 关于等价关系∼\\sim∼ 的商空间，即 RPm=X/∼\\mathbb{R}P^m = X/\\sim RPm=X/∼ 用[x][x][x]表示点x=(x1,∼,xm+1)∈Xx = (x^1,\\sim,x^{m+1})\\in Xx=(x1,∼,xm+1)∈X 所在的等价类，则有 RPm={[x]∣x=(x1,⋯ ,xm+1)∈X}\\mathbb{R} P^{m}=\\left\\{[x] \\mid x=\\left(x^{1}, \\cdots, x^{m+1}\\right) \\in X\\right\\} RPm={[x]∣x=(x1,⋯,xm+1)∈X} 商映射记作π\\piπ ，RPm\\mathbb{R}P^mRPm上的拓扑为商拓扑，即U⊂RPmU \\subset \\mathbb{R}P^mU⊂RPm为开集当且仅当π−1(U)\\pi^{-1}(U)π−1(U)是XXX的开集。通常把xxx的坐标(x1,⋯ ,xm+2)(x^1,\\cdots,x^{m+2})(x1,⋯,xm+2)称为RPm\\mathbb{R}P^mRPm 中点[x][x][x]的齐次坐标，显然，一个点的齐次坐标不是唯一的，并且当xα≠0x^{\\alpha}\\ne 0xα​=0 s时，总有 [(x1,⋯ ,xm+1)]=[(x1/xα,⋯ ,xα−1/xα,1,xα+1/xα,⋯ ,xm+1/xα)]\\left[\\left(x^{1}, \\cdots, x^{m+1}\\right)\\right]=\\left[\\left(x^{1} / x^{\\alpha}, \\cdots, x^{\\alpha-1} / x^{\\alpha}, 1, x^{\\alpha+1} / x^{\\alpha}, \\cdots, x^{m+1} / x^{\\alpha}\\right)\\right] [(x1,⋯,xm+1)]=[(x1/xα,⋯,xα−1/xα,1,xα+1/xα,⋯,xm+1/xα)] (ii)开集、有限开覆盖选取，与Rm\\mathbb{R}^mRm 的同胚关系证明 如下定义RPm\\mathbb{R}P^mRPm 的m+1m+1m+1个开子集 Vα={[(x1,⋯ ,xm+1)]∈RPm∣xα≠0},α=1,⋯ ,m+1V_{\\alpha}=\\left\\{\\left[\\left(x^{1}, \\cdots, x^{m+1}\\right)\\right] \\in \\mathbb{R} P^{m} \\mid x^{\\alpha} \\neq 0\\right\\}, \\alpha=1, \\cdots, m+1 Vα​={[(x1,⋯,xm+1)]∈RPm∣xα​=0},α=1,⋯,m+1 并定义映射 φα:Vα→Rm,α=1,⋯ ,m+1\\varphi_{\\alpha}: V_{\\alpha} \\rightarrow \\mathbb{R}^{m}, \\alpha=1, \\cdots, m+1 φα​:Vα​→Rm,α=1,⋯,m+1 为 (ξ1,⋯ ,ξm)=φα([(x1,⋯ ,xm+1)])=(x1/xα,⋯ ,xα−1/xα,xα+1/xα,⋯ ,xm+1/xα)\\left(\\xi^{1}, \\cdots, \\xi^{m}\\right)=\\varphi_{\\alpha}\\left(\\left[\\left(x^{1}, \\cdots, x^{m+1}\\right)\\right]\\right)=\\left(x^{1} / x^{\\alpha}, \\cdots, x^{\\alpha-1} / x^{\\alpha}, x^{\\alpha+1} / x^{\\alpha}, \\cdots, x^{m+1} / x^{\\alpha}\\right) (ξ1,⋯,ξm)=φα​([(x1,⋯,xm+1)])=(x1/xα,⋯,xα−1/xα,xα+1/xα,⋯,xm+1/xα) 其中[(x1,⋯ ,xm+1)]∈Vα[(x^1,\\cdots,x^{m+1})]\\in V_{\\alpha}[(x1,⋯,xm+1)]∈Vα​ .易见φα\\varphi_{\\alpha}φα​是完全确定的，并且是从VαV_{\\alpha}Vα​到Rm\\mathbb{R}^mRm的同胚。 (2)光滑结构的证明 所以(Vα,φα)(V_{\\alpha},\\varphi_{\\alpha})(Vα​,φα​)是RPm\\mathbb{R}P^mRPm 的一个坐标卡，相应的局部坐标(η1,⋯ ,ηm)\\left(\\boldsymbol{\\eta}^{1}, \\cdots, \\boldsymbol{\\eta}^{m}\\right)(η1,⋯,ηm) 通常称为RPm\\mathbb{R}P^mRPm的非齐次坐标，当Vα∩Vβ≠∅V_{\\alpha} \\cap V_{\\beta} \\ne \\emptysetVα​∩Vβ​​=∅时(不妨设α&gt;β\\alpha &gt; \\betaα&gt;β) ,局部坐标变换 (η1,⋯ ,ηm)=φβ∘φα−1(ξ1,⋯ ,ξm),ξβ≠0\\left(\\eta^{1}, \\cdots, \\eta^{m}\\right)=\\varphi_{\\beta} \\circ \\varphi_{\\alpha}^{-1}\\left(\\xi^{1}, \\cdots, \\xi^{m}\\right), \\xi^{\\beta} \\neq 0 (η1,⋯,ηm)=φβ​∘φα−1​(ξ1,⋯,ξm),ξβ​=0 为 η1=ξ1/ξβ,⋯ ,ηβ−1=ξβ−1/ξβ,ηβ=ξβ+1/ξβ,⋯ηα−2=ξα−1/ξβ,ηα−1=1/ξβ,ηα=ξα/ξβ,⋯ ,ηm=ξm/ξβ\\begin{aligned} \\eta^{1} &amp;=\\xi^{1} / \\xi^{\\beta}, \\cdots, \\eta^{\\beta-1}=\\xi^{\\beta-1} / \\xi^{\\beta}, \\eta^{\\beta}=\\xi^{\\beta+1} / \\xi^{\\beta}, \\cdots \\\\ \\eta^{\\alpha-2} &amp;=\\xi^{\\alpha-1} / \\xi^{\\beta}, \\eta^{\\alpha-1}=1 / \\xi^{\\beta}, \\eta^{\\alpha}=\\xi^{\\alpha} / \\xi^{\\beta}, \\cdots, \\eta^{m}=\\xi^{m} / \\xi^{\\beta} \\end{aligned} η1ηα−2​=ξ1/ξβ,⋯,ηβ−1=ξβ−1/ξβ,ηβ=ξβ+1/ξβ,⋯=ξα−1/ξβ,ηα−1=1/ξβ,ηα=ξα/ξβ,⋯,ηm=ξm/ξβ​ 他们都是(ξ1,⋯ ,ξm)(\\xi^1,\\cdots,\\xi^m)(ξ1,⋯,ξm)的光滑函数，因而 φβ∘φα−1:φα(Vα∩Vβ)→φβ(Vα∩Vβ)\\varphi_{\\beta} \\circ \\varphi_{\\alpha}^{-1}: \\varphi_{\\alpha}\\left(V_{\\alpha} \\cap V_{\\beta}\\right) \\rightarrow \\varphi_{\\beta}\\left(V_{\\alpha} \\cap V_{\\beta}\\right) φβ​∘φα−1​:φα​(Vα​∩Vβ​)→φβ​(Vα​∩Vβ​) 是光滑映射，从而{(Vα,φα)∣α=1,⋯ ,m+1}\\{(V_{\\alpha},\\varphi_{\\alpha})|\\alpha = 1,\\cdots,m+1\\}{(Vα​,φα​)∣α=1,⋯,m+1}是RPm\\mathbb{R}P^mRPm上的一个光滑结构，使得RPm\\mathbb{R}P^mRPm为一个mmm维光滑流形，称之为mmm 维实射影空间。 浸入、嵌入和淹没的概念，并举例说明 浸入： MMM 和NNN 都是光滑流形，f:M→Nf:M \\to Nf:M→N 是光滑映射。如果fff 在点p∈Mp \\in Mp∈M 的秩等于MMM 的维数，则称映射fff 在ppp 点为浸入。如果fff 在MMM 的每一点都是浸入，则称fff 为浸入。 嵌入：设f:M→Nf:M \\to Nf:M→N 是单浸入，如果对于f(M)⊂Nf(M) \\subset Nf(M)⊂N 的子空间拓扑，f:M→f(M)f:M \\to f(M)f:M→f(M) 是同胚，则称fff 为嵌入，f(M)f(M)f(M) 为NNN 的嵌入子流形。 淹没：设M,NM,NM,N 分别为mmm 维和nnn 维光滑流形，m&gt;nm&gt;nm&gt;n 。如果光滑映射f:Mm→Nnf:M^m \\to N^nf:Mm→Nn 在点$p \\in M $ 的秩等于NNN 的维数，则称映射fff 在ppp 点为淹没。如果fff 在MMM 的每一点都是淹没，则称fff 为淹没。 参考例1.7，例1.8，例1.9，例1.10 (1). 对于正整数m,n,m&lt;nm,n,m&lt;nm,n,m&lt;n ，有包含映射f:Mm→Nn,f(x1,⋯ ,xm)=(x1,⋯ ,xm,0,⋯ ,0)f: M^m \\to N^n,f(x^1,\\cdots,x^m)=(x^1,\\cdots,x^m,0,\\cdots,0)f:Mm→Nn,f(x1,⋯,xm)=(x1,⋯,xm,0,⋯,0)，则fff是浸入，称为典型浸入。 (2). 设f：R→R2f：\\mathbb{R} \\to \\mathbb{R}^2f：R→R2定义为(t)=(2cos⁡(t−π2),sin⁡2(t−π2)) (t)=\\left(2 \\cos \\left(t-\\frac{\\pi}{2}\\right), \\sin 2\\left(t-\\frac{\\pi}{2}\\right)\\right) (t)=(2cos(t−2π​),sin2(t−2π​)) ，容易验证映射fff为浸入。 (3). 设U⊂RmU \\subset \\mathbb{R}^mU⊂Rm 是开子集，g:U→Rk(k&lt;m)g:U \\to \\mathbb{R}^k(k&lt;m)g:U→Rk(k&lt;m) 定义为： f(x1,⋯ ,xk,xk+1,⋯ ,xm)=(x1,⋯ ,xk)f\\left(x^{1}, \\cdots, x^{k}, x^{k+1}, \\cdots, x^{m}\\right)=\\left(x^{1}, \\cdots, x^{k}\\right) f(x1,⋯,xk,xk+1,⋯,xm)=(x1,⋯,xk) 则fff 是淹没，称为典型淹没 (4). 设f:Rm−0→Rf: \\mathbb{R}^m - {0} \\to \\mathbb{R}f:Rm−0→R定义为f(x1,⋯ ,xm)=∑i=1m(xi)2 f\\left(x^{1}, \\cdots, x^{m}\\right)=\\sum_{i=1}^{m}\\left(x^{i}\\right)^{2} f(x1,⋯,xm)=∑i=1m​(xi)2 ，映射fff 在Rm−0\\mathbb{R}^m - {0}Rm−0 的所有点处秩为1，所以映射fff 为淹没。 单位分解的概念，单位分解定理 设III 为自然数集，若光滑流形MMM 上的一族光滑函数{fi∣i∈I}\\{f_i|i \\in I\\}{fi​∣i∈I} 满足下列性质： 对于任一点p∈M,fi(p)≥0p \\in M,f_i(p) \\ge 0p∈M,fi​(p)≥0 对于任一点p∈M,∑ifi(p)=1p \\in M,\\sum_i f_i(p) = 1p∈M,∑i​fi​(p)=1 {supp(fi)}\\{supp(f_i)\\}{supp(fi​)} 为MMM 的局部有限的覆盖(支集，定义为使得f(p)≠0f(p) \\ne 0f(p)​=0的点集的闭包) 则称{fi∣i∈I}\\{f_i|i \\in I\\}{fi​∣i∈I} 为MMM 上的单位分解。 单位分解定理： 设MMM 为具有可数基的mmm 维光滑流形，{(Ui,φi;Vi)}\\{(U_i,\\varphi_i;V_i)\\}{(Ui​,φi​;Vi​)} 为MMM 的正则覆盖(局部有限覆盖，加细)，则存在单位分解{fi}\\{f_i\\}{fi​} ，使得在ViV_iVi​ 上fi&gt;0f_i &gt; 0fi​&gt;0，且supp(fi)⊂φi−1(B3/4m(0)ˉ)supp(f_i) \\subset \\varphi_i^{-1}(\\bar{B_{3/4}^m(0)})supp(fi​)⊂φi−1​(B3/4m​(0)ˉ​) 。 外积的定义及其性质： 设ξ\\xiξ 是外kkk 次向量(反对称的kkk 阶反变张量)，η\\etaη 是外lll 次向量，令 ξ∧η=(k+l)!k!l!Ak+l(ξ⊗η)\\xi \\wedge \\eta = \\frac{(k+l)!}{k!l!}A_{k+l}(\\xi \\otimes \\eta) ξ∧η=k!l!(k+l)!​Ak+l​(ξ⊗η) 其中Ak+lA_{k+l}Ak+l​ 是反对称化算子，则ξ∧η\\xi \\wedge \\etaξ∧η 是外k+lk+lk+l 次向量，称为外向量ξ\\xiξ 和η\\etaη 的外积。 性质： 外积是双线性的，且满足下列运算规律： （分配律）(aξ1+bξ2)∧η=aξ1∧η+bξ2∧η,ξ∧(aη1+bη2)=aξ∧η1+bξ∧η2\\left(a \\xi_{1}+b \\xi_{2}\\right) \\wedge \\eta=a \\xi_{1} \\wedge \\eta+b \\xi_{2} \\wedge \\eta, \\quad \\xi \\wedge\\left(a \\eta_{1}+b \\eta_{2}\\right)=a \\xi \\wedge \\eta_{1}+b \\xi \\wedge \\eta_{2}(aξ1​+bξ2​)∧η=aξ1​∧η+bξ2​∧η,ξ∧(aη1​+bη2​)=aξ∧η1​+bξ∧η2​ （反交换律）ξ∧η=(−1)klη∧ξ\\xi \\wedge \\eta=(-1)^{k l} \\eta \\wedge \\xiξ∧η=(−1)klη∧ξ （结合律）(ξ∧η)∧ζ=ξ∧(η∧ζ)(\\xi \\wedge \\eta) \\wedge \\zeta=\\xi \\wedge(\\eta \\wedge \\zeta)(ξ∧η)∧ζ=ξ∧(η∧ζ) 其中ξ,ξ1,ξ2∈Λk(V),η,η1,η2∈Λl(V),ζ∈Λh(V),a,b∈F\\xi, \\xi_{1}, \\xi_{2} \\in \\Lambda^{k}(V), \\eta, \\eta_{1}, \\eta_{2} \\in \\Lambda^{l}(V), \\zeta \\in \\Lambda^{h}(V), a, b \\in Fξ,ξ1​,ξ2​∈Λk(V),η,η1​,η2​∈Λl(V),ζ∈Λh(V),a,b∈F 推论 设ξ∈A1(V)=V\\xi \\in A^1(V) = Vξ∈A1(V)=V，则ξ∧ξ=0\\xi \\wedge \\xi = 0ξ∧ξ=0 设{e1,⋯ ,en}\\{e_1,\\cdots,e_n\\}{e1​,⋯,en​} 是VVV 的一组基，则： ei1∧⋯∧eir=r!Ar(ei1⊗⋯⊗eir)e_{i_{1}} \\wedge \\cdots \\wedge e_{i_{r}}=r ! A_{r}\\left(e_{i_{1}} \\otimes \\cdots \\otimes e_{i_{r}}\\right) ei1​​∧⋯∧eir​​=r!Ar​(ei1​​⊗⋯⊗eir​​) 其中1≤i1,⋯ ,ir≤m1 \\le i_1, \\cdots, i_r \\le m1≤i1​,⋯,ir​≤m 向量v1,⋯ ,vr∈Vv_1,\\cdots,v_r \\in Vv1​,⋯,vr​∈V 线性相关的充要条件是 v1∧⋯vr=0v_1 \\wedge \\cdots v_r = 0 v1​∧⋯vr​=0 Cartan引理的内容及其证明： 设v1,⋯ ,vr;w1,⋯ ,wrv_1,\\cdots,v_r;w_1,\\cdots,w_rv1​,⋯,vr​;w1​,⋯,wr​ 是VVV 中两组向量，r≤n=dimVr \\le n = dimVr≤n=dimV ,满足 ∑α=1rvα∧wa=0(1)\\sum_{\\alpha = 1}^{r} v_{\\alpha} \\wedge w_a = 0 \\tag{1} α=1∑r​vα​∧wa​=0(1) 如果v1,⋯ ,vrv_1,\\cdots,v_rv1​,⋯,vr​ 线性无关，则wαw_{\\alpha}wα​ 可表示成它们的线性组合 wα=∑β=1raαβvβ，1≤α≤rw_{\\alpha} = \\sum_{\\beta = 1}^r a_{\\alpha \\beta}v_{\\beta}， \\qquad 1 \\le \\alpha \\le r wα​=β=1∑r​aαβ​vβ​，1≤α≤r 且aαβ=aβαa_{\\alpha \\beta} = a_{\\beta \\alpha}aαβ​=aβα​ 证明： 因为v1,⋯ ,vr∈Vv_1,\\cdots,v_r \\in Vv1​,⋯,vr​∈V 线性无关，则可将他们扩充为VVV 的一组基{v1,⋯ ,vr−1,vr,vr+1,⋯ ,vn}\\{v_1, \\cdots, v_{r-1}, v_r, v_{r+1}, \\cdots, v_n\\}{v1​,⋯,vr−1​,vr​,vr+1​,⋯,vn​}. 则可设 wα=∑β=1raαβvβ+∑i=r+1naαiviw_{\\alpha}=\\sum_{\\beta=1}^{r} a_{\\alpha \\beta} v_{\\beta}+\\sum_{i=r+1}^{n} a_{\\alpha i} v_{i} wα​=β=1∑r​aαβ​vβ​+i=r+1∑n​aαi​vi​ 把上式带入(1)(1)(1) 得： 0=∑α,β=1raαβvα∧vβ+∑α=1r∑i=r+1naαivα∧vi=∑1≤α&lt;β≤r(aαβ−aβα)vα∧vβ+∑α=1r∑i=r+1naαivα∧vi\\begin{aligned} 0 &amp;=\\sum_{\\alpha, \\beta=1}^{r} a_{\\alpha \\beta} v_{\\alpha} \\wedge v_{\\beta}+\\sum_{\\alpha=1}^{r} \\sum_{i=r+1}^{n} a_{\\alpha i} v_{\\alpha} \\wedge v_{i} \\\\ &amp;=\\sum_{1 \\leq \\alpha&lt;\\beta \\leq r}\\left(a_{\\alpha \\beta}-a_{\\beta \\alpha}\\right) v_{\\alpha} \\wedge v_{\\beta}+\\sum_{\\alpha=1}^{r} \\sum_{i=r+1}^{n} a_{\\alpha i} v_{\\alpha} \\wedge v_{i} \\end{aligned} 0​=α,β=1∑r​aαβ​vα​∧vβ​+α=1∑r​i=r+1∑n​aαi​vα​∧vi​=1≤α&lt;β≤r∑​(aαβ​−aβα​)vα​∧vβ​+α=1∑r​i=r+1∑n​aαi​vα​∧vi​​ 因vi∧vj,1≤i&lt;j≤nv_i \\wedge v_j,1 \\le i &lt; j \\le nvi​∧vj​,1≤i&lt;j≤n 是Λ2(V)\\Lambda^{2}(V)Λ2(V) 的一组基，故： aαβ−aβα=0,aαi=0a_{\\alpha \\beta}-a_{\\beta \\alpha}=0, a_{\\alpha i}=0 aαβ​−aβα​=0,aαi​=0 即 wα=∑β=1raαβvβ,1≤α≤rw_{\\alpha}=\\sum_{\\beta=1}^{r} a_{\\alpha \\beta} v_{\\beta}, \\quad 1 \\leq \\alpha \\leq r wα​=β=1∑r​aαβ​vβ​,1≤α≤r 且aαβ=aβαa_{\\alpha \\beta} = a_{\\beta \\alpha}aαβ​=aβα​ 李括号（Poisson括号）的定义及其性质： 设MMM 为光滑流形，对任意X,Y∈Γ(TM)X,Y \\in \\Gamma(TM)X,Y∈Γ(TM)，定义[X,Y]:C∞(M)→R[X,Y]:C^{\\infty}(M) \\to \\mathbb{R}[X,Y]:C∞(M)→R 为 [X,Y](f)=X(Y(f))−Y(X(f))[X, Y](f)=X(Y(f))-Y(X(f)) [X,Y](f)=X(Y(f))−Y(X(f)) 称[X,Y][X,Y][X,Y] 为X,YX,YX,Y 的李括号或Poisson括号。 注： Γ(TM)\\Gamma(TM)Γ(TM):MMM上所有光滑切向量场X:M→TMX: M \\to TMX:M→TM的集合 TM={(p,v)∣p∈M,v∈TpM}TM= \\{(p,v)|p \\in M, v\\in T_pM\\}TM={(p,v)∣p∈M,v∈Tp​M}（TpMT_pMTp​M 为切空间） fff：X∈Γ(TM),f∈C∞(M),p∈MX \\in \\Gamma (TM),f \\in C^{\\infty}(M),p \\in MX∈Γ(TM),f∈C∞(M),p∈M 则(fX)(p)=f(p)X(p)(fX)(p)=f(p)X(p)(fX)(p)=f(p)X(p) 光滑向量场的李括号具有下面的性质： 设MMM 为光滑流形，则对任意X,Y∈Γ(TM),f,g∈C∞(M),λ,μ∈RX,Y \\in \\Gamma(TM),f,g \\in C^{\\infty}(M), \\lambda,\\mu \\in \\mathbb{R}X,Y∈Γ(TM),f,g∈C∞(M),λ,μ∈R ，有 [X,Y](λ⋅f+μ⋅g)=λ⋅[X,Y](f)+μ⋅[X,Y](g)[X, Y](\\lambda \\cdot f+\\mu \\cdot g)=\\lambda \\cdot[X, Y](f)+\\mu \\cdot[X, Y](g)[X,Y](λ⋅f+μ⋅g)=λ⋅[X,Y](f)+μ⋅[X,Y](g) [X,Y](f⋅g)=[X,Y](f)⋅g+f⋅[X,Y](g)[X, Y](f \\cdot g)=[X, Y](f) \\cdot g+f \\cdot[X, Y](g)[X,Y](f⋅g)=[X,Y](f)⋅g+f⋅[X,Y](g) 设MMM 是光滑流形，[,][,][,] 是李括号，则 [X,f⋅Y]=X(f)⋅Y+f⋅[X,Y][f⋅X,Y]=f⋅[X,Y]−Y(f)⋅X\\begin{array}{l} {[X, f \\cdot Y]=X(f) \\cdot Y+f \\cdot[X, Y]} \\\\ {[f \\cdot X, Y]=f \\cdot[X, Y]-Y(f) \\cdot X} \\end{array} [X,f⋅Y]=X(f)⋅Y+f⋅[X,Y][f⋅X,Y]=f⋅[X,Y]−Y(f)⋅X​ ​ 其中X,Y∈Γ(TM),f∈C∞(M)X,Y \\in \\Gamma(TM),f \\in C^{\\infty}(M)X,Y∈Γ(TM),f∈C∞(M) 李群的定义，并举例说明 设GGG 是一个群，并且是mmm 维光滑流形。如果GGG 的乘法运算φ:G×G→G,φ(g1,g2)=g1g2\\varphi: G \\times G \\to G, \\varphi(g_1,g_2) = g_1 g_2φ:G×G→G,φ(g1​,g2​)=g1​g2​ 以及求逆运算τ:G→G,τ(g)=g−1\\tau: G \\to G, \\tau(g) = g^{-1}τ:G→G,τ(g)=g−1 都是光滑映射，则称GGG 是一个mmm 维李群。 举例：例2.2，例2.3 RmR^mRm 关于向量加法成为一个mmm 维李群。 实矩阵构成的一般线性群GL(m,R)GL(m,\\mathbb{R})GL(m,R) 和GL(m,C)GL(m,\\mathbb{C})GL(m,C) 都是李群(GL(m,R)GL(m,\\mathbb{R})GL(m,R)表示mmm阶非退化实矩阵构成的集合)。 GL(m,R)GL(m,\\mathbb{R})GL(m,R)是mmm阶非退化实矩阵构成的集合，群运算是矩阵的乘法，因为GL(m,R)GL(m,\\mathbb{R})GL(m,R) 是Rm2\\mathbb{R}^{m^2}Rm2 的开子集，有自然诱导的光滑结构，容易验证矩阵的乘法运算和求逆运算都是光滑映射，所以GL(m,R)GL(m,\\mathbb{R})GL(m,R) 是m2m^2m2 维李群。类似GL(m,C)GL(m,\\mathbb{C})GL(m,C)是2m22m^22m2 维李群。 **设(G,∗)(G,*)(G,∗) 和 (H,⋅)(H,\\centerdot)(H,⋅)是两个李群，证明乘积流形 $G\\times H $ 有李群的结构 ** G,HG,HG,H是李群, →G,H\\to G,H→G,H 是光滑流形,G×HG \\times HG×H 是光滑流形。 定义G×H={(g,h)∣g∈G,h∈H}G \\times H=\\{(g,h)|g \\in G, h \\in H\\}G×H={(g,h)∣g∈G,h∈H} 在其上定义的乘法运算φ:(G×H)×(G×H)→(G×H):φ((g1,h1),(g2,h2))=(g1∗g2,h1⋅h2)\\varphi: (G\\times H) \\times (G \\times H) \\to (G \\times H):\\quad \\varphi((g_1,h_1),(g_2,h_2)) = (g_1 * g_2,h_1\\centerdot h_2)φ:(G×H)×(G×H)→(G×H):φ((g1​,h1​),(g2​,h2​))=(g1​∗g2​,h1​⋅h2​) 以及求逆运算τ:(G×H)→(G×H):τ((g,h))=(g−1,h−1)\\tau: (G \\times H) \\to (G \\times H): \\quad \\tau((g,h)) = (g^{-1},h^{-1})τ:(G×H)→(G×H):τ((g,h))=(g−1,h−1) 都是光滑映射 从而乘积流形 $G\\times H $ 有李群的结构 Poincare引理及其证明，以及它在古典向量分析中的应用 d2=0d^2 = 0d2=0，即对任意的外微分式www ，有d(dw)=0d(dw) = 0d(dw)=0 **证明：**因为ddd 是线性算子，所以只需取www 为单项式即可。由于外微分ddd 是局部算子，故只需在一个局部坐标系(U,φ;xi)(U,\\varphi;x_i)(U,φ;xi​) 中讨论，设 ω=fdx1∧⋯∧dxr,f∈C∞(M)\\omega=f d x^{1} \\wedge \\cdots \\wedge d x^{r}, f \\in C^{\\infty}(M) ω=fdx1∧⋯∧dxr,f∈C∞(M) 则 dω=df∧dx1∧⋯∧dxrd \\omega=d f \\wedge d x^{1} \\wedge \\cdots \\wedge d x^{r} dω=df∧dx1∧⋯∧dxr 再外微分一次，得 d(dω)=d(df)∧dx1∧⋯∧dxr−df∧d(dx1)∧⋯∧dxr+⋯=0d(d \\omega)=d(d f) \\wedge d x^{1} \\wedge \\cdots \\wedge d x^{r}-d f \\wedge d\\left(d x^{1}\\right) \\wedge \\cdots \\wedge d x^{r}+\\cdots=0 d(dω)=d(df)∧dx1∧⋯∧dxr−df∧d(dx1)∧⋯∧dxr+⋯=0 注：MMM上的外向量丛和外形式丛分别记作Λr(M)=∪p∈MΛr(TpM)\\Lambda^r(M)=\\cup_{p \\in M} \\Lambda^{r}\\left(T_{p} M\\right)Λr(M)=∪p∈M​Λr(Tp​M) 和 Λr(M∗)=⋃p∈MΛr(Tp∗M)\\Lambda^{r}\\left(M^{*}\\right)=\\bigcup_{p \\in M} \\Lambda^{r}\\left(T_{p}^{*} M\\right)Λr(M∗)=⋃p∈M​Λr(Tp∗​M) 用Ar(M)A^r(M)Ar(M)记rrr次外形式丛Λr(M∗)\\Lambda^r(M^*)Λr(M∗)的光滑截面构成的空间，即Ar(M)=Γ(Λr(M∗))A^{r}(M)=\\Gamma\\left(\\Lambda^{r}\\left(M^{*}\\right)\\right)Ar(M)=Γ(Λr(M∗)) 截面空间A(M)A(M)A(M)的元素称为MMM的外微分式，A(M)=∑r=0mAr(M)A(M)=\\sum_{r=0}^{m} A^{r}(M)A(M)=∑r=0m​Ar(M) 证明过程中用到的性质： 若w1w_1w1​是rrr次外微分式，则d(ω1∧ω2)=dω1∧ω2+(−1)rω1∧dω2d\\left(\\omega_{1} \\wedge \\omega_{2}\\right)=d \\omega_{1} \\wedge \\omega_{2}+(-1)^{r} \\omega_{1} \\wedge d \\omega_{2}d(ω1​∧ω2​)=dω1​∧ω2​+(−1)rω1​∧dω2​ 若f∈A0(M)f \\in A^0(M)f∈A0(M), 则d(df)=0d(df)=0d(df)=0 Poincare引理在古典向量分析中的应用： 设(x,y,z)(x,y,z)(x,y,z) 是R3\\mathbb{R}^3R3 上的光滑函数，则 df=∂f∂xdx+∂f∂ydy+∂f∂zdzd f=\\frac{\\partial f}{\\partial x} d x+\\frac{\\partial f}{\\partial y} d y+\\frac{\\partial f}{\\partial z} d z df=∂x∂f​dx+∂y∂f​dy+∂z∂f​dz ​ 其系数构成的向量场(∂f∂x,∂f∂y,∂f∂z)\\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}\\right)(∂x∂f​,∂y∂f​,∂z∂f​) 是fff 的梯度场gradfgrad fgradf. 设w=Adx+Bdy+Cdzw = A dx + Bdy + Cdzw=Adx+Bdy+Cdz ,其中A,B,CA,B,CA,B,C 是R3\\mathbb{R}^3R3 上的光滑函数，则 dω=dA∧dx+dB∧dy+dC∧dz=(∂A∂xdx+∂A∂ydy+∂A∂zdz)∧dx+(∂B∂xdx+∂B∂ydy+∂C∂zdz)∧dy+(∂C∂xdx+∂C∂ydy+∂C∂zdz)∧dz=(∂C∂y−∂B∂z)dy∧dz+(∂A∂z−∂C∂x)dz∧dx+(∂B∂x−∂A∂y)dx∧dy\\begin{aligned} d \\omega &amp;=d A \\wedge d x+d B \\wedge d y+d C \\wedge d z \\\\ &amp;= (\\frac{\\partial A}{\\partial x}dx + \\frac{\\partial A}{\\partial y}dy + \\frac{\\partial A}{\\partial z}dz) \\wedge dx + (\\frac{\\partial B}{\\partial x}dx + \\frac{\\partial B}{\\partial y}dy + \\frac{\\partial C}{\\partial z}dz) \\wedge dy + (\\frac{\\partial C}{\\partial x}dx + \\frac{\\partial C}{\\partial y}dy + \\frac{\\partial C}{\\partial z}dz) \\wedge dz\\\\ &amp;=\\left(\\frac{\\partial C}{\\partial y}-\\frac{\\partial B}{\\partial z}\\right) d y \\wedge d z+\\left(\\frac{\\partial A}{\\partial z}-\\frac{\\partial C}{\\partial x}\\right) d z \\wedge d x+\\left(\\frac{\\partial B}{\\partial x}-\\frac{\\partial A}{\\partial y}\\right) d x \\wedge d y \\end{aligned} dω​=dA∧dx+dB∧dy+dC∧dz=(∂x∂A​dx+∂y∂A​dy+∂z∂A​dz)∧dx+(∂x∂B​dx+∂y∂B​dy+∂z∂C​dz)∧dy+(∂x∂C​dx+∂y∂C​dy+∂z∂C​dz)∧dz=(∂y∂C​−∂z∂B​)dy∧dz+(∂z∂A​−∂x∂C​)dz∧dx+(∂x∂B​−∂y∂A​)dx∧dy​ ​ 若记向量场X=(A,B,C)X = (A,B,C)X=(A,B,C) ，则dwdwdw 的系数构成的向量场 (∂C∂y−∂B∂z,∂A∂z−∂C∂x,∂B∂x−∂A∂y)\\left(\\frac{\\partial C}{\\partial y}-\\frac{\\partial B}{\\partial z}, \\frac{\\partial A}{\\partial z}-\\frac{\\partial C}{\\partial x}, \\frac{\\partial B}{\\partial x}-\\frac{\\partial A}{\\partial y}\\right) (∂y∂C​−∂z∂B​,∂z∂A​−∂x∂C​,∂x∂B​−∂y∂A​) ​ 恰是向量场XXX 的旋度curlXcurl XcurlX 设ω=Ady∧dz+Bdz∧dx+Cdx∧dy\\omega=A d y \\wedge d z+B d z \\wedge d x+C d x \\wedge d yω=Ady∧dz+Bdz∧dx+Cdx∧dy ，则 dω=dA∧dy∧dz+dB∧dz∧dx+dC∧dx∧dy=∂A∂xdx∧dy∧dz+∂B∂ydy∧dz∧dx+∂C∂zdz∧dx∧dy=(∂A∂x+∂B∂y+∂C∂z)dx∧dy∧dz=div⁡dx∧dy∧dz\\begin{aligned} d \\omega &amp;= dA \\wedge dy \\wedge dz + dB \\wedge dz \\wedge dx + dC \\wedge dx \\wedge dy\\\\ &amp;= \\frac{\\partial A}{\\partial x} dx \\wedge dy \\wedge dz + \\frac{\\partial B}{\\partial y} dy \\wedge dz \\wedge dx + \\frac{\\partial C}{\\partial z} dz \\wedge dx \\wedge dy\\\\ &amp;=\\left(\\frac{\\partial A}{\\partial x}+\\frac{\\partial B}{\\partial y}+\\frac{\\partial C}{\\partial z}\\right) d x \\wedge d y \\wedge d z \\\\ &amp;=\\operatorname{div} d x \\wedge d y \\wedge d z \\end{aligned} dω​=dA∧dy∧dz+dB∧dz∧dx+dC∧dx∧dy=∂x∂A​dx∧dy∧dz+∂y∂B​dy∧dz∧dx+∂z∂C​dz∧dx∧dy=(∂x∂A​+∂y∂B​+∂z∂C​)dx∧dy∧dz=divdx∧dy∧dz​ ​ 其中X=(A,B,C),divXX=(A,B,C),divXX=(A,B,C),divX 表示向量场XXX 的散度。 由Poincare定理，立即得到古典场论的两个基本公式：curl(gradf)=0,div(curlX)=0curl(grad f) = 0, \\quad div(curl X)=0curl(gradf)=0,div(curlX)=0 流行定向的概念 设MMM 是一个微分流形，如果在MMM 上存在一族容许局部坐标系U={(Uα,φα)∣α∈I}\\mathscr{U}=\\left\\{\\left(U_{\\alpha}, \\varphi_{\\alpha}\\right) \\mid \\alpha \\in I\\right\\}U={(Uα​,φα​)∣α∈I} 满足下面两个条件： (1). ⋃α∈IUα=M\\bigcup_{\\alpha \\in I} U_{\\alpha}=M⋃α∈I​Uα​=M (2). 对任意两个局部坐标系Uα,φα;xiU_{\\alpha},\\varphi_{\\alpha};x^iUα​,φα​;xi 和Uβ,φβ;yiU_{\\beta},\\varphi_{\\beta};y^iUβ​,φβ​;yi, Uα⋂Uβ=∅U_{\\alpha} \\bigcap U_{\\beta} = \\emptysetUα​⋂Uβ​=∅ 时，在Uα⋂UβU_{\\alpha} \\bigcap U_{\\beta}Uα​⋂Uβ​ 上有 ∂(xα1,⋯ ,xαm)∂(yβ1,⋯ ,yβm)&gt;0\\frac{\\partial\\left(x_{\\alpha}^{1}, \\cdots, x_{\\alpha}^{m}\\right)}{\\partial\\left(y_{\\beta}^{1}, \\cdots, y_{\\beta}^{m}\\right)}&gt;0 ∂(yβ1​,⋯,yβm​)∂(xα1​,⋯,xαm​)​&gt;0 则称MMM 是可定向微分流形。满足(2)条件的两个局部坐标系 Uα,φα;xi U_{\\alpha},\\varphi_{\\alpha};x^i Uα​,φα​;xi和Uβ,φβ;yiU_{\\beta},\\varphi_{\\beta};y^iUβ​,φβ​;yi 称为定向相符的。 设 MMM 是可定向的 mmm 维微分流形，如果U={(Uα,φα)∣α∈I}\\mathscr{U}=\\left\\{\\left(U_{\\alpha}, \\varphi_{\\alpha}\\right) \\mid \\alpha \\in I\\right\\}U={(Uα​,φα​)∣α∈I} 是满足上面两个条件的一族局部坐标系，并且满足： (3). U\\mathscr{U}U是极大的，即对任意的容许局部坐标系U;xiU;x^iU;xi ，如果对于任意的α∈I\\alpha \\in Iα∈I, (U;xi)(U;x^i)(U;xi)和Uα;xαiU_{\\alpha};x^i_{\\alpha}Uα​;xαi​都是定向相符的，便有(U;xi)∈U(U;x^i) \\in \\mathscr{U}(U;xi)∈U 则称U\\mathscr{U}U是 MMM 的一个定向。具有指定定向的微分流形称为定向微分流形。 或通过外微分式给出等价的定义 mmm维光滑流形 MMM 称为可定向的，如果在 MMM 上存在一个连续的，处处不为零的mmm 次外微分式。如果在MMM上给定了这样一个外微分式，则称MMM是定向的。如果给出MMM 定向的两个外微分式彼此差一个处处为正的函数因子，则称他们规定了MMM的一个方向。 流形上外微分式的积分如何定义 设MMM 是mmm 维定向的光滑流形，φ\\varphiφ 是MMM 上有紧致支集的mmm 次微分式，由 ∫Mφ=∑α∫Mgα⋅φ\\int_{M} \\varphi=\\sum_{\\alpha} \\int_{M} g_{\\alpha} \\cdot \\varphi ∫M​φ=α∑​∫M​gα​⋅φ 所定义的数∫Mφ\\int_{M} \\varphi∫M​φ 称为外微分式φ\\varphiφ 在MMM 上的积分。 注：任取MMM 的一个定向相符的坐标卡构成的覆盖Σ={Wi}\\Sigma = \\{W_i\\}Σ={Wi​}， 设{gα}\\{g_{\\alpha}\\}{gα​} 是从属于Σ\\SigmaΣ 的单位分解，则φ=(∑αgα)⋅φ=∑α(gα⋅φ)\\varphi=(\\sum_{\\alpha}g_{\\alpha}) \\centerdot \\varphi = \\sum_{\\alpha}(g_{\\alpha} \\centerdot \\varphi)φ=(∑α​gα​)⋅φ=∑α​(gα​⋅φ) 用实例解释说明流形上的Stocks公式 设MMM 是mmm 维定向光滑流形，www 是MMM 上具有紧致支集的m−1m-1m−1 次外微分式，则 ∫∂Mω=∫Mdω\\int_{\\partial M} \\omega=\\int_{M} d \\omega ∫∂M​ω=∫M​dω 若∂M=∅\\partial M = \\emptyset∂M=∅, 则规定左边的积分是零。 参考例3.4 设Σ\\SigmaΣ 是R3\\mathbb{R}^3R3 中的一块定向曲面，其边界∂Σ\\partial \\Sigma∂Σ 为定向闭曲线，而且∂Σ\\partial \\Sigma∂Σ 的正向法向量符合右手法则。设P,Q,RP,Q,RP,Q,R 是包含在Σ\\SigmaΣ 在内的一个区域上的连续可微函数，则有Stocks公式： ∫∂ΣPdx+Qdy+Rdz=∬Σ(∂R∂y−∂Q∂z)dydz+(∂P∂z−∂R∂x)dzdx+(∂Q∂x−∂P∂y)dxdy\\int_{\\partial \\Sigma} P d x+Q d y+R d z=\\iint_{\\Sigma}\\left(\\frac{\\partial R}{\\partial y}-\\frac{\\partial Q}{\\partial z}\\right) d y d z+\\left(\\frac{\\partial P}{\\partial z}-\\frac{\\partial R}{\\partial x}\\right) d z d x+\\left(\\frac{\\partial Q}{\\partial x}-\\frac{\\partial P}{\\partial y}\\right) d x d y ∫∂Σ​Pdx+Qdy+Rdz=∬Σ​(∂y∂R​−∂z∂Q​)dydz+(∂z∂P​−∂x∂R​)dzdx+(∂x∂Q​−∂y∂P​)dxdy 若记w=Pdx+Qdy+Rdzw = Pdx+Qdy+Rdzw=Pdx+Qdy+Rdz，则上式可以写成 ∫∂Σω=∫Σdω\\int_{\\partial \\Sigma} \\omega=\\int_{\\Sigma} d \\omega\\\\ ∫∂Σ​ω=∫Σ​dω 叙述Brouwer映射度的定义和性质， 并利用Brouwer映射度证明6维单位球面S6S^6S6上的恒同映射和对径映射不同伦 定义 设p∈Mp \\in Mp∈M 是光滑映射f:Mn→Nnf: M^n \\to N ^nf:Mn→Nn 的一个正则点，MMM 是紧致光滑的定向流形，NNN 是连通光滑的定向流形，则切映射f∗p:TpM→Tf(p)Nf_{*p}:T_pM \\to T_{f(p)}Nf∗p​:Tp​M→Tf(p)​N 为定向向量空间之间的线性同构，根据同构保持定向或反转定向，我们定义f∗pf_{*p}f∗p​ 的符号，以及在ppp 处的度数degpfdeg_pfdegp​f 和正则点的类型如下： 同构f∗pf_{*p}f∗p​ f∗pf_{*p}f∗p​ 的符号 degpfdeg_pfdegp​f 类型 保持定向 +1 +1 正型 反转定向 -1 -1 负型 进而，对于fff 的正则值q∈Nq \\in Nq∈N，定义 deg⁡(f,q)=∑p∈f−1(q)deg⁡pf\\operatorname{deg}(f, q)=\\sum_{p \\in f^{-1}(q)} \\operatorname{deg}_{p} f deg(f,q)=p∈f−1(q)∑​degp​f 称之为fff 关于正则值qqq 的Brouwer度。 整数deg(f,q)deg(f,q)deg(f,q) 不依赖于正则值qqq 的选取，把整数deg(f,q)deg(f,q)deg(f,q) 称为映射fff 的Brouwer度或映射度，记为deg(f)deg(f)deg(f) 。 性质 设M,N,PM,N,PM,N,P 是具有相同维数的定向光滑流形，且MMM 和NNN 紧致，NNN 和PPP 连通,则 (1). 若f:M→Nf: M \\to Nf:M→N 和g:N→Pg: N \\to Pg:N→P 为光滑映射，则deg⁡(g∘f)=deg⁡(g)⋅deg⁡(f)\\operatorname{deg}(g \\circ f)=\\operatorname{deg}(g) \\cdot \\operatorname{deg}(f)deg(g∘f)=deg(g)⋅deg(f) ； (2). 若id:M→Mid: M \\to Mid:M→M 为恒同映射，则deg(id)=1deg(id)=1deg(id)=1，即恒同映射的映射度为1； (3). 若f:M→Nf: M \\to Nf:M→N 为微分同胚，则deg(f)=±1deg(f)= \\pm 1deg(f)=±1 ； (4). 若f,g:M→Nf,g: M \\to Nf,g:M→N 为C∞C^{\\infty}C∞ 映射，且fff 同伦于ggg， 则deg(f)=deg(g)deg(f)= deg(g)deg(f)=deg(g)，即映射度是同伦不变量。 证明 参考例3.5 例3.5考虑nnn 单位球面 Sn(n≥1) S^n(n \\ge 1) Sn(n≥1)上的恒同映射和对径映射.显然，SnS^nSn 上的恒同映射的Brouwer 度为1.我们定义SSS上的反射ri:Sn→Snr_i:S_n \\to S_nri​:Sn​→Sn​为 ri:(x1,⋯ ,xn+1)=(x1,⋯ ,xi−1,−xi,xi+1,⋯ ,an+1),i=1,⋯ ,n+1r_i:(x_1,\\cdots,x_{n+1})=(x_1,\\cdots ,x_{i-1},-x_i,x{i+1},\\cdots,a_{n+1}),i=1,\\cdots ,n+1 ri​:(x1​,⋯,xn+1​)=(x1​,⋯,xi−1​,−xi​,xi+1,⋯,an+1​),i=1,⋯,n+1 它是一个反转定向的微分同胚，故其映射度deg(ri)=−1deg(r_i)=-1deg(ri​)=−1. SnS_nSn​ 上的对径映射r:Sn→Snr: S^n \\to S^nr:Sn→Sn 定义为 r(x)=−xr(x)=-xr(x)=−x，它可以看成是n+1n+1n+1个反射的复合，即r=r1∘r2∘⋯∘rn+1r=r_{1} \\circ r_{2} \\circ \\cdots \\circ r_{n+1}r=r1​∘r2​∘⋯∘rn+1​,其映射度为 deg⁡(r)=deg⁡(r1)deg⁡(r2)⋯deg⁡(rn+1)=(−1)n+1\\operatorname{deg}(r)=\\operatorname{deg}\\left(r_{1}\\right) \\operatorname{deg}\\left(r_{2}\\right) \\cdots \\operatorname{deg}\\left(r_{n+1}\\right)=(-1)^{n+1} deg(r)=deg(r1​)deg(r2​)⋯deg(rn+1​)=(−1)n+1 当n为偶数时，deg(r)=−1deg(r)=-1deg(r)=−1，故SnS^nSn 上的对径映射和恒同映射不同伦. 黎曼度量的定义。证明：任意光滑流形上都存在黎曼度量。 设MMM 是mmm 维光滑流形，MMM 上的一个黎曼度量ggg 是MMM 上的一个光滑的二阶协变张量场，使得对每一点p∈M,g(p)p \\in M,g(p)p∈M,g(p) 是切空间Tp(M)T_p(M)Tp​(M) 上的一个对称，正定的二阶协变张量(M,g)(M,g)(M,g) 称为mmm 维黎曼流形。 注：g(p)∈Tp∗M⊗Tp∗M:Tp(M)×Tp(M)→Rg(p) \\in T_p^*M \\otimes T_p^*M:T_p(M) \\times T_p(M) \\to \\mathbb{R}g(p)∈Tp∗​M⊗Tp∗​M:Tp​(M)×Tp​(M)→R 证明：定理4.1 设MMM 是一个满足第二可数公理的mmm 维光滑流形，则在MMM 上必存在黎曼度量。 证明: 由于在流形的每一个局部坐标邻域上都可以给定一个黎曼度量。非常自然的想法就是利用单位分解定理，把这些局部定义的黎曼度量拼接成为流形M上的一个黎曼度量。 (1). 取定局部坐标邻域与单位分解，在每一个局部邻域上给定黎曼度量 由于M满足第二可数公理，可取MMM的一个局部有限的坐标覆盖{Uα;xαi∣α∈I}\\{U_{\\alpha};x^i_{\\alpha}|\\alpha \\in I\\}{Uα​;xαi​∣α∈I}，其中III是自然数集。由单位分解定理，存在MMM上的光滑函数族{fa}\\{f_a\\}{fa​}，使得对任意的a∈Ia \\in Ia∈I，有 supp⁡fα⊂Uα,0≤fα≤1,∑α∈Ifα=1\\operatorname{supp} f_{\\alpha} \\subset U_{\\alpha}, \\quad 0 \\leq f_{\\alpha} \\leq 1, \\sum_{\\alpha \\in I} f_{\\alpha}=1 suppfα​⊂Uα​,0≤fα​≤1,α∈I∑​fα​=1 对于每一个α∈I\\alpha \\in Iα∈I，在UαU_{\\alpha}Uα​ 上定义黎曼度量 g(α)=∑i=1mdxαi⊗dxαig^{(\\alpha)}=\\sum_{i=1}^{m} d x_{\\alpha}^{i} \\otimes d x_{\\alpha}^{i} g(α)=i=1∑m​dxαi​⊗dxαi​ (2). 将局部定义的黎曼度量拼接 利用g(α)g^{(\\alpha)}g(α), 在MMM 上如下定义二阶协变张量场gαg_{\\alpha}gα​，对任意p∈Mp \\in Mp∈M，令 gα(p)={fα(p)⋅g(α)(p),p∈Uα0,p∉Uαg_{\\alpha}(p)=\\left\\{\\begin{array}{c}f_{\\alpha}(p) \\cdot g^{(\\alpha)}(p), p \\in U_{\\alpha} \\\\ 0, p \\notin U_{\\alpha}\\end{array}\\right. gα​(p)={fα​(p)⋅g(α)(p),p∈Uα​0,p∈/​Uα​​ 由于fα⋅g(α)f_{\\alpha}\\centerdot g^{(\\alpha)}fα​⋅g(α) 在UαU_{\\alpha}Uα​ 上光滑，且Suppgα∈Suppfα∈UαSuppg_{\\alpha} \\in Suppf_{\\alpha} \\in U_{\\alpha}Suppgα​∈Suppfα​∈Uα​，易见gαg_{\\alpha}gα​ 是大范围定义在MMM上的光滑张量场 令 g=∑αgαg=\\sum_{\\alpha} g_{\\alpha} g=α∑​gα​ 根据覆盖{Uα∣α∈I}\\{U_{\\alpha}|\\alpha \\in I\\}{Uα​∣α∈I} 的局部有限性，上式右端在每一点p∈Mp \\in Mp∈M的某个邻域上是有限多项之和.所以ggg 是大范围定义在MMM 上的光滑，对称二阶协变张量场. (3). 证明正定 下面证明ggg 正定.对任意一点$ p \\in M$ ，由于 0≤fα≤1,∑α∈Ifα=10 \\leq f_{\\alpha} \\leq 1, \\sum_{\\alpha \\in I} f_{\\alpha}=1 0≤fα​≤1,α∈I∑​fα​=1 必有β∈I\\beta \\in Iβ∈I， 使得fβ(p)&gt;0f_{\\beta}(p) &gt; 0fβ​(p)&gt;0 。则对任意的v∈Tp(M)v \\in T_p(M)v∈Tp​(M)，有 (g(p))(v,v)=∑αfα(p)⋅gα(v,v)≥fβ(p)∑i=1m(dxβi(v))2≥0(g(p))(v, v)=\\sum_{\\alpha} f_{\\alpha}(p) \\cdot g^{\\alpha}(v, v) \\geq f_{\\beta}(p) \\sum_{i=1}^{m}\\left(d x_{\\beta}^{i}(v)\\right)^{2} \\geq 0 (g(p))(v,v)=α∑​fα​(p)⋅gα(v,v)≥fβ​(p)i=1∑m​(dxβi​(v))2≥0 当(g(p))(v,v)=0(g(p))(v,v)=0(g(p))(v,v)=0 时，因为fβ&gt;0f_{\\beta} &gt; 0fβ​&gt;0 ，所以 dxβi(v)=0,1≤i≤md x_{\\beta}^{i}(v)=0,1 \\leq i \\leq m dxβi​(v)=0,1≤i≤m 即v=0v=0v=0 ，因此ggg 是正定的，从而ggg 是MMM 上的一个黎曼度量。 向量丛上联络的定义 向量丛EEE上的联络是一个满足下列条件的映射: ∇:Γ(E)→Γ(T∗(M)⊗E)\\nabla: \\Gamma(E) \\rightarrow \\Gamma\\left(T^{*}(M) \\otimes E\\right) ∇:Γ(E)→Γ(T∗(M)⊗E) (1). 对任意的s1,s2∈Γ(E)s_1,s_2 \\in \\Gamma(E)s1​,s2​∈Γ(E)，有 ∇(s1+s2)=∇s1+∇s2\\nabla\\left(s_{1}+s_{2}\\right)=\\nabla s_{1}+\\nabla s_{2} ∇(s1​+s2​)=∇s1​+∇s2​ (2). 对任意的s∈Γ(E)s \\in \\Gamma(E)s∈Γ(E) 以及任意的f∈C∞(M)f \\in C^{\\infty}(M)f∈C∞(M)，有∇(fs)=df⊗s+f∇s\\nabla(f s)=d f \\otimes s+f \\nabla s∇(fs)=df⊗s+f∇s 注：Γ(E):\\Gamma(E):Γ(E): 光滑向量场，EEE的全体光滑截面的集合 EEE 是MMM 维光滑流形MMM 上的一个nnn 维向量丛 T∗(M)T^*(M)T∗(M) 为MMM 的切丛。 什么是Levi-Civita联络？设(M,g)(M,g)(M,g) 是一个黎曼流形，证明Levi-Civita联络 ∇\\nabla∇ 是MMM 的切从TMTMTM 上的联络。 设(M,g)(M,g)(M,g) 是黎曼流形，则通过下式定义的映射∇:Γ(TM)×Γ(TM)→Γ(TM)\\nabla: \\Gamma(T M) \\times \\Gamma(T M) \\rightarrow \\Gamma(T M)∇:Γ(TM)×Γ(TM)→Γ(TM) g(∇XY,Z)=12{X(g(Y,Z))+Y(g(Z,X))−Z(g(X,Y))+g(Z,[X,Y])+g(Y,[Z,X])−g(X,[Y,Z])}g\\left(\\nabla_{X} Y, Z\\right)=\\frac{1}{2}\\{X(g(Y, Z))+Y(g(Z, X))-Z(g(X, Y))+g(Z,[X, Y])+g(Y,[Z, X])-g(X,[Y, Z])\\} g(∇X​Y,Z)=21​{X(g(Y,Z))+Y(g(Z,X))−Z(g(X,Y))+g(Z,[X,Y])+g(Y,[Z,X])−g(X,[Y,Z])} 称为MMM 上的黎曼联络，或Levi-Civita联络 证明：参考定理4.7 对任意X,Y1,Y2,Z∈Γ(TM),λ,μ∈RX,Y_1,Y_2,Z \\in \\Gamma(TM),\\lambda,\\mu \\in \\mathbb{R}X,Y1​,Y2​,Z∈Γ(TM),λ,μ∈R，根据李括号的定义，并利用ggg 是一个张量场易得 g(∇X(λ⋅Y1+μ⋅Y2),Z)=λ⋅g(∇XY1,Z)+μ⋅g(∇XY2,Z)g(∇Y1+Y2X,Z)=g(∇Y1X,Z)+g(∇Y2X,Z)\\begin{array}{c} g\\left(\\nabla_{X}\\left(\\lambda \\cdot Y_{1}+\\mu \\cdot Y_{2}\\right), Z\\right)=\\lambda \\cdot g\\left(\\nabla_{X} Y_{1}, Z\\right)+\\mu \\cdot g\\left(\\nabla_{X} Y_{2}, Z\\right) \\\\ g\\left(\\nabla_{Y_{1}+Y_{2}} X, Z\\right)=g\\left(\\nabla_{Y_{1}} X, Z\\right)+g\\left(\\nabla_{Y_{2}} X, Z\\right) \\end{array} g(∇X​(λ⋅Y1​+μ⋅Y2​),Z)=λ⋅g(∇X​Y1​,Z)+μ⋅g(∇X​Y2​,Z)g(∇Y1​+Y2​​X,Z)=g(∇Y1​​X,Z)+g(∇Y2​​X,Z)​ 进一步，对任意的f∈C∞(M)f \\in C^{\\infty}(M)f∈C∞(M)，有 g(∇XfY,Z)=12{X(f⋅g(Y,Z))+f⋅Y(g(Z,X))−Z(f⋅g(X,Y))+g(Z,[X,f⋅Y])+f⋅g(Y,[Z,X])−g(X,[f⋅Y,Z])}=12{X(f)⋅g(Y,Z)+f⋅X(g(Y,Z))+f⋅Y(g(Z,X))−Z(f)⋅g(X,Y)−f⋅Z(g(X,Y))+g(Z,X(f)⋅Y+f⋅[X,Y])+f⋅g(Y,[Z,X])−g(X,−Z(f)⋅Y+f⋅[Y,Z])}=X(f)⋅g(Y,Z)+f⋅g(∇XY,Z)=g(X(f)⋅Y+f⋅∇XY,Z)\\begin{aligned} g\\left(\\nabla_{X} f Y, Z\\right)=&amp; \\frac{1}{2}\\{X(f \\cdot g(Y, Z))+f \\cdot Y(g(Z, X))-Z(f \\cdot g(X, Y))\\\\ &amp;+g(Z,[X, f \\cdot Y])+f \\cdot g(Y,[Z, X])-g(X,[f \\cdot Y, Z])\\} \\\\ =&amp; \\frac{1}{2}\\{X(f) \\cdot g(Y, Z)+f \\cdot X(g(Y, Z))+f \\cdot Y(g(Z, X))\\\\ &amp;-Z(f) \\cdot g(X, Y)-f \\cdot Z(g(X, Y))+g(Z, X(f) \\cdot Y+f \\cdot[X, Y]) \\\\ &amp;+f \\cdot g(Y,[Z, X])-g(X,-Z(f) \\cdot Y+f \\cdot[Y, Z])\\} \\\\ =&amp; X(f) \\cdot g(Y, Z)+f \\cdot g\\left(\\nabla_{X} Y, Z\\right) \\\\ =&amp; g\\left(X(f) \\cdot Y+f \\cdot \\nabla_{X} Y, Z\\right) \\end{aligned} g(∇X​fY,Z)====​21​{X(f⋅g(Y,Z))+f⋅Y(g(Z,X))−Z(f⋅g(X,Y))+g(Z,[X,f⋅Y])+f⋅g(Y,[Z,X])−g(X,[f⋅Y,Z])}21​{X(f)⋅g(Y,Z)+f⋅X(g(Y,Z))+f⋅Y(g(Z,X))−Z(f)⋅g(X,Y)−f⋅Z(g(X,Y))+g(Z,X(f)⋅Y+f⋅[X,Y])+f⋅g(Y,[Z,X])−g(X,−Z(f)⋅Y+f⋅[Y,Z])}X(f)⋅g(Y,Z)+f⋅g(∇X​Y,Z)g(X(f)⋅Y+f⋅∇X​Y,Z)​ 以及 g(∇f⋅XY,Z)=12{f⋅X(g(Y,Z))+Y(f⋅g(Z,X))−Z(f⋅g(X,Y))+g(Z,[f⋅X,Y])+g(Y,[Z,f⋅X])−f⋅g(X,[Y,Z])}=12{f⋅X(g(Y,Z))+Y(f)⋅g(Z,X)+f⋅Y(g(Z,X))−Z(f)⋅g(X,Y)−f⋅Z(g(X,Y))+g(Z,−Y(f)⋅X)+g(Z,f⋅[X,Y])+g(Y,Z(f)⋅X)+f⋅g(Y,[Z,X])−f⋅g(X,[Y,Z])}=f⋅g(∇XY,Z)\\begin{aligned} g\\left(\\nabla_{f \\cdot X} Y, Z\\right)=&amp; \\frac{1}{2}\\{f \\cdot X(g(Y, Z))+Y(f \\cdot g(Z, X))-Z(f \\cdot g(X, Y))+g(Z,[f \\cdot X, Y])\\\\ &amp;+g(Y,[Z, f \\cdot X])-f \\cdot g(X,[Y, Z])\\} \\\\ =&amp; \\frac{1}{2}\\{f \\cdot X(g(Y, Z))+Y(f) \\cdot g(Z, X)+f \\cdot Y(g(Z, X))\\\\ &amp;-Z(f) \\cdot g(X, Y)-f \\cdot Z(g(X, Y))+g(Z,-Y(f) \\cdot X) \\\\ &amp;+g(Z, f \\cdot[X, Y])+g(Y, Z(f) \\cdot X)+f \\cdot g(Y,[Z, X])-f \\cdot g(X,[Y, Z])\\} \\\\ =&amp; f \\cdot g\\left(\\nabla_{X} Y, Z\\right) \\end{aligned} g(∇f⋅X​Y,Z)===​21​{f⋅X(g(Y,Z))+Y(f⋅g(Z,X))−Z(f⋅g(X,Y))+g(Z,[f⋅X,Y])+g(Y,[Z,f⋅X])−f⋅g(X,[Y,Z])}21​{f⋅X(g(Y,Z))+Y(f)⋅g(Z,X)+f⋅Y(g(Z,X))−Z(f)⋅g(X,Y)−f⋅Z(g(X,Y))+g(Z,−Y(f)⋅X)+g(Z,f⋅[X,Y])+g(Y,Z(f)⋅X)+f⋅g(Y,[Z,X])−f⋅g(X,[Y,Z])}f⋅g(∇X​Y,Z)​ 证毕 曲率张量的定义以及性质 设 ((M,∇))( (M, \\nabla) )((M,∇)) 是 mmm 维仿射联络空间. 对任意的 $ X, Y, Z \\in \\Gamma(T M)$, 定义 R(X,Y):Γ(TM)→Γ(TM)R(X, Y): \\Gamma(T M) \\rightarrow \\Gamma(T M)R(X,Y):Γ(TM)→Γ(TM) 为 R(X,Y)Z=∇X,Y2Z−∇Y,X2Z=∇X∇YZ−∇Y∇XZ−∇[X,Y]ZR(X, Y) Z=\\nabla_{X, Y}^{2} Z-\\nabla_{Y, X}^{2} Z=\\nabla_{X} \\nabla_{Y} Z-\\nabla_{Y} \\nabla_{X} Z-\\nabla_{[X, Y]} Z R(X,Y)Z=∇X,Y2​Z−∇Y,X2​Z=∇X​∇Y​Z−∇Y​∇X​Z−∇[X,Y]​Z 称R(X,Y)R(X,Y)R(X,Y)为仿射联络空间(M,∇)(M,\\nabla)(M,∇) 关于光滑切向量场X,YX,YX,Y的曲率算子.RRR 是一个(1,3)(1,3)(1,3)型的光滑张 量场,称为仿射联络空间(M,∇)(M,\\nabla)(M,∇) 的曲率张量 设(M,∇)(M,\\nabla)(M,∇) 为仿射联络空间，则对任意的X,Y∈Γ(TM)X,Y \\in \\Gamma(TM)X,Y∈Γ(TM)，曲率算子R(X,Y)R(X,Y)R(X,Y) 具有下面的性质： $R(X, Y)=-R(Y, X) $ R(fX,Y)=R(X,fY)=fR(X,Y)R(f X, Y)=R(X, f Y)=f R(X, Y)R(fX,Y)=R(X,fY)=fR(X,Y) $ R(X, Y)(f Z)=f R(X, Y) Z $ 当 $ \\nabla $ 的抗率 T≡0T \\equiv 0T≡0 时 R(X,Y)Z+R(Z,X)Y+R(Y,Z)X=0R(X, Y) Z+R(Z, X) Y+R(Y, Z) X=0 R(X,Y)Z+R(Z,X)Y+R(Y,Z)X=0 其中 $ f \\in C^{\\infty}(M), Z \\in \\Gamma(T M) $ 黎曼曲率张量的定义以及性质 对于黎曼流形(M,g)(M,g)(M,g)，它有唯一确定的Levi-Civita联络∇\\nabla∇，它的曲率张量称为黎曼流形(M,g)(M,g)(M,g)的黎曼曲率张量. 设(M,g)(M,g)(M,g) 是黎曼流形，对任意X,Y,Z,W∈Γ(TM)X,Y,Z,W \\in \\Gamma(TM)X,Y,Z,W∈Γ(TM)，令 R(X,Y,Z,W)=g(R(Z,W)X,Y)\\mathcal{R}(X, Y, Z, W)=g(R(Z, W) X, Y) R(X,Y,Z,W)=g(R(Z,W)X,Y) 则得到一个四重线性映射 R(X,Y,Z,W)=g(R(Z,W)X,Y)\\mathcal{R}(X, Y, Z, W)=g(R(Z, W) X, Y) R(X,Y,Z,W)=g(R(Z,W)X,Y) 它是MMM 上的四阶协变张量场，称之为黎曼流形(M,g)(M,g)(M,g) 的黎曼曲率张量场 设(M,g)(M,g)(M,g) 是一个光滑的黎曼流形，则对任意X,Y,Z,W∈Γ(TM)X,Y,Z,W \\in \\Gamma(TM)X,Y,Z,W∈Γ(TM)，黎曼曲率张量场 R(X,Y,Z,W)=g(R(Z,W)X,Y)\\mathcal{R}(X, Y, Z, W)=g(R(Z, W) X, Y) R(X,Y,Z,W)=g(R(Z,W)X,Y) 具有下面的性质： 反对称性 R(X,Y,Z,W)=−R(Y,X,Z,W)\\mathcal{R}(X,Y,Z,W) = -\\mathcal{R}(Y,X,Z,W) R(X,Y,Z,W)=−R(Y,X,Z,W) R(X,Y,Z,W)=−R(X,Y,W,Z)\\mathcal{R}(X,Y,Z,W) = -\\mathcal{R}(X,Y,W,Z) R(X,Y,Z,W)=−R(X,Y,W,Z) 第一Bianchi恒等式 R(X,Y,Z,W)+R(Z,Y,W,X)+R(W,Y,X,Z)=0\\mathcal{R}(X,Y,Z,W) + \\mathcal{R}(Z,Y,W,X) + \\mathcal{R}(W,Y,X,Z) = 0 R(X,Y,Z,W)+R(Z,Y,W,X)+R(W,Y,X,Z)=0 对称性 R(X,Y,Z,W)=R(Z,W,X,Y)\\mathcal{R}(X,Y,Z,W) = \\mathcal{R}(Z,W,X,Y) R(X,Y,Z,W)=R(Z,W,X,Y) **在三维光滑流形M=R3M = \\mathbb{R}^3M=R3 上，令α=dx1−x1dx2∈Ω1(M),β=x2dx1∧dx3−dx2∧dx3∈Ω2(M)\\alpha = dx^1 - x^1 dx^2 \\in \\Omega^1(M), \\beta = x^2dx^1 \\wedge dx^3 - dx^2 \\wedge dx^3 \\in \\Omega^2(M)α=dx1−x1dx2∈Ω1(M),β=x2dx1∧dx3−dx2∧dx3∈Ω2(M) ，计算 dα,dβd \\alpha, d \\betadα,dβ 以及 α∧β\\alpha \\wedge \\betaα∧β ** dα=−dx1∧dx2dβ=dx2∧dx1∧dx3d \\alpha = -dx^1 \\wedge dx^2 \\qquad d \\beta = dx^2 \\wedge dx^1 \\wedge dx^3 dα=−dx1∧dx2dβ=dx2∧dx1∧dx3 α∧β=−dx1∧dx2∧dx3−x1dx2∧x2dx1∧dx3=(x1x2−1)dx1∧dx2∧dx3\\begin{aligned} \\alpha \\wedge \\beta &amp;= -dx^1 \\wedge dx^2 \\wedge dx^3 - x^1 dx^2 \\wedge x^2 dx^1 \\wedge dx^3 \\\\ &amp;= (x^1 x^2 -1)dx^1 \\wedge dx^2 \\wedge dx^3 \\end{aligned} α∧β​=−dx1∧dx2∧dx3−x1dx2∧x2dx1∧dx3=(x1x2−1)dx1∧dx2∧dx3​ 在R3R^3R3 上定义三个光滑向量场如下： X=y∂∂x−x∂∂y,Y=z∂∂y−y∂∂z,Z=∂∂x+∂∂y+2∂∂zX=y \\frac{\\partial}{\\partial x} - x \\frac{\\partial}{\\partial y}, \\quad Y=z \\frac{\\partial}{\\partial y}-y \\frac{\\partial}{\\partial z},\\quad Z=\\frac{\\partial}{\\partial x} +\\frac{\\partial}{\\partial y} + 2\\frac{\\partial}{\\partial z} X=y∂x∂​−x∂y∂​,Y=z∂y∂​−y∂z∂​,Z=∂x∂​+∂y∂​+2∂z∂​ 求[X,Y][X,Y][X,Y]和[Y,Z][Y,Z][Y,Z] [X,Y]=X(Y−Y(X=(y∂∂x−x∂∂y)(z∂∂y−y∂∂z)−(z∂∂y−y∂∂z)(y∂∂x−x∂∂y)=yz∂2∂x∂y−y2∂2∂x∂z−xz∂2∂y2+x∂∂z+xy∂2∂y∂z−(z∂∂x+zy∂2∂y∂x−zx∂2∂y2−y2∂2∂z∂x+yx∂2∂z∂y)=x∂∂z−z∂∂x\\begin{aligned} \\left[X,Y\\right] &amp;= X(Y - Y(X \\\\ &amp;= (y \\frac{\\partial}{\\partial x} - x \\frac{\\partial}{\\partial y})(z \\frac{\\partial }{\\partial y}-y \\frac{\\partial }{\\partial z}) - (z \\frac{\\partial}{\\partial y}-y \\frac{\\partial}{\\partial z})(y \\frac{\\partial}{\\partial x} - x \\frac{\\partial}{\\partial y}) \\\\ &amp;= yz\\frac{\\partial ^2}{\\partial x \\partial y} - y^2 \\frac{\\partial ^2}{\\partial x \\partial z} - xz\\frac{\\partial ^2}{\\partial y^2} + x \\frac{\\partial }{\\partial z} + xy\\frac{\\partial ^2}{\\partial y \\partial z}\\\\ &amp;- (z \\frac{\\partial }{\\partial x} + zy\\frac{\\partial ^2}{\\partial y \\partial x} - zx \\frac{\\partial ^2}{\\partial y^2} - y^2\\frac{\\partial ^2}{\\partial z \\partial x} + yx\\frac{\\partial ^2}{\\partial z \\partial y}) \\\\ &amp;= x \\frac{\\partial }{\\partial z} - z \\frac{\\partial }{\\partial x} \\end{aligned} [X,Y]​=X(Y−Y(X=(y∂x∂​−x∂y∂​)(z∂y∂​−y∂z∂​)−(z∂y∂​−y∂z∂​)(y∂x∂​−x∂y∂​)=yz∂x∂y∂2​−y2∂x∂z∂2​−xz∂y2∂2​+x∂z∂​+xy∂y∂z∂2​−(z∂x∂​+zy∂y∂x∂2​−zx∂y2∂2​−y2∂z∂x∂2​+yx∂z∂y∂2​)=x∂z∂​−z∂x∂​​ [Y,Z]=(z∂∂y−y∂∂z)(∂∂x+∂∂y+2∂∂z)−(∂∂x+∂∂y+2∂∂z)(z∂∂y−y∂∂z)=z∂2∂y∂x+z∂2∂y2+z∂2∂y∂z−y∂2∂z∂x−y∂2∂z∂y−y∂2∂z2−(z∂∂x∂y−y∂2∂x∂z+z∂2∂y2−∂∂z−y∂2∂y∂z+∂∂y+z∂2∂z∂y−y∂2∂z2)=∂∂z−∂∂y\\begin{aligned} \\left[Y,Z\\right] &amp;= (z \\frac{\\partial}{\\partial y}-y \\frac{\\partial}{\\partial z})(\\frac{\\partial}{\\partial x} +\\frac{\\partial}{\\partial y} + 2\\frac{\\partial}{\\partial z}) -(\\frac{\\partial}{\\partial x} +\\frac{\\partial}{\\partial y} + 2\\frac{\\partial}{\\partial z})(z \\frac{\\partial}{\\partial y}-y \\frac{\\partial}{\\partial z}) \\\\ &amp;= z\\frac{\\partial ^2}{\\partial y \\partial x} + z \\frac{\\partial ^2}{\\partial y^2} + z \\frac{\\partial ^2}{\\partial y \\partial z} - y \\frac{\\partial ^ 2}{\\partial z \\partial x} - y\\frac{\\partial ^2}{\\partial z \\partial y} - y \\frac{\\partial ^ 2}{\\partial z^2}\\\\ &amp;- (z\\frac{\\partial}{\\partial x \\partial y} - y \\frac{\\partial ^2}{\\partial x \\partial z} + z \\frac{\\partial ^2}{\\partial y^2} - \\frac{\\partial}{\\partial z} - y \\frac{\\partial ^ 2}{\\partial y \\partial z} + \\frac{\\partial}{\\partial y} + z \\frac{\\partial ^2}{\\partial z \\partial y} - y \\frac{\\partial ^ 2}{\\partial z^2})\\\\ &amp;= \\frac{\\partial}{\\partial z} - \\frac{\\partial}{\\partial y} \\end{aligned} [Y,Z]​=(z∂y∂​−y∂z∂​)(∂x∂​+∂y∂​+2∂z∂​)−(∂x∂​+∂y∂​+2∂z∂​)(z∂y∂​−y∂z∂​)=z∂y∂x∂2​+z∂y2∂2​+z∂y∂z∂2​−y∂z∂x∂2​−y∂z∂y∂2​−y∂z2∂2​−(z∂x∂y∂​−y∂x∂z∂2​+z∂y2∂2​−∂z∂​−y∂y∂z∂2​+∂y∂​+z∂z∂y∂2​−y∂z2∂2​)=∂z∂​−∂y∂​​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"现代微分几何","slug":"现代微分几何","permalink":"/tags/现代微分几何/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"实验室快速上手教程","slug":"实验室快速上手","date":"2020-09-12T07:06:10.341Z","updated":"2020-09-13T10:01:07.383Z","comments":true,"path":"2020/09/12/实验室快速上手/","link":"","permalink":"/2020/09/12/实验室快速上手/","excerpt":"","text":"基础篇 Ubuntu系统的基本使用 终端基本命令 快捷键ctrl + alt + T打开终端 终端基本使用方法： $ cd path # 切换到目录 $ ls # 列出当前目录下的文件 $ pwd # 查看当前所在目录 $ cp file path # 文件复制到另一个目录 $ rm file # 删除文件 $ rm -rf path # 删除文件夹 $ touch file # 新建文件 $ mkdir dir_name # 新建文件夹 $ cat file # 查看文件内容(内容为文本的文件) $ gedit file # 利用gedit文本编辑器编辑文本 $ vim file # 利用vim编辑文本 $ nvidia-smi # 查看当前显卡状态 $ top # 查看当前内存、cpu等使用状态 $ mv filename newfilename # 文件或文件夹的重命名，也可以用来移动文件或文件夹 $ bash file.sh # 执行sh文件 $ tar -zxvf file # 解压文件 $ sudo dpkg -i file # 安装.deb文件(类似windows中的exe文件) $ chmod 777 file # 开放文件权限 $ chmod 777 -R file # 开放文件夹权限 补充： .. 代表上级文件夹 按Tab 键可自动补全 sudo代表使用管理员权限，当出现permission denied或权限不足提示时，首先尝试在命令前加sudo 需要安装的基本应用 Nvidia显卡驱动 CUDA CUDNN Pycharm Anaconda（或miniconda） 安装过程自行搜索，或者参考我的另一篇博客传送门 实验和代码 实验环境 首先安装好上面的基本应用 以pixel2pixel 为例： # 创建conda环境，环境名为pix2pix，python版本为3.6 $ conda create -n pix2pix python=3.6 # 激活conda环境 $ conda activate pix2pix # 安装所需的python第三方包，pytorch等 $ conda install pytorch $ conda install torchvision $ conda install visdom $ pip install dominate 然后按照实验的README下载数据集，运行代码等 一些常见问题： 兼容问题 例如pytorch不同版本和tensorflow不同版本之间的函数接口可能不同(pytorch还好，tensorflow这个问题比较严重)；python、cuda、cudnn、pytorch、tensorflow等版本互相不兼容等问题 解决：安装对应的版本，以pytorch为例： # 寻找pytorch所有版本(我在结果里删掉了一部分便于显示) $ conda search pytorch Loading channels: done # Name Version Build Channel pytorch 0.4.1 py37ha74772b_0 pkgs/main pytorch 1.0.1 cuda100py27he554f03_0 pkgs/main pytorch 1.0.1 cuda80py27ha8650f8 pkgs/main pytorch 1.0.1 cuda80py36ha8650f8_0 pkgs/main pytorch 1.1.0 cuda92py36h65efead_0 pkgs/main pytorch 1.1.0 cuda92py37h65efead_0 pkgs/main pytorch 1.2.0 cpu_py27h00be3c6_0 pkgs/main pytorch 1.2.0 cuda92py36hd3e106c_0 pkgs/main pytorch 1.2.0 cuda92py37hd3e106c_0 pkgs/main pytorch 1.3.1 cpu_py37h62f834f_0 pkgs/main # 安装对应的版本： $ conda install pytorch==1.3.1 或 $ conda install pytorch=1.3.1=cpu_py37h62f834f_0 注意到conda search pytorch 的build列中可能会出现cpu…/py…/.cuda…cudnn…等，其含义： cpu：pytorch是在cpu环境下编译的，安装这种类型的包一般无法使用GPU py: python版本，例如py37指实在python3.7环境下编译的(python2和python3之间可能会起冲突，一般python2.几内部和python3.几内部影响不大) cuda: cuda版本,安装后会自动下载cudatookit，可以代替电脑中的cuda使用。例如cuda92代表在cuda9.2环境下编译 cudnn： cudnn版本，安装后自动下载cudnn，可以代替电脑中的cudnn使用。 因此多数情况下的cudnn error、cuda问题等直接通过conda解决更好解决。 cuda问题 cuda爆显存：CUDA out of memory 一般情况下是由于batch-size较大导致的，尝试减小batch-size 如果是因为模型太大，参数量太多，改代码减少参数量或者换去服务器里多调用几块显卡 $ watch nvidia-smi 去观察显存使用情况，如果显存逐渐升高然后爆显存，大多是因为在执行测试或evaluation时没有阻止反向传播，修改测试部分的代码： with torch.no_grad(): model.test() cuda,cudnn版本问题：首先尝试用conda install 去安装不同版本的cudatookit或cudnn，如果失败建议使用docker搭建环境。 cuda和cudatookit的区别 大多数情况下使用cudatookit就足够了，必须使用电脑中的cuda实际是需要使用gcc,g++,nvcc等，例如PointNet++的tensorflow代码中需要使用nvcc编译一部分代码。 有些古老版本的代码会直接和显卡冲突 没找到解决方案，感觉无解…… 实验室还有几块1080Ti的显卡，借着用一下 看代码、修改代码 初学建议使用Pycharm中的debug逐行看代码，观察代码的运行过程。 可以参考这篇文章——&gt; 传送门 在pycharm中运行和debug代码需要提前配置好python环境： File————&gt; Settings————&gt; Project Interpreter————&gt; 小齿轮————&gt; Add————&gt;Conda environment————&gt; Existing environment————&gt; 更改Interpreter的位置 注: ununtu中默认的conda环境位置为：/home/username/anaconda3/envs/envname/bin/python 服务器的使用 详细使用教程见另一篇博客 传送门 进阶篇 科学上网 上外网下载数据集，可以找机场节点(一般是用ssr代理)，对应的客户端为： windows: shadosocksR ubuntu: electron-ssr mac: ShadowsocksX-NG-R 有些机场也有v2ray代理的节点，对应客户端为： Windows:·v2rayN Mac:v2rayU Linux:v2rayL 我自搭的v2ray节点分享出来(速度一般)： vmess://ewogICJ2IjogIjIiLAogICJwcyI6ICJ3dWxhYmluZ19jbG91ZC56YXNldmVuemFlaWdodC50b3AiLAogICJhZGQiOiAiY2xvdWQuemFzZXZlbnphZWlnaHQudG9wIiwKICAicG9ydCI6ICI1NTU1IiwKICAiaWQiOiAiMjVjNTc3NWItYTdkMi00ZmY5LWJlNjgtMjdjZWE2MjA4NGFiIiwKICAiYWlkIjogIjIiLAogICJuZXQiOiAid3MiLAogICJ0eXBlIjogIm5vbmUiLAogICJob3N0IjogImNsb3VkLnphc2V2ZW56YWVpZ2h0LnRvcCIsCiAgInBhdGgiOiAiLzkwMTBiNzllLyIsCiAgInRscyI6ICJ0bHMiCn0K 复制后，在v2ray客户端中导入 注： ssr的作者被请去喝茶了，现在ssr已经不再更新，并且ssr容易被监测到，推荐使用v2ray。 利用Git进行版本控制 基本命令参考另一篇博客 传送门 利用tensorboardX进行结果可视化： 参考： ref1. ​ ref2. 利用Docker搭建环境，防止冲突 当环境问题和系统的cuda版本、cudnn版本，以及一些内核文件冲突时，推荐使用Docker。 Dockerhub 参考 利用Neptune调参、模型可视化等 ref Neptune","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"neptune","slug":"Neptune","date":"2020-09-12T06:42:00.000Z","updated":"2020-09-12T06:51:30.153Z","comments":true,"path":"2020/09/12/Neptune/","link":"","permalink":"/2020/09/12/Neptune/","excerpt":"","text":"Getting started Neptune 比较和调试ML实验和模型 监测实验结果 保存模型、参数 监测GPU利用状态 Install Neptune client library $ pip install neptune-client # or $ conda install -c conda-forge neptune-client Copy and export your API token 在~/.bashrc 中添加 export NEPTUNE_API_TOKEN=\"your API token\" 执行 $ source ~\\.bashrc 测试 新建python文件，执行 import neptune # The init() function called this way assumes that # NEPTUNE_API_TOKEN environment variable is defined. neptune.init('ybb-ybb/sandbox') neptune.create_experiment(name='minimal_example') # log some metrics for i in range(100): neptune.log_metric('loss', 0.95**i) neptune.log_metric('AUC', 0.96) $ bash main.py 使用 create an experiment 定义参数运行新实验： # Define parameters PARAMS = {'decay_factor' : 0.5, 'n_iterations' : 117} # Create experiment with defined parameters neptune.create_experiment (name='example_with_parameters', params=PARAMS) 记录图像： # Log image data import numpy as np array = np.random.rand(10, 10, 3)*255 array = np.repeat(array, 30, 0) array = np.repeat(array, 30, 1) neptune.log_image('mosaics', array) 记录文本 # Log image data import numpy as np array = np.random.rand(10, 10, 3)*255 array = np.repeat(array, 30, 0) array = np.repeat(array, 30, 1) neptune.log_image('mosaics', array) 保存模型 # log some file # replace this file with your own file from local machine neptune.log_artifact('model_weights.pkl') # log file to some specific directory (see second parameter below) # replace this file with your own file from local machine neptune.log_artifact('model_checkpoints/checkpoint_3.pkl', 'training/model_checkpoints/checkpoint_3.pkl') 上传代码 # Upload source code # replace these two source files with your own files. neptune.create_experiment(upload_source_files=['main.py', 'model.py']) 标签 # add tag when experiment is created neptune.create_experiment(tags=['training']) # add single tag neptune.append_tag('transformer') # add few tags at once neptune.append_tags('BERT', 'ELMO', 'ideas-exploration')","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"},{"name":"neptune","slug":"neptune","permalink":"/tags/neptune/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"python序列方法","slug":"python序列方法","date":"2020-08-17T13:49:00.000Z","updated":"2020-08-17T13:51:04.318Z","comments":true,"path":"2020/08/17/python序列方法/","link":"","permalink":"/2020/08/17/python序列方法/","excerpt":"","text":"注：有些方法在 python2 中不存在 列表方法 通过列表推导式创建列表 &gt;&gt;&gt; x=[i**2 for i in range(3)] &gt;&gt;&gt; x [0, 1, 4] append : 将一个对象附加到列表末尾 clear ：就地清空列表内容 copy ： 复制列表 count ： 计算指定元素在列表中出现了多少次 extend ： 使用列表扩展另一个列表 index ： 查找指定值第一次出现的索引 insert ： 讲一个对象插入列表 pop ： 从列表删除一个元素(默认最后一个元素)，并返回这一元素 remove ： 删除第一个为指定值的元素 reverse ： 按相反的顺序排列列表中的元素 sort ： 对列表就地排序，接受两个可选参数key和values。 &gt;&gt;&gt; lst = [1, 2, 3] &gt;&gt;&gt; lst.append(4) &gt;&gt;&gt; lst [1, 2, 3, 4] &gt;&gt;&gt; lst.clear() &gt;&gt;&gt; lst [] &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b=a.copy() &gt;&gt;&gt; b [1, 2, 3] &gt;&gt;&gt; a.count(1) 1 &gt;&gt;&gt; a.extend(b) &gt;&gt;&gt; a [1, 2, 3, 1, 2, 3] &gt;&gt;&gt; a.index(2) 1 &gt;&gt;&gt; a.insert(1,4) &gt;&gt;&gt; a [1, 4, 2, 3, 1, 2, 3] &gt;&gt;&gt; a.pop() 3 &gt;&gt;&gt; a [1, 4, 2, 3, 1, 2] &gt;&gt;&gt; a.pop(0) 1 &gt;&gt;&gt; a [4, 2, 3, 1, 2] &gt;&gt;&gt; a.remove(1) &gt;&gt;&gt; a [4, 2, 3, 2] &gt;&gt;&gt; a.reverse() &gt;&gt;&gt; a [2, 3, 2, 4] &gt;&gt;&gt; a.sort() &gt;&gt;&gt; a [2, 2, 3, 4] &gt;&gt;&gt; a.sort(key=lambda x : x % 3,reverse=True) &gt;&gt;&gt; a [2, 2, 4, 3] 字符串方法 center ： 通过在字符串两边添加填充字符使字符串居中 find ：在字符串中查找子串，找到则返回子串第一个字符的索引，否则返回 -1(可指定起点值和终点值) join ： 合并序列的元素，与 split 相反 lower ： 返回字符串的小写版本 replace ： 将指定子串都替换为另一个字符串，并返回替换后的结果 split ： 与 join 相反，将字符串拆分为序列 strip ： 将字符串开头和末尾的指定字符删除 isspace ： 是否是空格 isdight ： 是否是数字 isupper : 是否是大写字母 islower ： 是否是小写字母 &gt;&gt;&gt; x=\"Hello World!\" &gt;&gt;&gt; x.center(15) ' Hello World! ' &gt;&gt;&gt; x.center(15,'*') '**Hello World!*' &gt;&gt;&gt; x.find('llo') 2 &gt;&gt;&gt; x.find('llo',3) -1 &gt;&gt;&gt; seq = ['1','2','3'] &gt;&gt;&gt; '+'.join(seq) '1+2+3' &gt;&gt;&gt; x.lower() 'hello world!' &gt;&gt;&gt; x.replace('hello','Hello') 'Hello World!' &gt;&gt;&gt; x.split() ['Hello', 'World!'] &gt;&gt;&gt; x = x.center(15,'*') &gt;&gt;&gt; x '**Hello World!*' &gt;&gt;&gt; x.strip('*') 'Hello World!' &gt;&gt;&gt; x '**Hello World!*' 字典 创建字典：通过键——值对序列或者字典推导式创建 &gt;&gt;&gt; items = [('name','Gumby'),('age',42)] &gt;&gt;&gt; d=dict(items) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; x={i:i ** 2 for i in range(3)} &gt;&gt;&gt; x {0: 0, 1: 1, 2: 4} **clear ** ：删除所有字典项 copy ： 返回一个新字典，包含的键值对与原字典相同 fromkeys ：创建一个新字典，包含指定的键，且每个键对应的值都是None，可指定对应的默认值 get ：访问不存在的建时返回None，可指定返回值 items ： 返回一个包含所有字典项的列表，其中每个元素都是（key,value）的形式 pop ： 获取与指定键相关联的值，并将该键值对从字典中删除 popitem ： 随机弹出一个字典项 setdefault ： 类似get ，当字典中不包含指定的键时，在字典中添加指定的键值对 update ： 使用一个字典中的项来更新另一个字典 keys ： 返回一个字典视图，其中包含字典中的键 values ： 返回一个字典视图，其中包含字典中的值 &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d.clear() &gt;&gt;&gt; d {} &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; x=d.copy() &gt;&gt;&gt; x {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; {}.fromkeys(['name','age']) {'name': None, 'age': None} &gt;&gt;&gt; {}.fromkeys(['name','age'],False) {'name': False, 'age': False} &gt;&gt;&gt; x.get('score') &gt;&gt;&gt; x.items() dict_items([('name', 'Gumby'), ('age', 42)]) IndentationError: expected an indented block &gt;&gt;&gt; for key,value in x.items(): ... print(value) Gumby 42 &gt;&gt;&gt; x {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; x.pop('name') 'Gumby' &gt;&gt;&gt; x.popitem() ('age', 42) &gt;&gt;&gt; x {} &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d.setdefault('name','N/A') 'Gumby' &gt;&gt;&gt; d.setdefault('score','N/A') 'N/A' &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42, 'score': 'N/A'} &gt;&gt;&gt; x={'score':88} &gt;&gt;&gt; d.update(x) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42, 'score': 88} &gt;&gt;&gt; x.keys() dict_keys(['name', 'age']) &gt;&gt;&gt; x.values() dict_values(['Gumby', 42])","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"十大排序算法","slug":"排序算法","date":"2020-06-27T01:44:00.000Z","updated":"2020-06-27T02:10:11.063Z","comments":true,"path":"2020/06/27/排序算法/","link":"","permalink":"/2020/06/27/排序算法/","excerpt":"","text":"算法 算法部分的介绍在十大经典排序算法动画与解析，看我就够了！ 关于时间复杂度： 平方阶 (O(n2))(O(n^2))(O(n2)) 排序 各类简单排序：插入排序、选择排序和冒泡排序。 线性对数阶$ (O(nlog2n))$ 排序：快速排序、堆排序和归并排序； O(n+§))O(n+§))O(n+§)) 排序，§§§ 是介于 0 和 1 之间的常数。 希尔排序 线性阶 (O(n)) 排序 基数排序，此外还有桶、箱排序 关于稳定性： 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 python实现 冒泡排序 def bubblesort(arr): n=len(arr) for i in range(n): for j in range(n-i-1): if arr[j] &gt; arr[j+1]: arr[j],arr[j+1]=arr[j+1],arr[j] return arr if __name__ == '__main__': arr = [5,4,2,3,8] bubblesort(arr) print(arr) 选择排序 def selectionsort(arr): n = len(arr) for i in range(n): min =i for j in range(i, n): if arr[j] &lt; arr[i]: min =j arr[min], arr[i] = arr[i], arr[min] return arr if __name__ == '__main__': arr=[5,4,2,3,8] selectionsort(arr) print(arr) 插入排序 def insertionSort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j &gt;= 0 and key &lt; arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key if __name__ == '__main__': arr = [5,4,2,3,8] insertionSort(arr) print(arr) 希尔排序 def shellsort(arr): n = len(arr) gap = n while gap &gt; 0: gap = gap // 2 for i in range(gap, n): key = arr[i] j = i - gap while j &gt;= 0 and key &lt; arr[j]: arr[j + gap] = arr[j] j -= gap arr[j + gap] = key return arr if __name__ == '__main__': arr = [5, 4, 2, 3, 8] shellsort(arr) print(arr) 归并排序 def sorttwoarray(arr_left, arr_right): m, n, i, j = len(arr_left), len(arr_right), 0, 0 arr = [] while i &lt; m and j &lt; n: if arr_left[i] &lt; arr_right[j]: arr.append(arr_left[i]) i += 1 else: arr.append(arr_right[j]) j += 1 arr.extend(arr_left[i:m]) arr.extend(arr_right[j:n]) return arr def mergesort(arr): n = len(arr) arr_left = arr[:(n + 1) // 2] arr_right = arr[(n + 1) // 2:] if len(arr_left) == 1: return sorttwoarray(arr_left, arr_right) else: return sorttwoarray(mergesort(arr_left), mergesort(arr_right)) if __name__ == '__main__': arr = [6, 4, 3, 7, 5, 1, 2] array = mergesort(arr) print(array) 快速排序 # -*- coding: UTF-8 -*- # 《算法导论》中的快速排序 def quick_sort(array, l, r): if l &lt; r: q = partition(array, l, r) quick_sort(array, l, q - 1) quick_sort(array, q + 1, r) def partition(array, l, r): x = array[r] i = l - 1 for j in range(l, r): if array[j] &lt;= x: i += 1 array[i], array[j] = array[j], array[i] array[i + 1], array[r] = array[r], array[i + 1] return i + 1 if __name__ == '__main__': arr = [6, 4, 3, 7, 5, 1, 2] quick_sort(arr,0,6) print(arr) 堆排序 # -*- coding: UTF-8 -*- def heapify(arr, n, i): largest = i l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 if l &lt; n and arr[i] &lt; arr[l]: largest = l if r &lt; n and arr[largest] &lt; arr[r]: largest = r if largest != i: arr[i], arr[largest] = arr[largest], arr[i] # 交换 heapify(arr, n, largest) def heapSort(arr): n = len(arr) # Build a maxheap. for i in range(n, -1, -1): heapify(arr, n, i) # 一个个交换元素 for i in range(n - 1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # 交换 heapify(arr, i, 0) if __name__ == '__main__': arr = [5, 2, 7, 3, 6, 1, 4] heapSort(arr) print (arr), 计数排序 def countingsort(arr): n = len(arr) min_arr = min(arr) max_arr = max(arr) length = max_arr - min_arr + 1 newarr = [0] * length sort_arr = [] for i in arr: newarr[i - min_arr] += 1 for i in range(length): sort_arr += [i + min_arr] * newarr[i] return sort_arr if __name__ == '__main__': arr = [5, 3, 4, 7, 2, 4, 3, 4, 7] sort_arr = countingsort(arr) print(sort_arr) 桶排序 def bucketsort(arr, num_bucket): min_arr = min(arr) max_arr = max(arr) buckets = [[] for i in range(num_bucket)] size = (max_arr - min_arr + 1) / num_bucket sorted_arr = [] for i in arr: index = (i - min_arr) // size buckets[index].append(i) for i in buckets: sorted_arr += (sorted(i)) return sorted_arr if __name__ == '__main__': arr = [7, 12, 56, 23, 19, 33, 35, 42, 42, 2, 8, 22, 39, 26, 17] sort_arr = bucketsort(arr, 5) print(sort_arr) 基数排序 def radixsort(arr): d = 1 max_arr = max(arr) while max_arr / 10 != 0: d += 1 max_arr /= 10 for i in range(d): s = [[] for k in range(10)] for j in arr: s[int(j / (10 ** i)) % 10].append(j) sorted_arr = [a for b in s for a in b] return sorted_arr if __name__ == '__main__': arr = [321, 1, 10, 30, 277, 753, 127] sorted_arr = radixsort(arr) print (sorted_arr)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"排序","slug":"排序","permalink":"/tags/排序/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"终端使用socks5代理","slug":"终端socks5","date":"2020-06-26T10:32:00.000Z","updated":"2020-07-13T05:52:42.000Z","comments":true,"path":"2020/06/26/终端socks5/","link":"","permalink":"/2020/06/26/终端socks5/","excerpt":"","text":"proxychains安装 # git仓库中编译安装 $ git clone https://github.com/rofl0r/proxychains-ng.git $ cd proxychains-ng $ ./configure $ make &amp;&amp; make install $ cp ./src/proxychains.conf /etc/proxychains.conf $ cd .. &amp;&amp; rm -rf proxychains-ng # 或直接安装 $ brew install proxychains-ng 编辑proxychains配置 $ vim /etc/proxychains.conf 将sock4 127.0.0.1 9095 改为socks5 12.0.0.1 1080 (配置同ssr或v2ray客户端一致) 使用方法 在需要代理的命令前加上 proxychains4 ，如 $ proxychains4 wget http://xxx.com/xxx.zip","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"科学上网","slug":"科学上网","permalink":"/tags/科学上网/"},{"name":"代理","slug":"代理","permalink":"/tags/代理/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"screen","slug":"screen","date":"2020-05-28T10:49:00.000Z","updated":"2020-06-26T06:24:41.865Z","comments":true,"path":"2020/05/28/screen/","link":"","permalink":"/2020/05/28/screen/","excerpt":"","text":"安装 $ sudo apt-get install screen 使用 # 创建一个名为screen-name的会话 $ screen -S screen-name # 离开会话: ctrl+a+d # 恢复创建的会话 $ screen -r screen-name # 或，如果只有一个会话 $ screen -r # 查看已经创建的会话 $ screen -ls # 退出会话 $ exit 其它命令 Ctrl + a，d #暂离当前会话 Ctrl + a，c #在当前screen会话中创建一个子会话 Ctrl + a，w #子会话列表 Ctrl + a，p #上一个子会话 Ctrl + a，n #下一个子会话 Ctrl + a，0-9 #在第0窗口至第9子会话间切换 鼠标回滚 screen模式下，无法在终端中使用鼠标滚轴进行翻页，解决： ctrl + a +[ 进入回滚模式 ctrl + c 切换回之前模式","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"screen","slug":"screen","permalink":"/tags/screen/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"Numpy","slug":"numpy基础用法总结","date":"2020-04-24T03:38:00.000Z","updated":"2020-09-12T07:58:26.474Z","comments":true,"path":"2020/04/24/numpy基础用法总结/","link":"","permalink":"/2020/04/24/numpy基础用法总结/","excerpt":"","text":"基础篇 创建数组： # 通过list创建 &gt;&gt;&gt;np.array([[1, 2], [3, 4]]) array([[1, 2], [3, 4]]) # 通过arange创建 &gt;&gt;&gt;np.arange(0,1,0.1) array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) # arange+广播 &gt;&gt;&gt;np.arange(1,60,10).reshape(-1,1)+np.arange(0,6) array([[ 1, 2, 3, 4, 5, 6], [11, 12, 13, 14, 15, 16], [21, 22, 23, 24, 25, 26], [31, 32, 33, 34, 35, 36], [41, 42, 43, 44, 45, 46], [51, 52, 53, 54, 55, 56]]) # linspace通过等差数列创建数组 &gt;&gt;&gt;np.linspace(0,1,10) array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ]) # 特殊形式数组 &gt;&gt;&gt;np.zeros((2,3),np.int) array([[0, 0, 0], [0, 0, 0]]) &gt;&gt;&gt;np.ones((2,3),np.int) array([[1, 1, 1], [1, 1, 1]]) # 长度为10，元素值为0-1的随机数数组 &gt;&gt;&gt;np.random.rand(2,3) array([[0.96064533, 0.55490284, 0.13219661], [0.3036712 , 0.95073354, 0.39364538]]) # 通过frombuffer,fromstring,fromfile和fromfunction等函数创建数组 &gt;&gt;&gt;np.fromfunction(lambda x,y:(x+1)*(y+1),(2,3)) array([[1., 2., 3.], [2., 4., 6.]]) # a和b之间的随机整数 &gt;&gt;&gt;np.random.randint(low=0,high=20,size=5) array([ 7, 19, 12, 18, 12]) 索引和切片 &gt;&gt;&gt;a=np.arange(5) &gt;&gt;&gt;a[2] 2 &gt;&gt;&gt;a[:2] array([0, 1]) &gt;&gt;&gt;a[:-1] array([0, 1, 2, 3]) # 加入步长 &gt;&gt;&gt;a[0:4:2] array([0, 2]) &gt;&gt;&gt;a[::-1] array([4, 3, 2, 1, 0]) # 布尔索引 &gt;&gt;&gt;mask=np.array([True,True,False,False,True]) &gt;&gt;&gt;a[mask] array([0, 1, 4]) &gt;&gt;&gt;a[a&gt;2] array([3, 4]) # 索引轴缺失 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;a[-1] array([3, 4, 5]) &gt;&gt;&gt;a[-1,:] array([3, 4, 5]) # 使用...补全索引轴 &gt;&gt;&gt;a=np.arange(24).reshape(2,3,4) &gt;&gt;&gt;a array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) &gt;&gt;&gt;a[0,...,1] array([1, 5, 9]) 转置及reshape reshape: # None可将一维行向量转置为列向量 &gt;&gt;&gt;a=np.random.rand(6) &gt;&gt;&gt;a array([0.25779506, 0.22348449, 0.19464385, 0.26307378, 0.76859958, 0.80357972]) &gt;&gt;&gt;a[:,None] array([[0.25779506], [0.22348449], [0.19464385], [0.26307378], [0.76859958], [0.80357972]]) # 等价于reshape &gt;&gt;&gt;a.reshape(-1,1) array([[0.25779506], [0.22348449], [0.19464385], [0.26307378], [0.76859958], [0.80357972]]) &gt;&gt;&gt;a.reshape(2,3) array([[0.25779506, 0.22348449, 0.19464385], [0.26307378, 0.76859958, 0.80357972]]) # flatten()返回一维数组 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;a.flatten() array([0, 1, 2, 3, 4, 5]) # 等价于ravel或reshape(-1) &gt;&gt;&gt;np.ravel(a) array([0, 1, 2, 3, 4, 5]) transpose: &gt;&gt;&gt;a=np.arange(24).reshape(2,3,4) &gt;&gt;&gt;a array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) &gt;&gt;&gt;a,shape (2, 3, 4) &gt;&gt;&gt;a.transpose(2,0,1) array([[[ 0, 4, 8], [12, 16, 20]], [[ 1, 5, 9], [13, 17, 21]], [[ 2, 6, 10], [14, 18, 22]], [[ 3, 7, 11], [15, 19, 23]]]) &gt;&gt;&gt;a.transpose(2,0,1).shape (4, 2, 3) Ufun numpy数学函数： 注：不提供axis时，按整个数组计算 函数 说明 np.sin(x) sin(x) np.cos(x) cos(x) np.tan(x tan(x) np.arcsin(x) arcsin(x) np.arccos(x) arccos(x) np.arctan(x) arctan(x) np.arctan2(x,y) arctan(x/y) np.deg2rad(x) 角度转弧度 np.rad2deg(x) 弧度转角度 np.prod(x,axis=None) 乘积 np.sum(x,axis=None) 求和 np.exp(x) exp(x) np.log(x) ln(x) np.sqrt(x) 开根 np.square(x) 平方 np.absolute(x) 绝对值 np.fabs(x) 绝对值 np.sign(x) 符号 np.maximum(x,y) 逐元素取最大值 np.minimum(x,y) 逐元素取最小值 np.mean(x,axis=None) 均值 np.std(x,axis=None) 标准差 np.var(x,axis=None) 方差 np.average(x,weight) (加权)平均 np.argmax(x,axis=None) 最大值索引 np.argmin(x,axis=None) 最小值索引 np.sort(x,axis=None) 从小到大排序 np.argsort(x,axis=None) 从小到大排序的索引 利用frompyfunc自定义Ufun： np.frompyfunc(func,n_in,n_out) &gt;&gt;&gt;def pow2(x): ... return x**2 &gt;&gt;&gt;fun_pow2=np.frompyfunc(pow2,1,1) &gt;&gt;&gt;a=np.arange(5) &gt;&gt;&gt;fun_pow2(a) array([0, 1, 4, 9, 16], dtype=object) 广播操作 广播是针对形状不同的数组的运算采取的操作。 当我们使用ufunc函数对两个数组进行计算时，ufunc函数会对这两个数组的对应元素进行计算，因此它要求这两个数组有相同的大小(shape相同)。如果两个数组的shape不同的话（行列规模不等），会进行如下的广播(broadcasting)处理： 1）. 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐。因此输出数组的shape是输入数组shape的各个轴上的最大值（往最大轴长上靠）。 2）. 如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错。 3）. 当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值。 &gt;&gt;&gt;np.arange(2)[:,None]+np.arange(3) array([[0, 1, 2], [1, 2, 3]]) 四则运算 +，-，*，/ ， 为逐元素四则运算** 矩阵乘法，注意要符合矩阵乘法规则: # 2*3矩阵 &gt;&gt;&gt;a=np.array([[1,2,3],[4,5,6]]) # 3*2矩阵 &gt;&gt;&gt;b=np.array([[3,4],[5,6],[7,8]]) &gt;&gt;&gt;a.dot(b) array([[34, 40], [79, 94]]) **内积：**对于两个一维数组，计算的是这两个数组对应下标元素的乘积和；对于多维数组a和b，它计算的结果数组中的每个元素都是数组a和b的最后一维的内积，因此数组a和b的最后一维的长度必须相同。 计算公式为：inner(a, b)[i,j,k,m] = sum(a[i,j,:]*b[k,m,:]) &gt;&gt;&gt;a=np.arange(12).reshape(2,3,2) &gt;&gt;&gt;b=np.arange(12,24).reshape(2,3,2) &gt;&gt;&gt;np.inner(a,b) array([[[[ 13, 15, 17], [ 19, 21, 23]], [[ 63, 73, 83], [ 93, 103, 113]], [[113, 131, 149], [167, 185, 203]]], [[[163, 189, 215], [241, 267, 293]], [[213, 247, 281], [315, 349, 383]], [[263, 305, 347], [389, 431, 473]]]]) **外积：**只按照一维数组进行计算，如果传入为多维数组，先展开再计算 &gt;&gt;&gt;np.outer([1,2,3],[4,5,6,7]) array([[ 4, 5, 6, 7], [ 8, 10, 12, 14], [12, 15, 18, 21]]) 其它一些 np.ndenumerate返回索引及数组值的迭代对象 &gt;&gt;&gt;for index, x in np.ndenumerate(c): ... print(index, x) ((0, 0), 1) ((0, 1), 2) ((1, 0), 3) ((1, 1), 4) &gt;&gt;&gt;np.ndenumerate(c) &lt;numpy.lib.index_tricks.ndenumerate at 0x7f21cc0dbb90&gt; &gt;&gt;&gt;np.ndenumerate(c).next() ((0, 0), 1) np.random.choice从一维数组或int对象随机选择元素 np.random.choice(a,size=None,replace=True,p=None) a:一维数据或int对象；replace=True：可重复选择；p：选取的概率 &gt;&gt;&gt;np.random.choice(5,3,p=[0.1,0,0.3,0.6,0]) Out[4]: array([3, 3, 2], dtype=int64) np.nonezeros返回非0元素的索引 &gt;&gt;&gt;np.nonzero([[0,1,2],[0,0,2]]) (array([0, 0, 1], dtype=int64), array([1, 2, 2], dtype=int64)) np.intersect1d求两个数组的交集： &gt;&gt;&gt; np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1]) array([1, 3]) # 利用reduce取多个交 &gt;&gt;&gt; from functools import reduce &gt;&gt;&gt; reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2])) array([3]) np.where找到矩阵中满足条件的元素的索引 &gt;&gt;&gt; x = np.arange(9.).reshape(3, 3) &gt;&gt;&gt; np.where( x &gt; 5 ) (array([2, 2, 2]), array([0, 1, 2])) &gt;&gt;&gt; x[np.where( x &gt; 3.0 )] # 返回大于3的值. array([ 4., 5., 6., 7., 8.]) &gt;&gt;&gt; np.where( x == 3.0 ) # 返回等于3的值的索引. (array([1], dtype=int64), array([0], dtype=int64)) **np.indices:**获取数组shape属性的所有索引，其shpe为(dim,shape) &gt;&gt;&gt;np.indices((2,3)).shape (2, 2, 3) &gt;&gt;&gt;np.indices((2,3))[0] array([[0, 0, 0], [1, 1, 1]]) &gt;&gt;&gt;np.indices((2,3))[1] array([[0, 1, 2], [0, 1, 2]]) # 用于索引 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;b=np.indices((2,3))[0] &gt;&gt;&gt;a[b] array([[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[3, 4, 5], [3, 4, 5], [3, 4, 5]]]) &gt;&gt;&gt;c=np.indices((2,3))[1] &gt;&gt;&gt;a[:,c] array([[[0, 1, 2], [0, 1, 2]], [[3, 4, 5], [3, 4, 5]]]) # 配合布尔索引进行mask操作 &gt;&gt;&gt;a[b&gt;0] array([3, 4, 5]) &gt;&gt;&gt;b&gt;0 array([[False, False, False], [ True, True, True]]) **np.repeat:**对数组进行扩展 np.repeat(a,repeats,axis=None) &gt;&gt;&gt;a=np.array([[10,20],[30,40]]) &gt;&gt;&gt;np.repeat(a,[3,2],axis=0) array([[10, 20], [10, 20], [10, 20], [30, 40], [30, 40]]) **np.tile:**对整个数组进行复制拼接 np.tile(a,reps) &gt;&gt;&gt; a=np.array([10,20]) &gt;&gt;&gt;np.tile(a, (3,2)) array([[10, 20, 10, 20], [10, 20, 10, 20], [10, 20, 10, 20]]) **np.pad:**数组填充(padding)操作 np.pad(array,pad_width,mode,**kwags) &gt;&gt;&gt;A = np.arange(95,99).reshape(2,2) &gt;&gt;&gt;np.pad(A,((3,2),(2,3)),'constant',constant_values = (0,0)) array([[ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 95, 96, 0, 0, 0], [ 0, 0, 97, 98, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0]]) **np.flip:**沿着指定轴翻转 &gt;&gt;&gt;A = np.arange(4).reshape((2,2)) &gt;&gt;&gt;np.flip(A,0) array([[2, 3], [0, 1]]) **np.unravel_index:**返回indices的下标 np.unravel_index(indices,dims,order=‘C’) &gt;&gt;&gt;np.unravel_index([22, 41, 37], (7,6)) (array([3, 6, 6]), array([4, 5, 1])) &gt;&gt;&gt;a=np.array([[1,2,3],[4,3,2]]) &gt;&gt;&gt;np.argmax(a) 3 &gt;&gt;&gt;np.unravel_index(np.argmax(a),a.shape) (1, 0) **np.unique:**去除数组中重复数字，并进行排序后输出 &gt;&gt;&gt;a=np.array([[1,2,3],[4,3,2]]) &gt;&gt;&gt;np.unique(a) array([1, 2, 3, 4])","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"numpy","slug":"numpy","permalink":"/tags/numpy/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"矩阵求导","slug":"矩阵求导","date":"2020-04-15T23:27:00.000Z","updated":"2020-04-17T03:29:07.000Z","comments":true,"path":"2020/04/16/矩阵求导/","link":"","permalink":"/2020/04/16/矩阵求导/","excerpt":"","text":"符号与符号布局的规定 符号规定： LLL ：标量 x​x​x​ ：n​n​n​ 维向量 yyy ：mmm 维向量 XXX ：m∗nm*nm∗n 维矩阵 分母布局： 标量LLL 对向量xxx 求导得到的是nnn 维向量，其中(∂L∂x)i=∂L∂xi(\\frac{\\partial L}{\\partial x})_i=\\frac{\\partial L}{\\partial x_i}(∂x∂L​)i​=∂xi​∂L​ 标量LLL 对矩阵XXX 求导是大小为m∗nm*nm∗n 的矩阵，其中(∂L∂X)ij=∂L∂xij(\\frac{\\partial L}{\\partial X})_{ij}=\\frac{\\partial L}{\\partial x_{ij}}(∂X∂L​)ij​=∂xij​∂L​ 向量y​y​y​ 对向量xxx 求导是大小为m∗nm*nm∗n 的矩阵，其中(∂y∂x)ij=∂yj∂xi​(\\frac{\\partial y}{\\partial x})_{ij}=\\frac{\\partial y_j}{\\partial x_i}​(∂x∂y​)ij​=∂xi​∂yj​​​ 分子布局： 标量LLL 对向量xxx 求导得到的是nnn 维向量，其中(∂L∂x)i=∂L∂xi(\\frac{\\partial L}{\\partial x})_i=\\frac{\\partial L}{\\partial x_i}(∂x∂L​)i​=∂xi​∂L​ 标量LLL 对矩阵XXX 求导是大小为n∗mn*mn∗m 的矩阵，其中(∂L∂X)ij=∂L∂xji(\\frac{\\partial L}{\\partial X})_{ij}=\\frac{\\partial L}{\\partial x_{ji}}(∂X∂L​)ij​=∂xji​∂L​ 向量yyy 对向量xxx 求导是大小为n∗mn*mn∗m 的矩阵，其中(∂y∂x)ij=∂yi∂xj(\\frac{\\partial y}{\\partial x})_{ij}=\\frac{\\partial y_i}{\\partial x_j}(∂x∂y​)ij​=∂xj​∂yi​​ 注：数学界有两派人使用着自己的符号约定，从而将矩阵微积分划分成了两个派别。这两个约定都是被大家所接受的。这两个布局之间相差一个转置，大多数的转置异常问题根本上来说是由于没有统一求导布局所导致的 。 几个重要的向量对向量求导结论 利用定义验证： ∂x∂x=I\\frac{\\partial x}{\\partial x}=I∂x∂x​=I 设向量z=f(x)z=f(x)z=f(x) ，则∂Az∂x=∂f∂xAT\\frac{\\partial Az}{\\partial x}=\\frac{\\partial f}{\\partial x}A^T∂x∂Az​=∂x∂f​AT ，特别地：∂Ax∂x=AT\\frac{\\partial Ax}{\\partial x}=A^T∂x∂Ax​=AT 设向量z=g(x)z=g(x)z=g(x) ，则∂zTA∂x=∂g∂xA\\frac{\\partial z^TA}{\\partial x}=\\frac{\\partial g}{\\partial x}A∂x∂zTA​=∂x∂g​A ，特别地：∂xTA∂x=A\\frac{\\partial x^TA}{\\partial x}=A∂x∂xTA​=A 设fff 为按元素运算的函数，则∂f(x)∂x=diag(f′(x))\\frac{\\partial f(x)}{\\partial x}=diag(f&#x27;(x))∂x∂f(x)​=diag(f′(x)) 标量求导的迹方法 多元函数微积分中，有df=∑i=1n∂f∂xidxi=∂f∂xTdxd f=\\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_{i}} d x_{i}=\\frac{\\partial f}{\\partial x}^{T} d xdf=∑i=1n​∂xi​∂f​dxi​=∂x∂f​Tdx ，在矩阵导数中： dL=∑i=1m∑j=1n∂L∂XijdXij=tr⁡((∂L∂X)TdX)d L=\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} d \\mathbf{X}_{ij}=\\operatorname{tr}\\left(\\left(\\frac{\\partial L}{\\partial \\mathbf{X}}\\right)^{T} d \\mathbf{X}\\right) dL=i=1∑m​j=1∑n​∂Xij​∂L​dXij​=tr((∂X∂L​)TdX) 迹的常用性质： tr(XT)=tr(X)tr(X^T)=tr(X)tr(XT)=tr(X) tr(X+Y)=tr(X)+tr(Y)tr(X+Y)=tr(X)+tr(Y)tr(X+Y)=tr(X)+tr(Y) tr(XY)=tr(YX)tr(XY)=tr(YX)tr(XY)=tr(YX) tr⁡(ATB)=∑i,jAijBij\\operatorname{tr}\\left(A^{T} B\\right)=\\sum_{i, j} A_{i j} B_{i j}tr(ATB)=∑i,j​Aij​Bij​ tr(AT(B⊙C))=tr((A⊙B)TC)=∑i,jAijBijCijtr\\left(A^{T}(B \\odot C)\\right)=tr\\left((A \\odot B)^{T} C\\right)=\\sum_{i, j} A_{i j} B_{i j} C_{i j}tr(AT(B⊙C))=tr((A⊙B)TC)=∑i,j​Aij​Bij​Cij​ 其中⊙\\odot⊙ 为Hadamard乘积 常用全微分公式及法则 常用公式 d(X+Y)=dX+dYd(X+Y)=dX+dYd(X+Y)=dX+dY d(tr(X))=tr(dX)d(tr(X))=tr(dX)d(tr(X))=tr(dX) d(XY)=(dX)Y+XdYd(XY)=(dX)Y+XdYd(XY)=(dX)Y+XdY d(X⊙Y)=(dX)⊙Y+X⊙dYd(X \\odot Y)=(dX) \\odot Y +X \\odot dYd(X⊙Y)=(dX)⊙Y+X⊙dY dσ(X)=σ′(X)⊙dXd \\sigma(X)=\\sigma&#x27;(X)\\odot dXdσ(X)=σ′(X)⊙dX （$\\sigma $ 为逐元素运算） dX−1=−X−1(dX)X−1dX^{-1}=-X^{-1}(dX)X^{-1}dX−1=−X−1(dX)X−1 d∣X∣=∣X∣tr(X−1dX)d|X|=|X|tr(X^{-1}dX)d∣X∣=∣X∣tr(X−1dX) dln∣X∣=tr(X−1dX)dln|X|=tr(X^{-1}dX)dln∣X∣=tr(X−1dX) dXT=(dX)TdX^T=(dX)^TdXT=(dX)T 乘法法则 若x∈Rp,y=f(x)∈Rq,z=g(x)∈Rq\\mathbf{x} \\in R^{p}, \\mathbf{y}=f(\\mathbf{x}) \\in R^{q}, \\mathbf{z}=g(\\mathbf{x}) \\in R^{q}x∈Rp,y=f(x)∈Rq,z=g(x)∈Rq,则$ \\frac{\\partial \\mathbf{y}^{T} \\mathbf{z}}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\mathbf{z}+\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\mathbf{y} \\in R^{p}$ 若 x∈Rp,y=f(x)∈R,z=g(x)∈Rq\\mathbf{x} \\in R^{p}, y=f(\\mathbf{x}) \\in R, \\mathbf{z}=g(\\mathbf{x}) \\in R^{q}x∈Rp,y=f(x)∈R,z=g(x)∈Rq,则 ∂yz∂x=∂y∂xzT+∂z∂xy∈Rp×q\\frac{\\partial y \\mathbf{z}}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial \\mathbf{x}} \\mathbf{z}^{T}+\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} y \\in R^{p \\times q}∂x∂yz​=∂x∂y​zT+∂x∂z​y∈Rp×q 链式法则 向量对向量求导(包含向量对标量求导、标量对向量求导)： 设多个向量之间存在依赖关系：a⇒b⇒…⇒x⇒y⇒z\\mathbf{a} \\Rightarrow \\mathbf{b} \\Rightarrow \\ldots \\Rightarrow \\mathbf{x} \\Rightarrow \\mathbf{y} \\Rightarrow \\mathbf{z}a⇒b⇒…⇒x⇒y⇒z 计算方法：∂z∂a=∂b∂a∂c∂b…∂y∂x∂z∂y\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{a}}=\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}} \\ldots \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}∂a∂z​=∂a∂b​∂b∂c​…∂x∂y​∂y∂z​ 标量对矩阵求导： 若矩阵X∈Rp∗qX\\in R^{p*q}X∈Rp∗q ，矩阵y=g(X)∈Rs∗ty=g(X)\\in R^{s*t}y=g(X)∈Rs∗t ，标量z=f(Y)∈Rz=f(Y) \\in Rz=f(Y)∈R，则∂z∂Xij=tr⁡((∂z∂Y)T∂Y∂Xij)\\frac{\\partial z}{\\partial \\mathbf{X}_{ij}}=\\operatorname{tr}\\left(\\left(\\frac{\\partial z}{\\partial \\mathbf{Y}}\\right)^{T} \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{X}_{ij}}\\right)∂Xij​∂z​=tr((∂Y∂z​)T∂Xij​∂Y​) 由上式，若链式关系中存在有矩阵对矩阵求导，则难以直接写出标量对矩阵的导数，下面则给出常用的线性关系下的&quot;链式&quot;求导 ： 若存在关系：X⇒Y=AX+B⇒L=f(Y)\\mathbf{X} \\Rightarrow \\mathbf{Y}=\\mathbf{A} \\mathbf{X}+\\mathbf{B} \\Rightarrow L=f(\\mathbf{Y})X⇒Y=AX+B⇒L=f(Y) ，则∂L∂X=AT∂L∂Y\\frac{\\partial L}{\\partial \\mathbf{X}}=A^{T} \\frac{\\partial L}{\\partial \\mathbf{Y}}∂X∂L​=AT∂Y∂L​ 若存在关系： X⇒Y=XA+B⇒L=f(Y)\\mathbf{X} \\Rightarrow \\mathbf{Y}=\\mathbf{X} \\mathbf{A}+\\mathbf{B} \\Rightarrow L=f(\\mathbf{Y})X⇒Y=XA+B⇒L=f(Y)，则∂L∂X=∂L∂YAT\\frac{\\partial L}{\\partial \\mathbf{X}}= \\frac{\\partial L}{\\partial \\mathbf{Y}}A^T∂X∂L​=∂Y∂L​AT 链式法则不再成立，往往通过迹方法来求标量对矩阵的导数 常见技巧 一维下标求和考虑写成向量内积的形式 二位下标求和考虑写成trtrtr的形式 向量模长的平方考虑内积运算∑ixi2=xTx\\sum_{i} x_{i}^{2}=\\mathbf{x}^{T} \\mathbf{x}∑i​xi2​=xTx 矩阵的Frobenius范数考虑写成trtrtr 的形式∥X∥F2=tr(XXT)\\|\\mathbf{X}\\|_{F}^{2}=t r\\left(\\mathbf{X} \\mathbf{X}^{T}\\right)∥X∥F2​=tr(XXT) 推荐阅读： 知乎，矩阵求导术(上) 知乎，矩阵求导术(下)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"矩阵求导","slug":"矩阵求导","permalink":"/tags/矩阵求导/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"内网穿透proxyer","slug":"proxyer做内网穿透","date":"2020-04-10T01:15:00.000Z","updated":"2020-04-10T01:48:36.000Z","comments":true,"path":"2020/04/10/proxyer做内网穿透/","link":"","permalink":"/2020/04/10/proxyer做内网穿透/","excerpt":"","text":"Github地址：https://github.com/khvysofq/proxyer 服务端 安装docker： $ curl -sSL https://get.docker.com/ | sh $ systemctl start docker $ systemctl enable docker 安装docker compose $ curl -L \"https://get.daocloud.io/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose $ chmod +x /usr/local/bin/docker-compose 安装proxyer： $ wget https://raw.githubusercontent.com/khvysofq/proxyer/master/docker-compose.yml # 后面1.1.1.1改成服务器ip地址 $ export PROXYER_PUBLIC_HOST=1.1.1.1 $ docker-compose up -d 安装完成后，通过ip:6789访问服务端WEB管理面板了，进去后需要设置一个客户端认证密码。 客户端 从web管理面板下载对应系统客户端 windows系统直接运行，linux解压后运行./proxyer，按照提示浏览器进入127.0.0.1:9876 如图，内网地址填127.0.0.1:22，序列号自定义 安装ssh，已安装可跳过： $ sudo apt-get install ssh 以图片为例，远程ssh连接时，连接使用： $ ssh -p 43537 yu@66.152.179.100 设置为开机自启 (支持Ubuntu16.04，18.04好像不支持这种方式了) 编辑/etc/rc.local文件，在exit 0前添加proxyer文件的位置 如：~/proxyer","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"内网穿透","slug":"内网穿透","permalink":"/tags/内网穿透/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"Git！","slug":"Git","date":"2020-03-30T10:49:00.000Z","updated":"2020-03-30T13:08:35.000Z","comments":true,"path":"2020/03/30/Git/","link":"","permalink":"/2020/03/30/Git/","excerpt":"","text":"工作原理/流程 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 配置 在Github注册后，做一下本地的Git仓库配置： $ git config --global user.name \"ybb-ybb\" $ git config --global user.email \"21901037@mail.dlut.edu.cn\" 如果对某个仓库使用不同的用户名和邮箱，去掉--global参数即可 本地仓库 提交 # 将当前目录变成git可管理的仓库 $ git init # 将readme.md添加到缓存区 $ git add readme.md # 将所有文件添加到缓存区 $ git add . # 将缓存区文件提交到仓库 $ git commit -m \"提交信息\" # 查看是否还有文件未提交 $ git status # 查看文件修改内容 $ git diff readme.md # 查看历史记录 $ git log # 查看历史记录的简单信息 $ git log –pretty=oneline 版本回退 # 回到上一个版本 $ git reset --hard HEAD^ # 同理，回到上上个版本等，以此类推 $ git reset --hard HEAD^^ # 或 $ git reset --hard HEAD~2 # 查看版本号变化 $ git reflog # 版本号回退 $ git reset --hard {版本号} # 丢弃工作区的修改（回到暂存区的状态） $ git checkout -- filename 远程仓库 配置 本地Git仓库和Github仓库之间是通过ssh加密的，因此需要先进行一些配置： 第一步：创建SSH Key。在主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果有的话，直接跳过此如下命令，如果没有的话： ssh-keygen -t rsa –C “youremail@example.com” id_rsa是私钥，id_rsa.pub是公钥 第二步：登录github,打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”,填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容 ，Add key 推送到远程仓库 # 先创建一个github仓库，复制http或ssh地址,然后关联 $ git remote add origin {http adress} # 推送，第一次推送时添加-u参数,把master分支推送到远程 $ git push -u origin master 分支 # 创建分支 $ git branch backup # 切换分支 $ git checkout backup # 或者：创建+切换分支 $ git checkout -b backup # 查看当前分支 $ git branch # 在master分支上，将分支backup合并到master上 $ git merge backup # 删除分支 $ git branch -d backup","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"Git","slug":"Git","permalink":"/tags/Git/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"离线下载+在线播放网盘","slug":"离线下载+在线播放网盘","date":"2020-03-15T04:13:00.000Z","updated":"2020-03-15T04:39:06.210Z","comments":true,"path":"2020/03/15/离线下载+在线播放网盘/","link":"","permalink":"/2020/03/15/离线下载+在线播放网盘/","excerpt":"","text":"主要过程是用Aria2进行下载，然后上传到OneDrive云盘并用OneIndex关联云盘实现网页访问。 获取OneDrive 申请OneDrive5T账号 两个申请OneDrive云盘5T的方法： 1、申请微软的Office 365开发者计划，地址：免费获得一年的21TB OneDrive和Microsoft Office 365企业 2、使用热心大佬提供的临时邮箱申请一个，方法如下： 1)、进入注册地址https://products.office.com/en-us/student?tab=students 2)、输入如有乐享提供的临时邮箱，地址：https://51.ruyo.net/8263.html 3)、填入密码，和从临时邮箱获取的验证码 授权 授权认证： 点击右侧URL登录并授权，授权地址→ 国际版 世纪互联 授权后会获取一个localhost开头打不开的链接，这里只需要记住code，也就是链接中code=和&amp;中间的参数。 安装Aria2 以下操作都是在服务器端执行 $ cd /root $ wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubiBackup/doubi/master/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh #备用地址 $ wget -N --no-check-certificate https://www.moerats.com/usr/shell/Aria2/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh 安装后可以使用bash aria2.sh命令修改Aria2的默认下载目录、端口号、密码等 安装OneDriveUploader $ wget https://raw.githubusercontent.com/MoeClub/OneList/master/OneDriveUploader/amd64/linux/OneDriveUploader -P /usr/local/bin/ # 授权 $ chmod +x /usr/local/bin/OneDriveUploader # 初始化配置 #将moerats替换成授权步骤中获取的code参数 $ code=\"moerats\" $ OneDriveUploader -a \"${code}\" 如果提示 Init config file: /root/auth.json 类似信息，则初始化成功。 手动上传示例： # -c指定初始化文件位置，-s指定上传文件，-r指定网盘目录，不指定默认为根目录 # 将当前目录下的Download文件夹上传到OneDrive网盘Test目录中 $ OneDriveUploader -c /root/auth.json -s \"Download\" -r \"Test\" 配置Aria2的自动上传 # 新建文件 $ touch rcloneupload.sh # 修改文件内容 $ vim rcloneupload.sh 将文件内容修改为下面内容，注意替换Aria2的下载目录 #!/bin/bash GID=\"$1\"; FileNum=\"$2\"; File=\"$3\"; MaxSize=\"15728640\"; Thread=\"3\"; #默认3线程，自行修改，服务器配置不好的话，不建议太多 Block=\"20\"; #默认分块20m，自行修改 RemoteDIR=\"\"; #上传到Onedrive的路径，默认为根目录，如果要上传到MOERATS目录，\"\"里面请填成MOERATS LocalDIR=\"/www/download/\"; #Aria2下载目录，记得最后面加上/ Uploader=\"/usr/local/bin/OneDriveUploader\"; #上传的程序完整路径，默认为本文安装的目录 Config=\"/root/auth.json\"; #初始化生成的配置auth.json绝对路径，参考第3步骤生成的路径 if [[ -z $(echo \"$FileNum\" |grep -o '[0-9]*' |head -n1) ]]; then FileNum='0'; fi if [[ \"$FileNum\" -le '0' ]]; then exit 0; fi if [[ \"$#\" != '3' ]]; then exit 0; fi function LoadFile(){ if [[ ! -e \"${Uploader}\" ]]; then return; fi IFS_BAK=$IFS IFS=$'\\n' tmpFile=\"$(echo \"${File/#$LocalDIR}\" |cut -f1 -d'/')\" FileLoad=\"${LocalDIR}${tmpFile}\" if [[ ! -e \"${FileLoad}\" ]]; then return; fi ItemSize=$(du -s \"${FileLoad}\" |cut -f1 |grep -o '[0-9]*' |head -n1) if [[ -z \"$ItemSize\" ]]; then return; fi if [[ \"$ItemSize\" -ge \"$MaxSize\" ]]; then echo -ne \"\\033[33m${FileLoad} \\033[0mtoo large to spik.\\n\"; return; fi ${Uploader} -c \"${Config}\" -t \"${Thread}\" -b \"${Block}\" -s \"${FileLoad}\" -r \"${RemoteDIR}\" if [[ $? == '0' ]]; then rm -rf \"${FileLoad}\"; fi IFS=$IFS_BAK } LoadFile; 授权 $ chmod +x rcloneupload.sh bash aria2.sh修改配置文件，加上一行： on-download-complete=/root/rcloneupload.sh 重启Aria2生效 测试一下 试一下 bash /root/rcloneupload.sh 正常为无反应，如果报错： 1、安装dos2unix $ apt-get install dos2unix -y 2、转换格式 $ dos2unix /root/rcloneupload.sh Aria2离线下载的使用 谷歌浏览器插件 aria2 for chrome 进去之后AriaNg设置——&gt;添加新RPC设置，配置如下： 然后添加任务即可进行离线下载，并自动上传到OneDrive OneIndex对接网盘 OneIndex可以对接Onedrive网盘，将网盘里的内容直接显示成目录，视频可以在线播放。也可以搭建自己的在线图床/视频播放系统。 使用docker安装Oneindex $ docker run -d -p 8181:80 --restart=always baiyuetribe/oneindex 然后访问ip:8181按照提示操作即可 效果图： 如果要进入系统管理页面，访问ip:8181/?/login 可选，域名访问 如果有自己的域名，可以通过反向代理进行http访问 宝塔反代：先进入宝塔面板，点击左侧网站，添加站点，完成后进入网站设置，点击反向代理，目标URL填入http://127.0.0.1:8181，再启用反向代理即可。 宝塔界面可一键安装： $ wget -O install.sh http://download.bt.cn/install/install-ubuntu.sh &amp;&amp; sudo bash install.sh","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"OneDrive","slug":"OneDrive","permalink":"/tags/OneDrive/"},{"name":"离线下载","slug":"离线下载","permalink":"/tags/离线下载/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"tensorboardX","slug":"tensorboardX","date":"2020-03-09T13:31:11.839Z","updated":"2020-03-15T04:35:24.214Z","comments":true,"path":"2020/03/09/tensorboardX/","link":"","permalink":"/2020/03/09/tensorboardX/","excerpt":"","text":"创建 TensorBoardX的GitHub地址：传送门 首先创建一个 SummaryWriter 的示例 ： from tensorboardX import SummaryWriter # Creates writer1 object. # The log will be saved in 'runs/exp' writer1 = SummaryWriter('runs/exp') # Creates writer2 object with auto generated file name # The log directory will be something like 'runs/Aug20-17-20-33' writer2 = SummaryWriter() # Creates writer3 object with auto generated file name, the comment will be appended to the filename. # The log directory will be something like 'runs/Aug20-17-20-33-resnet' writer3 = SummaryWriter(comment='resnet') 以上展示了三种初始化 SummaryWriter 的方法： 提供一个路径，将使用该路径来保存日志 无参数，默认将使用 runs/日期时间 路径来保存日志 提供一个 comment 参数，将使用 runs/日期时间-comment 路径来保存日志 在浏览器中查看这些可视化数据： tensorboard --logdir=&lt;your_log_dir&gt; 用各种add方法记录数据 数字(scalar) add_scalar(tag, scalar_value, global_step=None, walltime=None) 参数: tag (string): 数据名称，不同名称的数据使用不同曲线展示 scalar_value (float): 数字常量值 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() 需要注意，这里的 scalar_value 一定是 float 类型，如果是 PyTorch scalar tensor，则需要调用 .item() 方法获取其数值。我们一般会使用 add_scalar 方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，直观地监控训练过程。 示例： from tensorboardX import SummaryWriter writer = SummaryWriter('runs/scalar_example') for i in range(10): writer.add_scalar('quadratic', i**2, global_step=i) writer.add_scalar('exponential', 2**i, global_step=i) 图片(image) 需要pillow库的支持 用add_image记录单个图像数据，用add_images记录多个图像数据 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') CHW为channel*hight*width 示例： from tensorboardX import SummaryWriter import cv2 as cv writer = SummaryWriter('runs/image_example') for i in range(1, 6): writer.add_image('countdown', cv.cvtColor(cv.imread('{}.jpg'.format(i)), cv.COLOR_BGR2RGB), global_step=i, dataformats='HWC') 直方图(histogram) add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 示例： from tensorboardX import SummaryWriter import numpy as np writer = SummaryWriter('runs/embedding_example') writer.add_histogram('normal_centered', np.random.normal(0, 1, 1000), global_step=1) writer.add_histogram('normal_centered', np.random.normal(0, 2, 1000), global_step=50) writer.add_histogram('normal_centered', np.random.normal(0, 3, 1000), global_step=100) &quot;DISTRIBUTIONS&quot;和&quot;HISTOGRAMS&quot;两栏都是用来观察数据分布的。其中在&quot;HISTOGRAMS&quot;中，同一数据不同 step 时候的直方图可以上下错位排布 (OFFSET) 也可重叠排布 (OVERLAY)。 运行图(graph) add_graph(model, input_to_model=None, verbose=False, **kwargs) 可以可视化神经网络的结构，参考Github官方样例 嵌入张量(embedding) 使用 add_embedding 方法可以在二维或三维空间可视化 embedding 向量。 add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) 参数： mat (torch.Tensor or numpy.array): 一个MxN矩阵，每行代表特征空间的一个数据点 metadata (list or torch.Tensor or numpy.array, optional): 一个一维列表N，mat 中每行数据的 label，大小应和 mat 行数相同 label_img (torch.Tensor, optional): 一个形如 NxCxHxW 的张量，对应 mat 每一行数据显示出的图像，N 应和 mat 行数相同 global_step (int, optional): 训练的 step tag (string, optional): 数据名称，不同名称的数据将分别展示 示例： from tensorboardX import SummaryWriter import torchvision writer = SummaryWriter('runs/embedding_example') mnist = torchvision.datasets.MNIST('mnist', download=True) writer.add_embedding( mnist.train_data.reshape((-1, 28 * 28))[:100,:], #直接将mnist前100个数据展开成一维向量作为embedding metadata=mnist.train_labels[:100], #每个embedding的label label_img = mnist.train_data[:100,:,:].reshape((-1, 1, 28, 28)).float() / 255, #每个图像 global_step=0 )","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2020-03-09T12:16:01.000Z","updated":"2020-03-15T04:21:33.074Z","comments":true,"path":"2020/03/09/Hexo-Theme-Sakura/","link":"","permalink":"/2020/03/09/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakura主题 English document 基于WordPress主题Sakura修改成Hexo的主题。 demo预览 正在开发中… 交流群 若你是使用者，加群QQ: 801511924 若你是创作者，加群QQ: 194472590 主题特性 首页大屏视频 首页随机封面 图片懒加载 valine评论 fancy-box相册 pjax支持，音乐不间断 aplayer音乐播放器 多级导航菜单（按现在大部分hexo主题来说，这也算是个特性了） 赞赏作者 如果喜欢hexo-theme-sakura主题，可以考虑资助一下哦~非常感激！ paypal | Alipay 支付宝 | WeChat Pay 微信支付 未完善的使用教程 那啥？老实说我目前也不是很有条理233333333~ 1、主题下载安装 hexo-theme-sakura建议下载压缩包格式，因为除了主题内容还有些source的配置对新手来说比较太麻烦，直接下载解压就省去这些麻烦咯。 下载好后解压到博客根目录（不是主题目录哦，重复的选择替换）。接着在命令行（cmd、bash）运行npm i安装依赖。 2、主题配置 博客根目录下的_config配置 站点 # Site title: 你的站点名 subtitle: description: 站点简介 keywords: author: 作者名 language: zh-cn timezone: 部署 deploy: type: git repo: github: 你的github仓库地址 # coding: 你的coding仓库地址 branch: master 备份 （使用hexo b发布备份到远程仓库） backup: type: git message: backup my blog of https://honjun.github.io/ repository: # 你的github仓库地址,备份分支名 （建议新建backup分支） github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup 主题目录下的_config配置 其中标明【改】的是需要修改部门，标明【选】是可改可不改，标明【非】是不用改的部分 # site name # 站点名 【改】 prefixName: さくら荘その siteName: hojun # favicon and site master avatar # 站点的favicon和头像 输入图片路径（下面的配置是都是cdn的相对路径，没有cdn请填写完整路径，建议使用jsdeliver搭建一个cdn啦，先去下载我的cdn替换下图片就行了，简单方便~）【改】 favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # 站点url 【改】 url: https://sakura.hojun.cn # 站点介绍（或者说是个人签名）【改】 description: Live your life with passion! With some drive! # 站点cdn，没有就为空 【改】 若是cdn为空，一些图片地址就要填完整地址了，比如之前avatar就要填https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # 开启pjax 【选】 pjax: 1 # 站点首页的公告信息 【改】 notice: hexo-Sakura主题已经开源，目前正在开发中... # 懒加载的加载中图片 【选】 lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # 站点菜单配置 【选】 menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer first，1: older first. 【非】 homePageSortType: -1 # Home page article shown number) 【非】 homeArticleShown: 10 # 背景图片 【选】 bgn: 8 # startdash面板 url, title, desc img 【改】 startdash: - {url: /theme-sakura/, title: Sakura, desc: 本站 hexo 主题, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: 博主的b站视频, img: /img/startdash/bilibili.jpg} - {url: /, title: hojun的万事屋, desc: 技术服务, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # 你的站点建立日期 【改】 siteBuildingTime: 07/17/2018 # 社交按钮(social) url, img PC端配置 【改】 social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # 社交按钮(msocial) url, img 移动端配置 【改】 msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # 赞赏二维码（其中wechatSQ是赞赏单页面的赞赏码图片）【改】 donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # 首页视频地址为https://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4，配置如下 【改】 movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # 多个视频用逗号隔开，随机获取。支持的格式目前已知MP4,Flv。其他的可以试下，不保证有用 name: Unbroken.mp4 # 左下角aplayer播放器配置 主要改id和server这两项，修改详见[aplayer文档] 【改】 aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valine评论配置【改】 valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ 分类页和标签页配置 分类页 标签页 配置项在\\themes\\Sakura\\languages\\zh-cn.yml里。新增一个分类或标签最好加下哦，当然嫌麻烦可以直接使用一张默认图片（可以改主题或者直接把404图片替换下，征求下意见要不要给这个在配置文件中加个开关，可以issue或群里提出来），现在是没设置的话会使用那种倒立小狗404哦。 #category # 按分类名创建 技术: #中文标题 zh: 野生技术协会 # 英文标题 en: Geek – Only for Love # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg 生活: zh: 生活 en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # 标签名即是标题 悦读: # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg 单页面封面配置 如留言板页面页面，位于source下的comment下，打开index.md如下： --- title: comment date: 2018-12-20 23:13:48 keywords: 留言板 description: comments: true # 在这里配置单页面头部图片，自定义替换哦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- 单页面配置 番组计划页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # 番组图片 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # 番组名 title: 朝花夕誓——于离别之朝束起约定之花 # 追番状态 （追番ing/已追完） status: 已追完 # 追番进度 progress: 100 # 番剧日文名称 jp: さよならの朝に約束の花をかざろう # 放送时间 time: 放送时间: 2018-02-24 SUN. # 番剧介绍 desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: 朝花夕誓——于离别之朝束起约定之花 status: 已追完 progress: 50 jp: さよならの朝に約束の花をかざろう time: 放送时间: 2018-02-24 SUN. desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 --- 友链页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: links title: links # 创建日期，可以改下 date: 2018-12-19 23:11:06 # 图片上的标题，自定义修改 keywords: 友人帐 description: # true/false 开启/关闭评论 comments: true # 页面头部图片，自定义修改 photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # 友链配置 links: # 类型分组 - group: 个人项目 # 类型简介 desc: 充分说明这家伙是条咸鱼 &lt; (￣︶￣)&gt; items: # 友链链接 - url: https://shino.cc/fgvf # 友链头像 img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # 友链站点名 name: Google # 友链介绍 下面雷同 desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 # 类型分组... - group: 小伙伴们 desc: 欢迎交换友链 ꉂ(ˊᗜˋ) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 --- 写文章配置 主题集成了个人插件hexo-tag-bili和hexo-tag-fancybox_img。其中hexo-tag-bili用来在文章或单页面中插入B站外链视频，使用语法如下： 详细使用教程详见hexo-tag-bili。 hexo-tag-fancybox_img用来在文章或单页面中图片，使用语法如下： 详细使用教程详见hexo-tag-fancybox_img 还有啥，一时想不起来… To be continued…","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"web","slug":"web","permalink":"/tags/web/"},{"name":"悦读","slug":"悦读","permalink":"/tags/悦读/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]},{"title":"neural ode","slug":"Neural ODE","date":"2020-03-04T03:38:00.000Z","updated":"2020-07-02T08:52:50.662Z","comments":true,"path":"2020/03/04/Neural ODE/","link":"","permalink":"/2020/03/04/Neural ODE/","excerpt":"","text":"原文：Neural ordinary differential equations NIPS2018最佳论文 简介 简单复习一下ResNet： 通过残差块解决反向传播过程中的梯度消失问题 ResNet、RNN、Normalizing flow 等模型都是这种形式： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) 如果采用更多的层数和更小的步长，可以优化为一个常微分方程： dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta) dtdh(t)​=f(h(t),t,θ) 这就是ODE Net的核心idea了……下面进行具体的分析 给定常微分方程，数学理论上可以对其进行解析法求解，但通常我们只关心数值解：在已知h(t0)h(t_0)h(t0​) 的情况下，求出h(t1)h(t_1)h(t1​) 。这在神经网络里对应的是正向传播。用ResNet对比一下： ResNet的正向传播： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) ODE网络的正向传播： dh(t)dt=f(h(t),t,θ)∫t0t1dh(t)=∫t0t1f(h(t),t,θ)dth(t1)=h(t0)+∫t0t1f(h(t),t,θ)dt\\begin{array}{c} \\frac{d h(t)}{d t}=f(h(t), t, \\theta) \\\\ \\int_{t_{0}}^{t_{1}} d h(t)=\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\\\ h\\left(t_{1}\\right)=h\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\end{array} dtdh(t)​=f(h(t),t,θ)∫t0​t1​​dh(t)=∫t0​t1​​f(h(t),t,θ)dth(t1​)=h(t0​)+∫t0​t1​​f(h(t),t,θ)dt​ 求解这个常微分方程数值解的方法有很多，最原始的是欧拉法：固定Δt\\Delta tΔt ,通过逐步迭代来求解： h(t+Δt)=h(t)+Δt∗f(h(t),t,θ)h(t+\\Delta t)=h(t)+\\Delta t * f(h(t),t,\\theta) h(t+Δt)=h(t)+Δt∗f(h(t),t,θ) 我们看到，如果令Δt=1\\Delta t=1Δt=1 ,离散化的欧拉法就退化成残差模块的表达式，也就是说ResNet可以看成是ODENet的特殊情况。 但欧拉法只是解常微分方程最基础的解法，它每走一步都会产生误差，并且误差会层层累积起来。近百年来，在数学和物理学领域已经有更成熟的ODE Solve方法，它们不仅能保证收敛到真实解，而且能够控制误差，本文在不涉及ODE Solve内部结构的前提下(将ODE Solve作为一个黑盒来使用)，研究如何用ODE Solve帮助机器学习。 这篇文章使用了一种适应性的ODE solver，它不像ResNet那样固定步长，而是根据给定的误差容忍度自动调整步长，黑色的评估位置可以视作神经元，他的位置也会根据误差容忍度自动调整： 使用ODENet的几个好处（和原文不完全一致，详细可看原文）： 一般的神经网络利用链式法则，将梯度从最外层的函数逐层向内传播，并更新每一层的参数θ\\thetaθ ,这就需要在前向传播中需要保留所有层的激活值，并在沿计算路径反传梯度时利用这些激活值。这对内存的占用非常大，层数越多，占用的内存也越大，这限制了深度模型的训练过程。 本文给出的用ODENet反向传播的方法不存储任何中间过程，因而不管层数如何加深，只需要常数级的内存成本。 自适应的计算。传统的欧拉法会有误差逐层累积的缺陷，而ODENet可以在训练过程中实时的监测误差水平，并可以调整精度来控制模型的成本。例如：在训练时我们可以使用较高的精度使训练的模型尽可能准确，而在测试时可以使用较低的精度，减少测试成本。 应用在流模型上会极大简化变分公式的计算，在下文中详细讲解 在时间上的连续性，好理解不展开 对于ODEnet在流模型上的应用，可以看一下论文FFJORD。 反向传播 在训练连续神经网络的过程中，正向传播可以使用ODE slove。但对ODE solve求导来进行反向传播求解梯度是很困难的，本篇文章使用Pontryagin的伴随方法(adjoint method) 来求解梯度，该方法不仅在计算和内存上有更大优势，同时还能够精确地控制数值误差。 具体而言，对于： L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))(1)\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right)\\tag{1} L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ))(1) 为优化LLL ,我们需要计算他对于参数z(t0),t0,t1z(t_0),t_0,t_1z(t0​),t0​,t1​ 和θ\\thetaθ 的梯度。 第一步是确定loss的梯度如何取决于隐藏状态z(t)z(t)z(t) 的变化，这在文章中被称作伴随a(t)a(t)a(t) (adjointa(t)adjoint \\quad a(t)adjointa(t) ) a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 这个a(t)a(t)a(t) 实际等价于反向传播算法中的梯度，可以由另一个ODE给定(证明补充在后面)： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 在传统的基于链式法则的反向传播过程中，我们将后一层对前一层进行求导以传递梯度(∂L/∂z(t0)=∂L/∂z(t1)∗∂z(t1)/∂z(t0)\\partial L/\\partial z(t_0)=\\partial L/\\partial z(t_1) * \\partial z(t_1) / \\partial z(t_0)∂L/∂z(t0​)=∂L/∂z(t1​)∗∂z(t1​)/∂z(t0​))，而在ODENet中，可以再次调用ODESolve计算∂L/∂z(t0)\\partial L/\\partial z(t_0)∂L/∂z(t0​)。 对于计算相对于参数θ\\thetaθ 的梯度，公式类似： dLdθ=∫t1t0a(t)⊤∂f(z(t),t,θ)∂θdt(3)\\frac{d L}{d \\theta}=\\int_{t_{1}}^{t_{0}} a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t\\tag{3} dθdL​=∫t1​t0​​a(t)⊤∂θ∂f(z(t),t,θ)​dt(3) 这三个积分(带标号的三个)可以在同一个ODE solver过程中进行计算： 简单解释如何理解上面这个算法： 以前向传播为例，ODESolve(h(t0),f,t0,t1,θ)ODESolve(h(t_0),f,t_0,t_1,\\theta)ODESolve(h(t0​),f,t0​,t1​,θ) 表示求解常微分方程dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta)dtdh(t)​=f(h(t),t,θ) 的数值解h(t1)h(t_1)h(t1​) 。 将积分串联在一起以使用一次ODESolver解出所有量。 如果Loss不仅仅取决于最终状态，那么在用ODENet进行反向传播时，需要在这些状态中进行一系列的单独求解，每次都要调整算法中的 adjoint a(t)a(t)a(t) 。 用ODE网络替代ResNet 对图像进行两次下采样，然后分别应用六个残差块和一个ODENet进行对比： RK-Net是用Runge-Kutta积分器，直接进行反向误差的传播 LLL 表示ResNet的隐藏层数，L~\\tilde{L}L~ 表示调用ODESolve的次数。 误差控制，前向传播和反向传播的求值次数，网络深度表现在下图： a：求值次数和精度成反比 b：求值次数与时间成正比 c：求值次数与反向传播时间成正比，并且反向传播的时间大概是正向传播的一半 d：网络深度，由于ODENet是一个连续网络，没有隐藏层，因此将评估点的数量作为深度，可以看到在训练过程中网络深度逐渐增加 连续的归一化流模型 流模型的解读在前一篇博客中 流模型使用一个可逆函数fff 进行两个分布之间的映射，变换前后的两个分布满足变量代换定理： z1=f(z0)⟹log⁡p(z1)=log⁡p(z0)−log⁡∣det⁡∂f∂z0∣\\mathrm{z}_{1}=f\\left(\\mathrm{z}_{0}\\right) \\Longrightarrow \\log p\\left(\\mathrm{z}_{1}\\right)=\\log p\\left(\\mathrm{z}_{0}\\right)-\\log \\left|\\operatorname{det} \\frac{\\partial f}{\\partial \\mathrm{z}_{0}}\\right| z1​=f(z0​)⟹logp(z1​)=logp(z0​)−log∣∣∣∣​det∂z0​∂f​∣∣∣∣​ 平面归一化流(NICE之后的一篇流模型的文章，这篇论文没看……)使用的变换： z(t+1)=z(t)+uh(w⊤z(t)+b),log⁡p(z(t+1))=log⁡p(z(t))−log⁡∣1+u⊤∂h∂z∣\\mathbf{z}(t+1)=\\mathbf{z}(t)+u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\log p(\\mathbf{z}(t+1))=\\log p(\\mathbf{z}(t))-\\log \\left|1+u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}}\\right| z(t+1)=z(t)+uh(w⊤z(t)+b),logp(z(t+1))=logp(z(t))−log∣∣∣∣​1+u⊤∂z∂h​∣∣∣∣​ 在流模型中，为使∂f/∂z\\partial f/\\partial z∂f/∂z 的雅可比行列式易于计算，通常是通过精心构建函数fff 来实现。并且fff 还需要是可逆的。而在这篇文章里发现，将离散的流模型换成连续流模型，可以极大的简化计算：不需要去计算∂f/∂z\\partial f/ \\partial z∂f/∂z 的行列式，只需要计算迹，并且不需要构建fff 可逆：fff 可以是任意函数，它是天然可逆的(常微分方程决定的函数只要满足唯一性，就一定是双射的)，因此fff 理论上可以是任何网络。 核心定理（梯度变元定理）： 证明过程见附录 于是我们将平面归一化流模型连续化： dz(t)dt=uh(w⊤z(t)+b),∂log⁡p(z(t))∂t=−u⊤∂h∂z(t)\\frac{d \\mathbf{z}(t)}{d t}=u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}(t)} dtdz(t)​=uh(w⊤z(t)+b),∂t∂logp(z(t))​=−u⊤∂z(t)∂h​ 不同于求行列式的值，求迹还是一个连续函数，因此如果常微分方程dz/dtdz/dtdz/dt 是由一组函数的和给出的，那么对数概率密度也可以直接用迹的和表示： dz(t)dt=∑n=1Mfn(z(t)),dlog⁡p(z(t))dt=∑n=1Mtr⁡(∂fn∂z)\\frac{d \\mathbf{z}(t)}{d t}=\\sum_{n=1}^{M} f_{n}(\\mathbf{z}(t)), \\quad \\frac{d \\log p(\\mathbf{z}(t))}{d t}=\\sum_{n=1}^{M} \\operatorname{tr}\\left(\\frac{\\partial f_{n}}{\\partial \\mathbf{z}}\\right) dtdz(t)​=n=1∑M​fn​(z(t)),dtdlogp(z(t))​=n=1∑M​tr(∂z∂fn​​) 因此对于有M个隐藏状态的连续流模型来说，计算成本仅仅是O(M)\\mathcal{O}\\left(M\\right)O(M),而平面归一化流的计算成本是O(M3)\\mathcal{O}\\left(M^{3}\\right)O(M3) 。 NF和CNF的比较： 通过ODE对时间序列建模 zt0∼p(zt0)z_{t_{0}} \\sim p\\left(z_{t_{0}}\\right) zt0​​∼p(zt0​​) zt1,zt2,…,ztN=ODESolve(zt0,f,θf,t0,…,tN)z_{t_{1}}, z_{t_{2}}, \\ldots, z_{t_{N}} =ODESolve(z_{t_0},f,\\theta_f,t_0,\\ldots,t_N) zt1​​,zt2​​,…,ztN​​=ODESolve(zt0​​,f,θf​,t0​,…,tN​) eachxti∼p(x∣zti,θX)each \\quad x_{t_i} \\sim p(x|z_{t_i},\\theta_X) eachxti​​∼p(x∣zti​​,θX​) 具体而言，在给定初始状态 z0z_0z0​ 和观测时间 t0,…tNt_0,\\ldots t_Nt0​,…tN​ 的情况下，该模型计算潜在状态 zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 和输出 xt1…xtNx_{t_1} \\ldots x_{t_N}xt1​​…xtN​​。在实验部分，初始状态z0z_0z0​由RNN编码产生，潜在状态zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 由ODESolve产生，其中的fff 用神经网络训练，然后利用VAE的方式从潜在状态中生成数据。 实验：从采样点进行螺旋线重建 均方差比较： 附录 伴随法的证明 对于z(t)z(t)z(t) 给定常微分方程： dz(t)d(t)=f(z(t),t,θ)\\frac{dz(t)}{d(t)}=f(z(t),t,\\theta) d(t)dz(t)​=f(z(t),t,θ) L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right) L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ)) 定义便随状态： a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 则： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 证明： z(t+ε)=∫tt+εf(z(t),t,θ)dt+z(t)=Tε(z(t),t)\\mathbf{z}(t+\\varepsilon)=\\int_{t}^{t+\\varepsilon} f(\\mathbf{z}(t), t, \\theta) d t+\\mathbf{z}(t)=T_{\\varepsilon}(\\mathbf{z}(t), t) z(t+ε)=∫tt+ε​f(z(t),t,θ)dt+z(t)=Tε​(z(t),t) 然后应用链式法则，有： dL∂z(t)=dLdz(t+ε)dz(t+ε)dz(t) or a(t)=a(t+ε)∂Tε(z(t),t)∂z(t)\\frac{d L}{\\partial \\mathbf{z}(t)}=\\frac{d L}{d \\mathbf{z}(t+\\varepsilon)} \\frac{d \\mathbf{z}(t+\\varepsilon)}{d \\mathbf{z}(t)} \\quad \\text { or } \\quad \\mathbf{a}(t)=\\mathbf{a}(t+\\varepsilon) \\frac{\\partial T_{\\varepsilon}(\\mathbf{z}(t), t)}{\\partial \\mathbf{z}(t)} ∂z(t)dL​=dz(t+ε)dL​dz(t)dz(t+ε)​ or a(t)=a(t+ε)∂z(t)∂Tε​(z(t),t)​ 利用导数定义，并进行泰勒展开进行化简计算： da(t)dt=lim⁡ε→0+a(t+ε)−a(t)ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)Tε(z(t))ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)(z(t)+εf(z(t),t,θ)+O(ε2))ε=lim⁡ε→0+a(t+ε)−a(t+ε)(I+ε∂f(z(t),t,θ)θz(t)+O(ε2))ε=lim⁡ε→0+−εa(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε2)ε=lim⁡ε→0+−a(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε)=−a(t)∂f(z(t),t,θ)∂z(t)\\begin{aligned} \\frac{d \\mathbf{a}(t)}{d t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)} T_{\\varepsilon}(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)}\\left(\\mathbf{z}(t)+\\varepsilon f(\\mathbf{z}(t), t, \\theta)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon)\\left(I+\\varepsilon \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\theta \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{-\\varepsilon \\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}}-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}(\\varepsilon) \\\\ &amp;=-\\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)} \\end{aligned} dtda(t)​​=ε→0+lim​εa(t+ε)−a(t)​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​Tε​(z(t))​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​(z(t)+εf(z(t),t,θ)+O(ε2))​=ε→0+lim​εa(t+ε)−a(t+ε)(I+εθz(t)∂f(z(t),t,θ)​+O(ε2))​=ε→0+lim​ε−εa(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε2)​=ε→0+lim​−a(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε)=−a(t)∂z(t)∂f(z(t),t,θ)​​ 对于θ\\thetaθ 和ttt 定义增强状态： ddt[zθt](t)=faug⁡([z,θ,t]):=[f([z,θ,t])01],aaug:=[aaθat],aθ(t):=dLdθ(t),at(t):=dLdt(t)\\frac{d}{d t}\\left[\\begin{array}{l} \\mathrm{z} \\\\ \\theta \\\\ t \\end{array}\\right](t)=f_{\\operatorname{aug}}([\\mathrm{z}, \\theta, t]):=\\left[\\begin{array}{c} f([\\mathrm{z}, \\theta, t]) \\\\ 0 \\\\ 1 \\end{array}\\right], \\mathbf{a}_{a u g}:=\\left[\\begin{array}{l} \\mathrm{a} \\\\ \\mathrm{a}_{\\theta} \\\\ \\mathrm{a}_{t} \\end{array}\\right], \\mathrm{a}_{\\theta}(t):=\\frac{d L}{d \\theta(t)}, \\mathrm{a}_{t}(t):=\\frac{d L}{d t(t)} dtd​⎣⎡​zθt​⎦⎤​(t)=faug​([z,θ,t]):=⎣⎡​f([z,θ,t])01​⎦⎤​,aaug​:=⎣⎡​aaθ​at​​⎦⎤​,aθ​(t):=dθ(t)dL​,at​(t):=dt(t)dL​ 其中θ\\thetaθ 和ttt 无关，即dθ(t)/dt=0,dt(t)/dt=1d\\theta(t) /dt=0,dt(t)/dt=1dθ(t)/dt=0,dt(t)/dt=1 计算雅可比行列式： ∂faug∂[z,θ,t]=[∂f∂z∂f∂θ∂f∂t000000]\\frac{\\partial f_{a u g}}{\\partial[\\mathbf{z}, \\theta, t]}=\\left[\\begin{array}{ccc} \\frac{\\partial f}{\\partial z} &amp; \\frac{\\partial f}{\\partial \\theta} &amp; \\frac{\\partial f}{\\partial t} \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right] ∂[z,θ,t]∂faug​​=⎣⎡​∂z∂f​00​∂θ∂f​00​∂t∂f​00​⎦⎤​ 直接将faugf_{aug}faug​ 和aauga_{aug}aaug​ 代入上一小节的伴随法公式： daaug(t)dt=−[a(t)aθ(t)at(t)]∂faug∂[z,θ,t](t)=−[a∂f∂za∂f∂θa∂f∂t](t)\\frac{d \\mathbf{a}_{a u g}(t)}{d t}=-\\left[\\begin{array}{lllll} \\mathbf{a}(t) &amp; \\mathbf{a}_{\\theta}(t) &amp; \\mathbf{a}_{t}(t) \\end{array}\\right] \\frac{\\partial f_{\\text {aug}}}{\\partial[\\mathbf{z}, \\theta, t]}(t)=-\\left[\\begin{array}{lll} \\mathbf{a} \\frac{\\partial f}{\\partial \\mathbf{z}} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial \\theta} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial t} \\end{array}\\right](t) dtdaaug​(t)​=−[a(t)​aθ​(t)​at​(t)​]∂[z,θ,t]∂faug​​(t)=−[a∂z∂f​​a∂θ∂f​​a∂t∂f​​](t) 于是得到了最终的结论： dLdθ=∫tNt0a(t)∂f(z(t),t,θ)∂θdt\\frac{d L}{d \\theta}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t dθdL​=∫tN​t0​​a(t)∂θ∂f(z(t),t,θ)​dt dLdtN=−a(tN)∂f(z(tN),tN,θ)∂tNdLdt0=∫tNt0a(t)∂f(z(t),t,θ)∂tdt\\frac{d L}{d t_{N}}=-\\mathbf{a}\\left(t_{N}\\right) \\frac{\\partial f\\left(\\mathbf{z}\\left(t_{N}\\right), t_{N}, \\theta\\right)}{\\partial t_{N}} \\quad \\frac{d L}{d t_{0}}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial t} d t dtN​dL​=−a(tN​)∂tN​∂f(z(tN​),tN​,θ)​dt0​dL​=∫tN​t0​​a(t)∂t∂f(z(t),t,θ)​dt 梯度变元定理的证明 给定常微分方程： dz(t)d(t)=f(z(t),t)\\frac{dz(t)}{d(t)}=f(z(t),t) d(t)dz(t)​=f(z(t),t) fff 要求对zzz Lipschitz连续，对ttt 连续。则对数概率密度满足： ∂log⁡p(z(t))∂t=−tr⁡(dfdz(t))\\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-\\operatorname{tr}\\left(\\frac{d f}{d \\mathbf{z}}(t)\\right) ∂t∂logp(z(t))​=−tr(dzdf​(t)) 证明： 首先类似上面伴随法证明的过程，将z(t+ε)z(t+\\varepsilon)z(t+ε) 表示为Tε(z(t))T_{\\varepsilon}(\\mathbf{z}(t))Tε​(z(t)) fff 要求对zzz Lipschitz连续，对ttt 连续。这是为了使方程满足Picard存在定理，使得解存在且唯一。 首先是要推导出 ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ϵ∂∂zTε(z(t)))\\frac{\\partial \\log p(z(t))}{\\partial t}=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\epsilon} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(z(t))\\right) ∂t∂logp(z(t))​=−tr(ε→0+lim​∂ϵ∂​∂z∂​Tε​(z(t))) 过程： ∂log⁡p(z(t))∂t=lim⁡ε→0+log⁡p(z(t))−log⁡∣det⁡∂∂zTε(z(t))∣−log⁡p(z(t))ε=−lim⁡ε→0+log⁡∣det⁡∂∂zTε(z(t))∣ε=−lim⁡ε→0+∂∂εlog⁡∣det⁡∂∂zTε(z(t))∣∂∂εε=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣∣det⁡∂∂zTε(z(t))∣(∂log⁡(z)∂z∣z=1=1)=−(lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣)⏟bounded (lim⁡ε→0+1∣det⁡∂∂zTε(z(t))∣)=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log p(\\mathbf{z}(t))-\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|-\\log p(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon} \\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\frac{\\partial}{\\partial \\varepsilon} \\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|} \\qquad \\qquad \\quad \\left(\\left.\\frac{\\partial \\log (\\mathbf{z})}{\\partial \\mathbf{z}}\\right|_{\\mathbf{z}=1}=1\\right)\\\\ &amp;=-\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0+} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|\\right)}_{\\text {bounded }}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{1}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}\\right)\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right| \\end{aligned} ∂t∂logp(z(t))​​=ε→0+lim​εlogp(z(t))−log∣∣​det∂z∂​Tε​(z(t))∣∣​−logp(z(t))​=−ε→0+lim​εlog∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∂ε∂​ε∂ε∂​log∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​∂ε∂​∣∣​det∂z∂​Tε​(z(t))∣∣​​(∂z∂log(z)​∣∣∣∣​z=1​=1)=−bounded (ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​)​​(ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​1​)=−ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​​ 第一步用到的是流模型中的公式(本质是概率密度上的雅可比公式)，后面仅用到洛必达法则、链式法则等简单技巧。 然后应用雅可比公式： ∂log⁡p(z(t))∂t=−lim⁡ε→0+tr⁡(adj⁡(∂∂zTε(z(t)))∂∂ε∂∂zTε(z(t)))=−tr⁡((lim⁡ε→0+adj⁡(∂∂zTε(z(t))))⏟=I(lim⁡ε→0+∂∂ε∂∂zTε(z(t))))=−tr⁡(lim⁡ε→0+∂∂ε∂∂zTε(z(t)))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{tr}\\left(\\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right)}_{=I}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\end{aligned} ∂t∂logp(z(t))​​=−ε→0+lim​tr(adj(∂z∂​Tε​(z(t)))∂ε∂​∂z∂​Tε​(z(t)))=−tr⎝⎜⎜⎛​=I(ε→0+lim​adj(∂z∂​Tε​(z(t))))​​(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))⎠⎟⎟⎞​=−tr(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))​ 雅可比公式是： ddtdet(A(t))=tr(adj(A(t))ddtA(t))\\frac{d}{dt}det(A(t))=tr(adj(A(t))\\frac{d}{dt}A(t)) dtd​det(A(t))=tr(adj(A(t))dtd​A(t)) 最后进行泰勒展开即可： ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ε∂∂z(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+∂∂ε(I+∂∂zεf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+(∂∂zf(z(t),t)+O(ε)+O(ε2)+…))=−tr⁡(∂∂zf(z(t),t))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}}\\left(\\mathbf{z}+\\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left(I+\\frac{\\partial}{\\partial \\mathbf{z}} \\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)+\\mathcal{O}(\\varepsilon)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)\\right) \\end{aligned} ∂t∂logp(z(t))​​=−tr(ε→0+lim​∂ε∂​∂z∂​(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​∂ε∂​(I+∂z∂​εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​(∂z∂​f(z(t),t)+O(ε)+O(ε2)+…))=−tr(∂z∂​f(z(t),t))​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"gan,vae和flow","slug":"GAN VAE and Flow","date":"2020-02-15T13:19:00.000Z","updated":"2020-07-01T11:08:38.809Z","comments":true,"path":"2020/02/15/GAN VAE and Flow/","link":"","permalink":"/2020/02/15/GAN VAE and Flow/","excerpt":"","text":"前言 GAN，VAE和FLOW的目标是一致的——希望构建一个从隐变量ZZZ生成目标数据XXX的模型，其中先验分布P(z)P(z)P(z)通常被设置为高斯分布。我们希望找到一个变换函数f(x)f(x)f(x)，他能建立一个从zzz到xxx的映射：f:z→xf:z\\to xf:z→x，然后在P(Z)P(Z)P(Z)中随机采样一个点z′z&#x27;z′，通过映射fff，就可以找到一个新的样本点x′x&#x27;x′。 举个栗子： 如何将均匀分布U[0,1]U[0,1]U[0,1]映射成正态分布N(0,1)N(0,1)N(0,1)？ 将X∼U[0,1]X \\sim U[0,1]X∼U[0,1]经过函数Y=f(x)Y = f(x)Y=f(x)映射之后，就有Y∼N(0,1)Y\\sim N(0,1)Y∼N(0,1)了那么[x,x+dx][x,x+dx][x,x+dx]和[y,y+dy][y,y+dy][y,y+dy]两个区间上的概率应该相等，即： ρ(x)dx=12πexp⁡(−y22)dy\\rho(x) d x=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{y^{2}}{2}\\right) d y ρ(x)dx=2π​1​exp(−2y2​)dy 对其进行积分，有： ∫0xρ(t)dt=∫−∞y12πexp⁡(−t22)dt=Φ(y)\\int_{0}^{x} \\rho(t) d t=\\int_{-\\infty}^{y} \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{t^{2}}{2}\\right) d t=\\Phi(y) ∫0x​ρ(t)dt=∫−∞y​2π​1​exp(−2t2​)dt=Φ(y) y=Φ−1(∫0xρ(t)dt)=f(x)y=\\Phi^{-1}\\left(\\int_{0}^{x} \\rho(t) d t\\right)=f(x) y=Φ−1(∫0x​ρ(t)dt)=f(x) 可以看到Y=f(X)Y = f(X)Y=f(X)的解是存在的，但很复杂，无法用初等函数进行显示的表示，因此在大多数情况下，我们都是通过神经网络来拟合这个函数。 假设我们现在已经有一个映射fff，我们如何衡量映射fff构造出来的数据集f(z1),f(z2),...,f(zn)f(z_1),f(z_2),...,f(z_n)f(z1​),f(z2​),...,f(zn​)，是否和目标数据XXX分布相同？(注：KL和JS距离根据两个概率分布的表达式计算分布的相似度，而我们现在只有从构造的分布采样的数据和真实分布采样的数据，而离散化的KL和JS距离因为图像维度问题，计算量非常大)。在这里GAN采用了一个暴力的办法：训练一个判别器作为两者相似性的度量，而VAE(变分自编码器)和FLOW(流模型)在最大化最大似然。 VAE(变分自编码器) VAE的基本思路 对于连续随机变量，概率分布PPP和QQQ，KL散度(又称相对熵)的定义为： DKL(P∥Q)=∫−∞∞p(x)ln⁡p(x)q(x)dx=Ex∼P(x)[logP(x)−logQ(x)]D_{\\mathrm{KL}}(P \\| Q)=\\int_{-\\infty}^{\\infty} p(x) \\ln \\frac{p(x)}{q(x)} \\mathrm{d} x=E_{x \\sim P(x)}[logP(x)-logQ(x)] DKL​(P∥Q)=∫−∞∞​p(x)lnq(x)p(x)​dx=Ex∼P(x)​[logP(x)−logQ(x)] 给定一个概率分布DDD,已知其概率密度函数(连续分布)或概率质量函数(离散分布)为fDf_DfD​，以及一个分布参数θ\\thetaθ，我们可以从这个分布中抽出一个具有nnn个值的采样X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，利用fDf_DfD​计算出其似然函数： L(θ∣x1,…,xn)=fθ(x1,…,xn)\\mathbf{L}\\left(\\theta | x_{1}, \\ldots, x_{n}\\right)=f_{\\theta}\\left(x_{1}, \\ldots, x_{n}\\right) L(θ∣x1​,…,xn​)=fθ​(x1​,…,xn​) 若DDD是离散分布，fθf_{\\theta}fθ​即是在参数为θ\\thetaθ时观测到这一采样的概率。若其是连续分布，fθf_{\\theta}fθ​则为X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​联合分布的概率密度函数在观测值处的取值。一旦我们获得X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，我们就能求得一个关于θ\\thetaθ的估计。最大似然估计会寻找关于的最可能的值（即，在所有可能的取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的值即称为的最大似然估计。由定义，最大似然估计是样本的函数。 注：下面忽略积分号和求和的差异 VAE做最大似然估计，也就是要最大化概率： P(X)=∑iP(X∣zi;θ)P(zi)P(X)=\\sum_{i} P\\left(X | z_{i} ; \\theta\\right) P\\left(z_{i}\\right) P(X)=i∑​P(X∣zi​;θ)P(zi​) 一般选择P(Z)P(Z)P(Z)服从一个高斯分布，而p(X∣z)p(X|z)p(X∣z)可以是任意分布，例如条件高斯分布或狄拉克分布，理论上讲，这个积分形式的分布可以拟合任意分布。 实际上，最大似然估计与最小化两个分布之间的KL距离是等价的: argminKL(p(x)∥q(x))=argmin∫p(x)logp(x)q(x)dx=argmax∫p(x)q(x)dx=argmaxEx∼p(x)q(x)\\begin{aligned} argmin{KL(p(x)\\|q(x))} &amp;= argmin \\int p(x)log \\frac{p(x)}{q(x)}dx \\\\ &amp;= argmax \\int p(x)q(x)dx \\\\ &amp;=argmaxE_{x \\sim p(x)}q(x) \\end{aligned} argminKL(p(x)∥q(x))​=argmin∫p(x)logq(x)p(x)​dx=argmax∫p(x)q(x)dx=argmaxEx∼p(x)​q(x)​ 但是这里的P(X)P(X)P(X)是积分形式的，很难进行计算。VAE从让人望而生畏的变分和贝叶斯理论出发，推导出了一个很接地气的公式： log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)](1)\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] \\tag{1} logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)](1) VAE并没有选择直接去优化P(X)P(X)P(X)，而是选择去优化他的一个变分下界（公式1右端）。 而VAE的自编码器性质也从这个公式里开始体现出来：我们可以将D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]视作编码器的优化，使由真实数据编码出的隐变量分布Q(z∣X)Q(z|X)Q(z∣X)去尽量近似P(z)P(z)P(z)（标准高斯分布），而将Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]视作解码器的优化，使得服从分布QQQ的隐变量zzz解码出的xxx尽可能地服从真是数据分布，而将D[Q(z∣X)∥P(z∣X)]\\mathcal{D}[Q(z | X) \\| P(z | X)]D[Q(z∣X)∥P(z∣X)]视作误差项。 但VAE也因为它并没有直接去优化P(X)P(X)P(X)，而选择去优化它的变分下界，使得他只是一个近似模型，无法保证良好的生成效果。 VAE的优化过程 首先要确定概率密度Q(z∣X)Q(z|X)Q(z∣X)的形式，一般选择正态分布，即N(μ,σ2)\\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)N(μ,σ2)，其中μ(X;θμ),σ2(X;θσ)\\mu\\left(X ; \\theta_{\\mu}\\right) , \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)μ(X;θμ​),σ2(X;θσ​)通过两个神经网络(编码器)训练出来。公式中的D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]变为D[N(μ(X;θμ),σ2(X;θσ))∥N(0,I)]D\\left[\\mathcal{N}\\left(\\mu\\left(X ; \\theta_{\\mu}\\right), \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)\\right) \\| \\mathcal{N}(0, I)\\right]D[N(μ(X;θμ​),σ2(X;θσ​))∥N(0,I)]，这个时候就可以通过两个正态分布的KL散度的计算公式来计算这一项。 对于第一项Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]，对于一个batch来说，可以在QQQ中采样，然后将单个样本的log⁡P(X∣z)\\log P(X|z)logP(X∣z)求和取平均数作为期望的估计。但这样出现一个问题：把Q(z∣X)Q(z|X)Q(z∣X)弄丢了，也就是每次训练的时候梯度不传进QQQ里，论文里采用了一个称为重参数化技巧(reparamenterization trick)的方法，如图： 至此，整个VAE网络就可以训练了。 公式推导部分 D[Q(z∣X)∣∣P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)−log⁡P(X)]+log⁡P(X)\\begin{aligned} \\mathcal{D}[Q(z|X)||P(z|X)] &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X)] \\\\ &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X) - \\log P(X)] + \\log P(X) \\end{aligned} D[Q(z∣X)∣∣P(z∣X)]​=Ez∼Q​[logQ(z∣X)−logP(z∣X)]=Ez∼Q​[logQ(z∣X)−logP(z∣X)−logP(X)]+logP(X)​ 移项得 log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)]\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)] GAN 模型构建 由于大家都对GAN比较熟悉，本文直接从变分推断的角度去理解GAN。 不同于VAE将P(X∣z)P(X|z)P(X∣z)选为高斯分布，GAN的选择是： P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dzP(x | z)=\\delta(x-G(z)), \\quad P(x)=\\int P(x | z) P(z) d z P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dz 其中δ(x)\\delta (x)δ(x)是狄拉克函数，G(z)G(z)G(z)为生成器网络。 在VAE中z被当作是一个隐变量，但在GAN中，狄拉克函数意味着单点分布，即x和z为一一对应的关系。于是在GAN中z没有被当作隐变量处理(不需要考虑后验分布P(z∣x)P(z|x)P(z∣x)) 判别器的理解： 在GAN中引入了一个二元的隐变量y来构成联合分布，其中p~(x)\\tilde{p}(x)p~​(x) 为真实样本的分布： q(x,y)={p~(x)p1,y=1p(x)p0,y=0q(x, y)=\\left\\{\\begin{array}{l} {\\tilde{p}(x) p_{1}, y=1} \\\\ {p(x) p_{0}, y=0} \\end{array}\\right. q(x,y)={p~​(x)p1​,y=1p(x)p0​,y=0​ 这里y是图像的真实标签，当图片为真实图片时，y=1，当图片是生成图片时，y=0。 其中p0,p1p_0, p_1p0​,p1​代表权重，在下面讨论中我们直接取p0=p1=1/2p_0=p_1=1/2p0​=p1​=1/2 另一方面，我们需要使判别器的判别结果尽可能真实，设p(x,y)=p(y∣x)p~(x)p(x,y)=p(y|x)\\tilde{p}(x)p(x,y)=p(y∣x)p~​(x)，p(y∣x)p(y|x)p(y∣x)为一个条件伯努利分布(判别器的判别结果)。优化目标是KL(q(x,y)∣∣p(x,y))KL(q(x,y)||p(x,y))KL(q(x,y)∣∣p(x,y))： KL(q(x,y)∥p(x,y))=∫p~(x)p1log⁡p~(x)p1p(1∣x)p~(x)dx+∫p(x)p0log⁡p(x)p0p(0∣x)p~(x)dx∼∫p~(x)log⁡12p(1∣x)dx+∫p(x)log⁡p(x)2p(0∣x)p~(x)dx=−Ex∼p~(x)[log⁡2p(1∣x)]−Ex∼p(x)[log⁡2p(0∣x)]+KL(p(x)∣∣p~(x))\\begin{aligned} K L(q(x, y) \\| p(x, y)) &amp;=\\int \\tilde{p}(x) p_{1} \\log \\frac{\\tilde{p}(x) p_{1}}{p(1 | x) \\tilde{p}(x)} d x+\\int p(x) p_{0} \\log \\frac{p(x) p_{0}}{p(0 | x) \\tilde{p}(x)} d x \\\\ &amp; \\sim \\int \\tilde{p}(x) \\log \\frac{1}{2p(1 | x)} d x+\\int p(x) \\log \\frac{p(x)}{2p(0 | x) \\tilde{p}(x)} d x\\\\ &amp; = -E_{x \\sim \\tilde{p}(x)}[\\log 2p(1|x)]-E_{x \\sim p(x)}[\\log 2p(0|x)]+KL(p(x)||\\tilde{p}(x)) \\end{aligned} KL(q(x,y)∥p(x,y))​=∫p~​(x)p1​logp(1∣x)p~​(x)p~​(x)p1​​dx+∫p(x)p0​logp(0∣x)p~​(x)p(x)p0​​dx∼∫p~​(x)log2p(1∣x)1​dx+∫p(x)log2p(0∣x)p~​(x)p(x)​dx=−Ex∼p~​(x)​[log2p(1∣x)]−Ex∼p(x)​[log2p(0∣x)]+KL(p(x)∣∣p~​(x))​ 一旦成功优化，就有q(x,y)→p(x,y)q(x,y)\\to p(x,y)q(x,y)→p(x,y)，对于x求边缘概率分布，有： 12p~(x)+12p(x)→p(1∣x)p~(x)+p(0∣x)p~(x)=p~(x)\\frac{1}{2}\\tilde{p}(x)+\\frac{1}{2}p(x)\\to p(1|x)\\tilde{p}(x)+p(0|x)\\tilde{p}(x)=\\tilde{p}(x) 21​p~​(x)+21​p(x)→p(1∣x)p~​(x)+p(0∣x)p~​(x)=p~​(x) 即： p(x)→p~(x)p(x)\\to \\tilde{p}(x) p(x)→p~​(x) 这就完成了对模型的构建。 目标优化 现在我们有优化目标：p(1∣x)p(1|x)p(1∣x)和G(z)G(z)G(z)，分别是判别器(p(y∣x)p(y|x)p(y∣x)服从条件伯努利分布，可以直接由p(1∣x)p(1|x)p(1∣x)确定)和生成器(p(x)p(x)p(x)由G(z)G(z)G(z)决定)。类似EM算法，我们进行交替优化：先固定G(z)G(z)G(z),这也意味着p(x)p(x)p(x)固定了，然后优化p(y∣x)p(y|x)p(y∣x)，优化目标为： D=arg⁡min⁡D{−Ex∼p~(x)[log⁡2D(x)]−Ex∼p(x)[log⁡2(1−D(x))]}D=\\underset{D}{\\arg \\min }\\{-E_{x \\sim \\tilde{p}(x)}[\\log 2D(x)]-\\mathbb{E}_{x \\sim p(x)}[\\log 2(1-D(x))]\\} D=Dargmin​{−Ex∼p~​(x)​[log2D(x)]−Ex∼p(x)​[log2(1−D(x))]} 当生成器固定时，p(x)p(x)p(x) 也固定，分别记p~(x),p(x),D(x)\\tilde{p}(x),p(x),D(x)p~​(x),p(x),D(x) 为a,b,ta,b,ta,b,t ,忽略常数项，优化目标变为： D=argmax∫p~(x)log⁡t+p(x)log⁡(1−t)dxD=argmax \\int \\tilde{p}(x)\\log t + p(x) \\log(1-t)dx D=argmax∫p~​(x)logt+p(x)log(1−t)dx 求导算出其理论最优解为t=D(x)=p~(x)p~(x)+p0(x)t=D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}t=D(x)=p~​(x)+p0(x)p~​(x)​ 然后固定D(x)D(x)D(x)来优化G(x)G(x)G(x)，相关loss为： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dxG=\\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x G=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx 假设D(x)D(x)D(x)有足够的拟合能力，当D(x)=p~(x)p~(x)+p0(x)D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}D(x)=p~​(x)+p0(x)p~​(x)​时，有 KL(q(x,y)∥p0(x,y))=∫p~(x)log⁡12D(x)dx+∫p0(x)log⁡p0(x)2(1−D(x))p~(x)dx=∫p~(x)log⁡p~(x)+p0(x)2p~(x)+p0(x)log⁡p~(x)+p0(x)2p~(x)=KL(p~(x)+p0(x)∣∣2p~(x))\\begin{aligned} K L(q(x, y) \\| p^0(x, y)) &amp;= \\int \\tilde{p}(x) \\log \\frac{1}{2D(x)} d x+\\int p^0(x) \\log \\frac{p^0(x)}{2(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\int\\tilde{p}(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}+p^0(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}\\\\ &amp;= KL(\\tilde{p}(x) +p^0(x)||2\\tilde{p}(x) ) \\end{aligned} KL(q(x,y)∥p0(x,y))​=∫p~​(x)log2D(x)1​dx+∫p0(x)log2(1−D(x))p~​(x)p0(x)​dx=∫p~​(x)log2p~​(x)p~​(x)+p0(x)​+p0(x)log2p~​(x)p~​(x)+p0(x)​=KL(p~​(x)+p0(x)∣∣2p~​(x))​ 在优化判别器时，p0(x)p^0(x)p0(x)应该为上一阶段生成器优化的p(x)p(x)p(x) 。将这个D(x)D(x)D(x)代入生成器的相关loss： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dx=arg⁡min⁡G∫p(x)log⁡p(x)2D(x)p0(x)dx=arg⁡min⁡G[−Ex∼p(x)2D(x)+KL(p(x)∣∣p0(x))]=arg⁡min⁡G[−Ex∼p(x)2D(G(z))+KL(p(x)∣∣p0(x))]\\begin{aligned} G &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{ p(x)}{2D(x) p^0(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(x)+KL(p(x)||p^0(x))]\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(G(z))+KL(p(x)||p^0(x))] \\end{aligned} G​=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx=Gargmin​∫p(x)log2D(x)p0(x)p(x)​dx=Gargmin​[−Ex∼p(x)​2D(x)+KL(p(x)∣∣p0(x))]=Gargmin​[−Ex∼p(x)​2D(G(z))+KL(p(x)∣∣p0(x))]​ 可以看到，此时的第一项−Ex∼p(x)2D(G(z))-E_{x \\sim p(x)}2D(G(z))−Ex∼p(x)​2D(G(z))就是标准的GAN所采用的loss之一。而我们知道，目前标准的GAN生成器的loss都不包含KL(p(x)∣∣p0(x))KL(p(x)||p^0(x))KL(p(x)∣∣p0(x))，这实际上造成了loss的不完备。 第二个loss是在限制要求新的生成器跟旧的生成器生成结果不能差别太大 ，也就是生成器不能剧烈变化。在loss不完备的情况下，假设有一个优化算法总能找到G(z)G(z)G(z)的理论最优解、并且G(z)G(z)G(z)具有无限的拟合能力，那么G(z)G(z)G(z)只需要生成唯一一个使得D(x)D(x)D(x)最大的样本（不管输入的zzz是什么），这就是模型坍缩。模型塌缩的视频(需要梯子)。 然后对第二项进行估算，得到一个可以在实验中使用的正则项： 记po(x)=qθ−Δθ(x),p(x)=qθ(x)p^{o}(x)=q_{\\theta-\\Delta \\theta}(x), \\quad p(x)=q_{\\theta}(x)po(x)=qθ−Δθ​(x),p(x)=qθ​(x)，其中Δθ\\Delta \\thetaΔθ为生成器的参数变化，对qo(x)=qθ−Δθ(x)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)qo(x)=qθ−Δθ​(x)做泰勒展开，有： qo(x)=qθ−Δθ(x)=qθ(x)−Δθ⋅∇θqθ(x)+O((Δθ)2)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)=q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)+O\\left((\\Delta \\theta)^{2}\\right) qo(x)=qθ−Δθ​(x)=qθ​(x)−Δθ⋅∇θ​qθ​(x)+O((Δθ)2) KL(q(x)∥qo(x))≈∫qθ(x)log⁡qθ(x)qθ(x)−Δθ⋅∇θqθ(x)dx=−∫qθ(x)log⁡[1−Δθ⋅∇θqθ(x)qθ(x)]dx≈−∫qθ(x)[−Δθ⋅∇θqθ(x)qθ(x)−(Δθ⋅∇θqθ(x)qθ(x))2]dx=Δθ⋅∇θ∫qθ(x)dx+(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx=(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx≈(Δθ⋅c)2\\begin{aligned} K L\\left(q(x) \\| q^{o}(x)\\right) &amp; \\approx \\int q_{\\theta}(x) \\log \\frac{q_{\\theta}(x)}{q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)} d x \\\\ &amp;=-\\int q_{\\theta}(x) \\log \\left[1-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right] d x \\\\ &amp; \\approx-\\int q_{\\theta}(x)\\left[-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}-\\left(\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right)^{2}\\right] d x \\\\ &amp;=\\Delta \\theta \\cdot \\nabla_{\\theta} \\int q_{\\theta}(x) d x+(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp;=(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp; \\approx(\\Delta \\theta \\cdot c)^{2} \\end{aligned} KL(q(x)∥qo(x))​≈∫qθ​(x)logqθ​(x)−Δθ⋅∇θ​qθ​(x)qθ​(x)​dx=−∫qθ​(x)log[1−qθ​(x)Δθ⋅∇θ​qθ​(x)​]dx≈−∫qθ​(x)[−qθ​(x)Δθ⋅∇θ​qθ​(x)​−(qθ​(x)Δθ⋅∇θ​qθ​(x)​)2]dx=Δθ⋅∇θ​∫qθ​(x)dx+(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx=(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx≈(Δθ⋅c)2​ 上式中应用了log⁡(1+x)\\log(1+x)log(1+x)的泰勒展开式以及求导和积分可互换、可积分的假设。上面的粗略估计表明，生成器的参数不能变化太大。而我们用的是基于梯度下降的优化算法，所以Δθ\\Delta \\thetaΔθ正比于梯度，因此标准GAN训练时的很多trick，比如梯度裁剪、用Adam优化器、用BN，都可以解释得通了，它们都是为了稳定梯度，使得Δθ\\Delta \\thetaΔθ不至于过大，同时，G(z)G(z)G(z)的迭代次数也不能过多，因为过多同样会导致Δθ\\Delta \\thetaΔθ过大。 正则项 考虑如何添加正则项以改进GAN的稳定性： 直接对KL(q(x)∥qo(x))K L\\left(q(x) \\| q^{o}(x)\\right)KL(q(x)∥qo(x))进行估算是很困难的，但是我们上面提到q(z∣x)q(z|x)q(z∣x)和qo(z∣x)q^o(z|x)qo(z∣x)是狄拉克分布，而狄拉克分布可以看作方差为0的高斯分布，于是考虑用KL(q(x,z)∥qo(x,z))K L\\left(q(x,z) \\| q^{o}(x,z)\\right)KL(q(x,z)∥qo(x,z))进行估算： KL(q(x,z)∥q~(x,z))=∬q(x∣z)q(z)log⁡q(x∣z)q(z)q~(x∣z)q(z)dxdz=∬δ(x−G(z))q(z)log⁡δ(x−G(z))δ(x−Go(z))dxdz=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\iint q(x | z) q(z) \\log \\frac{q(x | z) q(z)}{\\tilde{q}(x | z) q(z)} d x d z \\\\ &amp;=\\iint \\delta(x-G(z)) q(z) \\log \\frac{\\delta(x-G(z))}{\\delta\\left(x-G^{o}(z)\\right)} d x d z \\\\ &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∬q(x∣z)q(z)logq~​(x∣z)q(z)q(x∣z)q(z)​dxdz=∬δ(x−G(z))q(z)logδ(x−Go(z))δ(x−G(z))​dxdz=∫q(z)logδ(G(z)−Go(z))δ(0)​dz​ 将狄拉克分布可以看作方差为0的高斯分布,并代入： δ(x)=lim⁡σ→01(2πσ2)d/2exp⁡(−x22σ2)\\delta(x)=\\lim _{\\sigma \\rightarrow 0} \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{d / 2}} \\exp \\left(-\\frac{x^{2}}{2 \\sigma^{2}}\\right) δ(x)=σ→0lim​(2πσ2)d/21​exp(−2σ2x2​) KL(q(x,z)∥q~(x,z))=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz=lim⁡σ→0∫q(x)log⁡[1/exp⁡(−(G(z)−G0(z))22σ2)]dx=lim⁡σ→0∫q(x)(−(G(z)−G0(z))22σ2)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\log \\left[ 1/{\\exp \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)} \\right]dx \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)dx \\\\ &amp; \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∫q(z)logδ(G(z)−Go(z))δ(0)​dz=σ→0lim​∫q(x)log[1/exp(−2σ2(G(z)−G0(z))2​)]dx=σ→0lim​∫q(x)(−2σ2(G(z)−G0(z))2​)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz​ 于是有 KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dzK L\\left(q(x) \\| q^{o}(x)\\right) \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dz 从而完整的生成器loss可以选择为 Ez∼q(z)[−log⁡D(G(z))+λ∥G(z)−Go(z)∥2]\\mathbb{E}_{z \\sim q(z)}\\left[-\\log D(G(z))+\\lambda\\left\\|G(z)-G^{o}(z)\\right\\|^{2}\\right] Ez∼q(z)​[−logD(G(z))+λ∥G(z)−Go(z)∥2] 实验结果 FLOW 基本思路：直接硬算积分式 ∫zp(x∣z)p(z)dz\\int_{z} p(x | z) p(z) d z ∫z​p(x∣z)p(z)dz 流模型有一个非常与众不同的特点是，它的转换通常是可逆的。也就是说，流模型不 仅能找到从 A 分布变化到 B 分布的网络通路，并且该通路也能让 B 变化到 A，简言之流模 型找到的是一条 A、B 分布间的双工通路。当然，这样的可逆性是具有代价的——A、B 的 数据维度必须是一致的。 A、B 分布间的转换并不是轻易能做到的，流模型为实现这一点经历了三个步骤：最初 的 NICE 实现了从 A 分布到高斯分布的可逆求解；后来 RealNVP 实现了从 A 分布到条件非 高斯分布的可逆求解；而最新的 GLOW，实现了从 A 分布到 B 分布的可逆求解，其中 B 分 布可以是与 A 分布同样复杂的分布，这意味着给定两堆图片，GLOW 能够实现这两堆图片 间的任意转换。 NICE 两个一维分布之间的转化参考前言中的栗子，下面考虑高维分布： 类似一维分布，两个分布在映射前后的相同区域应该有相同的概率。 p(x′)∣det⁡(Jf)∣=π(z′)p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|=\\pi\\left(z^{\\prime}\\right) p(x′)∣det(Jf​)∣=π(z′) 其中JfJ_fJf​为雅可比行列式，函数fff将zzz上的分布变换到xxx上的分布。 根据雅可比行列式的逆运算，同样有： p(x′)=π(z′)∣det⁡(Jf−1)∣p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right| p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​ 至此，我们得到了一个比较重要的结论：如果 zzz 与 xxx 分别满足两种分布，并且 zzz 通过 函数 fff 能够转变为 xxx，那么 zzz 与 xxx 中的任意一组对应采样点 𝑧′𝑧′z′ 与 𝑥′𝑥′x′ 之间的关系为： {π(z′)=p(x′)∣det⁡(Jf)∣p(x′)=π(z′)∣det⁡(Jf−1)∣\\left\\{\\begin{array}{c} {\\pi\\left(z^{\\prime}\\right)=p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|} \\\\ {p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right|} \\end{array}\\right. {π(z′)=p(x′)∣det(Jf​)∣p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​​ 从这个公式引入了Flow_based_model 的基本思路：设计一个神经网络，将分布 xxx 映射到分布 zzz ，具体来说，流模型选择 q(z)q(z)q(z) 为高斯分布，q(x∣z)q(x|z)q(x∣z) 为狄拉克分布 δ(x−g(z)\\delta(x-g(z)δ(x−g(z) ，其中ggg 是可逆的： x=g(z)⇔z=f(x)x=g(z) \\Leftrightarrow z=f(x) x=g(z)⇔z=f(x) 要从理论上实现可逆，需要 xxx 和 zzz 的维数相同，将 zzz 的分布代入，则有： q(z)=1(2π)D/2exp⁡(−12∥z∥2)q(z)=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|z\\|^{2}\\right) q(z)=(2π)D/21​exp(−21​∥z∥2) q(x)=1(2π)D/2exp⁡(−12∥f(x)∥2)∣det⁡[∂f∂x]∣(2)q(\\boldsymbol{x})=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|\\boldsymbol{f}(\\boldsymbol{x})\\|^{2}\\right)\\left|\\operatorname{det}\\left[\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}\\right]\\right|\\tag{2} q(x)=(2π)D/21​exp(−21​∥f(x)∥2)∣∣∣∣​det[∂x∂f​]∣∣∣∣​(2) 公式(2)(2)(2)对 fff 提出了三个基本要求： 可逆，且逆函数容易计算。 对应的雅可比行列式方便计算 拟合能力强 这样的话，就有 log⁡q(x)=−D2log⁡(2π)−12∥f(x)∥2+log⁡∣det⁡[∂f∂x]∣\\log q(x)=-\\frac{D}{2} \\log (2 \\pi)-\\frac{1}{2}\\|f(x)\\|^{2}+\\log \\left|\\operatorname{det}\\left[\\frac{\\partial f}{\\partial x}\\right]\\right| logq(x)=−2D​log(2π)−21​∥f(x)∥2+log∣∣∣∣​det[∂x∂f​]∣∣∣∣​ 这个优化目标是可计算的，并且因为 fff 可逆，那么我们在zzz 中取样，就可以生成相应的 xxx x=f−1(z)=g(z)x=f^{-1}(z)=g(z) x=f−1(z)=g(z) 为了满足这三个条件，NICE和REAL NVP、GLOW都采用了模块化思想，将 fff 设计成一组函数的复合，其中每个函数都满足要求一和要求二，经过复合之后函数也容易满足要求三。 f=fL∘…∘f2∘f1f=f_{L} \\circ \\ldots \\circ f_{2} \\circ f_{1} f=fL​∘…∘f2​∘f1​ 相对而言，雅可比行列式的计算要比函数求逆更加复杂，考虑第二个要求，我们知道三角行列式最容易计算，所以我们要想办法让变换 fff 的雅可比矩阵为三角阵。NICE的做法是：将 DDD 的 xxx 分为两部分 x1,x2x_1,x_2x1​,x2​，然后取下述变换： h1=x1h2=x2+m(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{x}_{2}+\\boldsymbol{m}\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=x2​+m(x1​)​ 其中 mmm 为任意函数，这个变换称为“加性耦合层” ，这个变换的雅可比矩阵 [∂h∂x][\\frac{\\partial h}{\\partial x}][∂x∂h​] 是一个三角阵，且对角线元素全部为1，用分块矩阵表示为： [∂h∂x]=(IdO∂m∂x1ID−d)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathrm{I}_{d}} &amp; {\\mathrm{O}} \\\\ \\frac{\\partial m}{\\partial x_1} &amp; I_{D-d} \\end{array}\\right) [∂x∂h​]=(Id​∂x1​∂m​​OID−d​​) 同时这个变换也是可逆的，其逆变换为 x1=xhx2=h2−m(h1)\\begin{array}{l} {\\boldsymbol{x}_{1}=\\boldsymbol{x}_{h}} \\\\ {\\boldsymbol{x}_{2}=\\boldsymbol{h}_{2}-\\boldsymbol{m}\\left(\\boldsymbol{h}_{1}\\right)} \\end{array} x1​=xh​x2​=h2​−m(h1​)​ 满足了要求一和要求二，同时这个雅可比行列式的值为1，行列式的值的物理含义是体积，所以这个变换暗含了变换前后的体积不变性。我们注意到：该变换的第一部分是平凡的（恒等变换），因此需要对调I1和I2两组维度，再输入加和耦合层，并将这个过程重复若干次， 以达到信息充分混合的目的，如图： 因为该变换需要满足 zzz 和 xxx 的维度相同，这会产生很严重的唯独浪费问题，NICE在最后一层里引入了一个尺度变换对维度进行缩放： z=s⊗h(n)z=s \\otimes h^{(n)} z=s⊗h(n) 其中s=(s1,s2,...,sD)s=(s_1,s_2,...,s_D)s=(s1​,s2​,...,sD​)也是一个要优化的参数向量，这个 sss 向量能够识别每个维度的重要程度， sss 越小，这个维度越不重要，起到压缩流形的作用。这个尺度变换层的雅可比行列式就不是一了，而是： [∂z∂h(n)]=diag⁡(s)\\left[\\frac{\\partial z}{\\partial \\boldsymbol{h}^{(n)}}\\right]=\\operatorname{diag}(\\boldsymbol{s}) [∂h(n)∂z​]=diag(s) 他的行列式的值为 ∏isi\\prod_{i} s_{i}∏i​si​,于是最后的对数似然为： log⁡q(x)∼−12∥s⊗f(x)∥2+∑ilog⁡si\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2}\\|\\boldsymbol{s} \\otimes \\boldsymbol{f}(\\boldsymbol{x})\\|^{2}+\\sum_{i} \\log \\boldsymbol{s}_{i} logq(x)∼−21​∥s⊗f(x)∥2+i∑​logsi​ 这个尺度变换实际上是将先验分布 q(z)q(z)q(z) 的方差也作为训练参数，方差越小，说明这个维度的“弥散”越小，若方差为0，这一维的特征就恒为均值，于是流行减小一维。 我们写出带方差的正态分布： q(z)=1(2π)D/2∏i=1Dσiexp⁡(−12∑i=1Dzi2σi2)q(z)=\\frac{1}{(2 \\pi)^{D / 2} \\prod_{i=1}^{D} \\sigma_{i}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{z_{i}^{2}}{\\sigma_{i}^{2}}\\right) q(z)=(2π)D/2∏i=1D​σi​1​exp(−21​i=1∑D​σi2​zi2​​) 将 z=f(x)z=f(x)z=f(x) 代入，并取对数，类似得： log⁡q(x)∼−12∑i=1Dfi2(x)σi2−∑i=1Dlog⁡σi\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{\\boldsymbol{f}_{i}^{2}(\\boldsymbol{x})}{\\boldsymbol{\\sigma}_{i}^{2}}-\\sum_{i=1}^{D} \\log \\boldsymbol{\\sigma}_{i} logq(x)∼−21​i=1∑D​σi2​fi2​(x)​−i=1∑D​logσi​ 与之前那个公式对比，就有 si=1/σis_i=1/\\sigma_isi​=1/σi​ ，所以尺度变换层等价于将先验分布的方差作为训练参数，若方差足够小，则维度减一，暗含了降维的可能。 REALNVP NICE构思巧妙，但在实验部分只是采取了简单的加性耦合层和将全连接层进行简单的堆叠，并没有使用卷积。REALNVP一般化了耦合层，并在耦合模型中引入了卷积层，使得模型可以更好地处理图像问题。论文里还引入了一个多尺度结构来处理维度浪费问题。 将加性耦合层换成仿射耦合层： h1=x1h2=s(x1)⊗x2+t(x1)(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{s}\\left(\\boldsymbol{x}_{1}\\right) \\otimes \\boldsymbol{x}_{2}+t\\left(\\boldsymbol{x}_{1}\\right)\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=s(x1​)⊗x2​+t(x1​)(x1​)​ 仿射耦合层的雅可比行列式仍然是一个对角阵 [∂h∂x]=(IdO[∂m∂x1]s)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathbb{I}_{d}} &amp; {\\mathbb{O}} \\\\ {\\left[\\frac{\\partial m}{\\partial x_{1}}\\right]} &amp; {s} \\end{array}\\right) [∂x∂h​]=(Id​[∂x1​∂m​]​Os​) 雅可比行列式的值不再是1，没有保持变换前后的体积不变。 在NICE中，通过交错的方式来混合信息流(直接反转原来的向量)，在REALNVP中发现：随机打乱维度可以使信息混合的更加充分。 引入卷积层：使用卷积的条件是具有局部相关性，因此指定向量的打乱和重排都是在channel维度上进行，在height和width维度上进行卷积。对通道的分割论文里还提出棋盘式分割的策略，但较为复杂，对模型的提升也不大，因此在GLOW中被舍弃了。 一般的图像通道数只有三层，MNIST等灰度图只有一层，因此REALNVP引入了squeeze操作来增加通道数。 其思想很简单：直接 reshape，但 reshape 时局部地进行。具体来说，假设原来图像为 h×w×c 大小，前两个轴是空间维度，然后沿着空间维度分为一个个 2×2×c 的块（ 2 可以自定义），然后将每个块直接 reshape 为 1×1×4c，最后变成了 h/2×w/2×4c。 REALNVP中还引入了一个多尺度结构： 最终的输出 z1,z3,z5z_1,z_3,z_5z1​,z3​,z5​ 怎么取？ p(z1,z3,z5)=p(z1∣z3,z5)p(z3∣z5)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{3}, z_{5}\\right) p\\left(z_{3} | z_{5}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z3​,z5​)p(z3​∣z5​)p(z5​) 由于 z3,z5z_3,z_5z3​,z5​ 是由 z2z_2z2​ 完全决定的，z5z_5z5​ 也是由 z4z_4z4​ 完全决定的，因此条件部分可以改为： p(z1,z3,z5)=p(z1∣z2)p(z3∣z4)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{2}\\right) p\\left(z_{3} | z_{4}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z2​)p(z3​∣z4​)p(z5​) RealNVP 和 Glow 假设右端三个概率分布都是正态分布，类似VAE， p(z1∣z2)p(z_1|z_2)p(z1​∣z2​) 的均值方差由 z2z_2z2​ 算出来，p(z3∣z4)p(z_3|z_4)p(z3​∣z4​) 的均值方差由 z4z_4z4​ 算出来，p(z5)p(z_5)p(z5​) 的均值方差直接学习出来。这相当于做了变量代换： z^1=z1−μ(z2)σ(z2),z^3=z3−μ(z4)σ(z4),z^5=z5−μσ\\hat{z}_{1}=\\frac{z_{1}-\\mu\\left(z_{2}\\right)}{\\sigma\\left(z_{2}\\right)}, \\quad \\hat{z}_{3}=\\frac{z_{3}-\\mu\\left(z_{4}\\right)}{\\sigma\\left(z_{4}\\right)}, \\quad \\hat{z}_{5}=\\frac{z_{5}-\\mu}{\\sigma} z^1​=σ(z2​)z1​−μ(z2​)​,z^3​=σ(z4​)z3​−μ(z4​)​,z^5​=σz5​−μ​ 然后认为 [z^1,z^3,z^5][\\hat{z}_1,\\hat{z}_3,\\hat{z}_5][z^1​,z^3​,z^5​]服从标准正态分布。类似NICE，这三个变换会导致一个非1的雅可比行列式，也就是往loss中加入 Σi=1Dlog⁡σi\\Sigma_{i=1}^{D} \\log \\sigma_{i}Σi=1D​logσi​ 这一项。 多尺度结构相当于抛弃了 p(z)p(z)p(z) 是标准正态分布的直接假设，而采用了一个组合式的条件分布，这样尽管输入输出的总维度依然一样，但是不同层次的输出地位已经不对等了，模型可以通过控制每个条件分布的方差来抑制维度浪费问题（极端情况下，方差为 0，那么高斯分布坍缩为狄拉克分布，维度就降低 1），条件分布相比于独立分布具有更大的灵活性。而如果单纯从 loss 的角度看，多尺度结构为模型提供了一个强有力的正则项。 GLOW 效果好的令人惊叹的生成模型： 改变图像属性 采样展示 潜在空间的插值 总体来说，GLOW引入1*1可逆卷积来代替通道维度的打乱和重排操作，并对 REALNVP 的原始模型做了简化和规范。 向量之间的元素置换操作可以用简单的行变换矩阵来操作： (badc)=(0100100000010010)(abcd)\\left(\\begin{array}{l} {b} \\\\ {a} \\\\ {d} \\\\ {c} \\end{array}\\right)=\\left(\\begin{array}{llll} {0} &amp; {1} &amp; {0} &amp; {0} \\\\ {1} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {1} \\\\ {0} &amp; {0} &amp; {1} &amp; {0} \\end{array}\\right)\\left(\\begin{array}{l} {a} \\\\ {b} \\\\ {c} \\\\ {d} \\end{array}\\right) ⎝⎜⎜⎛​badc​⎠⎟⎟⎞​=⎝⎜⎜⎛​0100​1000​0001​0010​⎠⎟⎟⎞​⎝⎜⎜⎛​abcd​⎠⎟⎟⎞​ GLOW中用一个更一般的矩阵 WWW 来代替这个置换矩阵 h=xWh=xW h=xW 这个变换的雅可比矩阵就是det(W)det(W)det(W)，因此需要将 −log∣det(W)∣-log|det(W)|−log∣det(W)∣ 加入到loss中，WWW 的初始选择要求可逆，不引入loss，因此选为随即正交阵。 这个变换引入了 det(W)det(W)det(W) 的计算问题，GLOW中逆用LU分解克服了这个问题，若 W=PLUW=PLUW=PLU (其中P是一个置换矩阵),则 log⁡∣det⁡W∣=∑log⁡∣diag⁡(U)∣\\log |\\operatorname{det} W|=\\sum \\log |\\operatorname{diag}(U)| log∣detW∣=∑log∣diag(U)∣ 这就是GLOW中给出的技巧：先随机生成一个正交矩阵，然后做 LULULU 分解，得到 P,L,UP,L,UP,L,U，固定 P，也固定 U 的对角线的正负号，然后约束 L 为对角线全 1 的下三角阵，U 为上三角阵，优化训练 L,U 的其余参数。 也可以理解为：在NICE和REALNVP中都是通过交换、打乱、重排等操作混合各个通道的信息，但通道信息的混合可直接通过1*1卷积实现。 整个GLOW模型如下： 对比 比较反转、打乱和1*1逆卷积的loss： 缺点 模型庞大，参数量极大，NICE模型在MNIST数据集上的训练参数就大概有两千万个。 再贴两个Glow模型在Gayhub Github上的issue感受下： 256*256的高清人脸生成，用一块GPU训练的话，大概要一年…… 一图对比GAN，VAE和FLOW 参考文献 Variational Inference: A Unified Framework of Generative Models and Some Revelations Tutorial on Variational Autoencoders 用变分推断统一理解生成模型（VAE、GAN、AAE、ALI） NICE: Non-linear Independent Components Estimation NOTE_FLOW Glow: Generative Flow with Invertible 1×1 Convolutions 细水长flow之NICE：流模型的基本概念与实现 RealNVP与Glow：流模型的传承与升华 Density estimation using Real NVP","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"gan","slug":"gan","permalink":"/tags/gan/"},{"name":"vae","slug":"vae","permalink":"/tags/vae/"},{"name":"flow","slug":"flow","permalink":"/tags/flow/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"shell脚本","slug":"shell脚本","date":"2020-02-10T05:32:00.000Z","updated":"2020-03-15T04:34:47.342Z","comments":true,"path":"2020/02/10/shell脚本/","link":"","permalink":"/2020/02/10/shell脚本/","excerpt":"","text":"# 指定解释器 #！/bin/bash # 向窗口输出文本 echo \"Hello world!\" printf \"Hello world!\" # for循环示例,使用变量要加$符号 for file in `ls /etc` do echo \"${file}\" done # 双引号和单引号 # 双引号中可以有变量，单引号中的变量是无效的 # if-else语句 if condition1 then command1 elif condition2 then commed2 else command3 fi","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"shell","slug":"shell","permalink":"/tags/shell/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"emacs基本操作","slug":"Emacs笔记","date":"2020-02-10T03:16:00.000Z","updated":"2020-03-15T04:18:17.714Z","comments":true,"path":"2020/02/10/Emacs笔记/","link":"","permalink":"/2020/02/10/Emacs笔记/","excerpt":"","text":"Emacs基本操作 C = Ctrl, M = Alt 光标移动 C-v 向下翻页 M-v 向上翻页 C-b 向左(back) C-f 向右(forward) C-n 向下(next) C-p 向上(previous) M-b 上一个单词 M-f 下一个单词 C-a 行首 C-e 行尾 M-a 句首 M-e 句尾 M-&lt; 文件头 M-&gt; 文件尾 M-g g 跳到某一行 选择区域 C-@ 标记 删除剪切复制粘贴 C-d 向后删除(delele) C-k 删掉光标后至行尾 M-w 复制区域 C-w 剪切/删除区域 C-y 粘贴 M-y 滚动选择粘贴内容 查找替换 C-s 向前查找 C-s C-r 向后查找 M-% 替换 文件操作 C-x C-f 打开文件(find) C-x C-s 保存文件(save) C-x C-w 另存为(write) C-k 关闭文件 窗口操作 C-x b 切换文件 C-x 1 关闭其它窗口 C-x 2/C-x 3 打开其它窗口 C-x o 跳到另一个窗口(other) 其它 C-/ 撤销 M-$ 拼写检查 M-x 输入命令 C-g 取消命令","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"emacs","slug":"emacs","permalink":"/tags/emacs/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"科学上网","slug":"科学上网","date":"2020-02-09T03:19:00.000Z","updated":"2020-09-12T10:46:07.846Z","comments":true,"path":"2020/02/09/科学上网/","link":"","permalink":"/2020/02/09/科学上网/","excerpt":"","text":"插件篇 如果只是想访问谷歌服务，可以直接在谷歌浏览器里安装谷歌上网助手插件。 这个插件只能访问谷歌旗下网站，YouTube等不能访问。 还有一个不错的谷歌浏览器插件VeePN，也能实现科学上网 机场篇 机场很多……随便推荐几个(不一定还能用) 免费机场 付费机场 付费的速度会比免费机场快，而且解锁奈非等 VPS篇 喜欢自己折腾的可以看，需要有VPS，以我的为例: ssh进行端口动态转发 # 本地1080端口和服务器动态转发，可添加参数-v打印一些数据流 $ ssh -D 1080 root@66.152.179.100 # 查看端口是否打开 $ netstat -nat ssh -D容易把VPS IP弄没……可以临时使用，长期使用换v2ray V2ray 在服务器端安装V2ray: 可以用一键脚本安装： bash &lt;(curl -s -L https://git.io/v2ray.sh) 也可以用Docker安装： 1.配置Docker环境： $ apt-get update $ apt-get -y install ca-certificates wget $ curl -sSL https://get.docker.com/ | sh 2.配置Portainer管理界面： $ docker run -d -p 9000:9000 --label owner=portainer \\ --restart=always --name=ui \\ --label owner=portainer \\ -v /var/run/docker.sock:/var/run/docker.sock \\ lihaixin/portainer -l owner=portainer 3.配置加速服务： $ wget https://github.com/chiakge/Linux-NetSpeed/raw/master/tcp.sh $ chmod +x tcp.sh $ ./tcp.sh 4.登入Web管理界面&lt;ip&gt;:9000 创建V2ray模板并启用 进入容器logs日志查看VMESS，复制下来，接下来会用 客户端使用V2ray： 下载对应客户端: Windows:·v2rayN Mac:v2rayU Linux:v2rayL 上面复制了vmess，然后点击v2ray客户端——&gt;从剪贴板批量导入URL 配置浏览器代理设置 在浏览器中安装Proxy SwitchyOmega扩展，新建PAC情景模式，在PAC网址中粘贴https://raw.githubusercontent.com/breakwa11/gfw_whitelist/master/proxy.pac 立即更新，应用更改。 或者新建情景模式——&gt;代理服务器——&gt;协议选择Socks5，代理服务器填127.0.0.1，代理端口为ssr或者V2ray的本地代理端口，一般默认为1080 浏览器切换到代理模式后，正常访问外网。 注意事项 谷歌上网助手和我的V2ray配置文件都选用了本地1080端口进行监听，在浏览器中同时开启Pro SwitchyOmega插件和谷歌上网助手插件可能会发生端口冲突。 解决方案:两个插件不要同时开，或者修改v2ray配置文件的本地端口。","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"},{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"},{"name":"科学上网","slug":"科学上网","permalink":"/tags/科学上网/"},{"name":"翻墙","slug":"翻墙","permalink":"/tags/翻墙/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用tensorboard和visdom","slug":"服务器使用tensorboard和visdom","date":"2020-01-28T03:27:21.351Z","updated":"2020-03-15T04:36:53.050Z","comments":true,"path":"2020/01/28/服务器使用tensorboard和visdom/","link":"","permalink":"/2020/01/28/服务器使用tensorboard和visdom/","excerpt":"","text":"服务器使用tensorboard和visdom 以tensorboard为例： 创建容器时开放6006端口 # 运行容器时将服务器docker容器的6006端口暴漏到自己主机ip下的16006端口(可自己指定) $ docker run -p &lt;ip&gt;:16006:6006 -it -v /data:/workspace/data --runtime=nvidia --net=host --name=temp /bin/bash 或者 # 在连接ssh时，将docker容器中的6006端口重新定向到自己机器上 $ ssh -p 1001 -L 16006:&lt;ip&gt;:6006 root@10.7.60.40","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"},{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用教程","slug":"服务器使用说明","date":"2020-01-24T03:38:00.000Z","updated":"2020-09-12T11:35:21.066Z","comments":true,"path":"2020/01/24/服务器使用说明/","link":"","permalink":"/2020/01/24/服务器使用说明/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Please enter the password to read the blog. Incorrect Password! No content to display! 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","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"docker基本使用","slug":"docker使用","date":"2020-01-19T12:48:00.000Z","updated":"2020-09-12T11:01:33.726Z","comments":true,"path":"2020/01/19/docker使用/","link":"","permalink":"/2020/01/19/docker使用/","excerpt":"","text":"Docker基本使用 docker和nvidia-docker 安装 略 安装后查看安装信息和测试docker是否安装正确 $ sudo docker --version $ sudo docker run hello-world # 验证nvidia-docker $ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi docker hub 链接:dockerhub 在dockerhub上搜索自己需要的镜像 选择自己要用的镜像，点进去，点Tags会出现不同的版本 复制命令，在终端中执行，就可以将镜像下载到本地 docker基本命令 # 查看主机下有多少镜像 $ sudo docker images # 删除镜像 $ sudo docker rmi &lt;image_id&gt; # 查看主机下的容器,不加-a表示查看运行中的容器 $ sudo docker ps -a # 启动、重启和停止容器 $ sudo docker start/restart/stop &lt;container_id&gt; # 进入容器 $ sudo docker attach &lt;container_id&gt; # 删除容器(需要先停止容器) $ sudo docker rm &lt;container_id&gt; ctrl+d退出并停止容器，ctrl+p+q退出但不停止容器 根据镜像新建并启动容器 $ sudo docker run -it [options] &lt;image_id&gt; bash 说明：-it为为容器分配一个输入终端，以交互式模式运行,bash为调用镜像里的bash 可选选项： --name 为容器指定名字 -p 端口映射 -v 给容器挂载存储卷 --net 指定容器网络 --runtime=nvidia 可调用gpu 例如： $ sudo docker run -it -p 8022:22 --name=tensorflow --runtime=nvidia -v /home/yu/code:/home --net=host ufoym/deepo bash 删除所有镜像和容器 $ sudo docker rmi `sudo docker images -q` $ sudo docker rm `sudo docker ps -a -q` 导出容器和保存加载镜像 容器导出为镜像 # 容器导出为镜像 $ docker commit &lt;continer_name&gt; &lt;image_name&gt; # 镜像导出为文件 $ docker save image_name&gt; /dir/name.tar # 文件导入为镜像 $ docker load &lt; /dir/filename docker和宿主机的文件拷贝 $ docker cp &lt;container_name&gt;:/dir/filename /hostdir/ $ docker cp /hostdir/filename &lt;container_name&gt;:/dir 不管容器有没有启动，拷贝命令都会生效","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"docker","slug":"docker","permalink":"/tags/docker/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"OpenPAI安装记录","slug":"OpenPai安装记录","date":"2020-01-16T02:48:00.000Z","updated":"2020-03-15T04:33:53.506Z","comments":true,"path":"2020/01/16/OpenPai安装记录/","link":"","permalink":"/2020/01/16/OpenPai安装记录/","excerpt":"","text":"OpenPAI安装记录 环境准备 一台master主机和多台worker主机，一台维护机 所有节点不要安装CUDA驱动，具有统一的登录账户和密码 开启ssh功能和ntp功能(互相访问，时间同步) 部署过程 安装docker-ce $ sudo apt-get -y install docker.io $ sudo docker pull docker.io/openpai/dev-box:v0.14.0 运行dev-box $ sudo docker run -itd \\ -e COLUMNS=$COLUMNS -e LINES=$LINES -e TERM=$TERM \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /pathConfiguration:/cluster-configuration \\ -v /hadoop-binary:/hadoop-binary \\ --pid=host \\ --privileged=true \\ --net=host \\ --name=dev-box \\ docker.io/openpai/dev-box:v0.14.0 登录dev-box $ sudo docker exec -it dev-box /bin/bash $ cd /pai/deployment/quick-start/ 修改配置信息 $ cp quick-start-example.yaml quick-start.yaml $ vim quick-start.yaml 修改内容： machines: - &lt;ip-of-master&gt; - &lt;ip-of-worker1&gt; - &lt;ip-of-worder2&gt; ssh-username: &lt;username&gt; ssh-password: &lt;password&gt; 生成OepnPai配置文件 $ cd /pai $ python paictl.py config generate -i /pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f $ cd ~/pai-config/ 修改kubernetes-configuration.yaml 将docker-registry替换为国内镜像库 docker-registry: docker.io/mirrorgooglecontainers 修改layout.yaml 修改自己机器的配置信息 machine-sku: GENERIC: mem: 256G gpu: type: TITAN V count: 1 cpu: vcore: 4 os: ubuntu16.04 Worker1: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 Worker2: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 修改services-configuration.yaml 解除common和data-path两个字段的注释，将data-path赋值到真实位置，作为服务数据存储路径 cluster: common: # cluster-id: pai-example # # # HDFS, zookeeper data path on your cluster machine. data-path: \"/data\" tag字段修改为真实版本 v014.0 可修改cluster-id,后面会用到 修改rest-server下的用户名和密码，作为登录平台的账户密码 指定显卡驱动版本，不指定的话默认安装384.11，这个驱动是不支持图灵核心显卡的，安装到后面会出现’nvidia-drm’ not found 错误，驱动版本只能从注释里的版本选择 drivers: set-nvidia-runtime: false # You can set drivers version here. If this value is miss, default value will be 384.111 # Current supported version list # 384.111 # 390.25 # 410.73 version: \"410.73\" 部署Kubernetes http::9090查看进度 $ cd /pai $ python paictl.py cluster k8s-bootup -p ~/pai-config 更改配置文件到kubernetes $ cd /pai python paictl.py config push -p ~/pai-config/ -c ~/.kube/config 若报错，卸载openpai组件和ks组件，检查之前的配置文件，重新安装 $ python paictl.py service [delete|start|stop] -c ~/.kube/config [-n name] # 卸载openpai组件 $ python paictl.py service delete -c ~/.kube/config # 卸载k8s组件 $ python paictl.py cluster k8s-clean -p ~/pai-config/ 启动Openpai $ python paictl.py service start -c ~/.kube/config 界面 http://&lt;master-ip&gt;:9090 http://&lt;master-ip&gt;:80 Reference https://github.com/kangapp/openPAI https://github.com/microsoft/pai https://zhuanlan.zhihu.com/p/64061072","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"openpai","slug":"openpai","permalink":"/tags/openpai/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"NVIDIA,CUDA,CUDNN,Anaconda","slug":"Ubuntu命令记录","date":"2020-01-11T12:32:00.000Z","updated":"2020-03-15T04:35:55.578Z","comments":true,"path":"2020/01/11/Ubuntu命令记录/","link":"","permalink":"/2020/01/11/Ubuntu命令记录/","excerpt":"","text":"NVIDIA,CUDA,CUDNN ppa安装NVIDIA驱动 $ sudo add-apt-repository ppa:graphics-drivers/ppa $ sudo apt-get update $ ubuntu-drivers devices $ sudo apt-get install nvidia-driver-xxx 自动安装NVIDIA驱动 # 卸载残余驱动 sudo apt-get --purge remove \"*nvidia*\" # 查看推荐驱动版本 ubuntu-drivers devices # 自动安装 sudo ubuntu-drivers autoinstall .deb安装CUDA 下载deb文件 $ sudo dpkg -i cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb $ sudo apt-get update $ sudo apt-get install cuda 安装CUDNN 下载符合自己cuda版本的cudnn 安装cudnn 安装过程实际是将cudnn的头文件复制到CUDA的头文件目录里 $ sudo cp cuda/include/* /usr/local/cuda-10.0/include/ $ sudo cp cuda/lib64/* /usr/local/cuda-10.0/lib64/ # 添加可执行权限 $ sudo chmod +x /usr/local/cuda-10.0/include/cudnn.h $ sudo chmod +x /usr/local/cuda-10.0/lib64/libcudnn* 检验 $ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 指定运行程序使用的GPU 在程序中添加 import os os.environ['CUDA_VISIBLE_DEVICES]='0' 或者在终端中 $ CUDA_VISIBLE_DEVICES=0 python main.py 命令行安装Anaconda $ wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh $ bash Anaconda3-5.0.1-Linux-x86_64.sh # 添加环境变量，可选 $ echo 'export PATH=\"~/anaconda3/bin:$PATH\"' &gt;&gt; ~/.bashrc $ source .bashrc","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"nvidia","slug":"nvidia","permalink":"/tags/nvidia/"},{"name":"cuda","slug":"cuda","permalink":"/tags/cuda/"},{"name":"cudnn","slug":"cudnn","permalink":"/tags/cudnn/"},{"name":"anaconda","slug":"anaconda","permalink":"/tags/anaconda/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"安利","slug":"安利区","date":"2020-01-11T02:48:00.000Z","updated":"2020-03-15T04:36:19.214Z","comments":true,"path":"2020/01/11/安利区/","link":"","permalink":"/2020/01/11/安利区/","excerpt":"","text":"一些软件 softdownloader: 一款布局清爽的下载工具 Fences: 桌面管理工具 copytranslator: 一款翻译软件 天若OCR文字识别 Snipaste: 一款简洁的截图和贴图软件 Mathpix: 每个月50次识别次数，快速识别图片中的公式，转化为 LaTeX\\LaTeXLATE​X 格式 Windows 的内置 Liunx 系统，支持 Linux 命令 TexStudio + Texlive 编辑LaTeX\\LaTeXLATE​X公式 Axmath: 个人感觉优于mathtype, Office中的公式编辑器 亿寻:百度云破解限速 Google浏览器插件 momentum: 浏览器壁纸标签页 LastPass: 密码记录插件 SwitchyOmega: 浏览器代理插件 OneTab: 标签页管理 一键管理扩展: 管理所有插件 AdblockPlus: 强烈安利，拦截广告 Read Viewer: 网页阅读模式 划词翻译 Tampermonkey:传说中的油猴 又在Github上找到个操作系统软件集合: Windows linux Mac","categories":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}],"tags":[{"name":"安利","slug":"安利","permalink":"/tags/安利/"}],"keywords":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-01-11T02:16:02.000Z","updated":"2020-03-15T04:19:40.002Z","comments":true,"path":"2020/01/11/hello-world/","link":"","permalink":"/2020/01/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]}]}