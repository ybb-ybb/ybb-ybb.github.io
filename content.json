{"meta":{"title":"鱼摆摆的blog","subtitle":null,"description":"自童年起，我就独自一人，照顾着历代星辰","author":"鱼摆摆","url":""},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"client/index.html","permalink":"/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"comment/index.html","permalink":"/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"donate/index.html","permalink":"/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"lab/index.html","permalink":"/lab/index.html","excerpt":"","text":"sakura主题 balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-03-09T12:29:18.823Z","comments":true,"path":"links/index.html","permalink":"/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2020-01-11T15:14:28.000Z","updated":"2020-03-09T12:35:33.243Z","comments":false,"path":"music/index.html","permalink":"/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-02-14T09:11:23.000Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-02-14T09:11:23.000Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"python序列方法","slug":"python序列方法","date":"2020-08-17T13:49:00.000Z","updated":"2020-08-17T13:51:04.318Z","comments":true,"path":"2020/08/17/python序列方法/","link":"","permalink":"/2020/08/17/python序列方法/","excerpt":"","text":"注：有些方法在 python2 中不存在 列表方法 通过列表推导式创建列表 &gt;&gt;&gt; x=[i**2 for i in range(3)] &gt;&gt;&gt; x [0, 1, 4] append : 将一个对象附加到列表末尾 clear ：就地清空列表内容 copy ： 复制列表 count ： 计算指定元素在列表中出现了多少次 extend ： 使用列表扩展另一个列表 index ： 查找指定值第一次出现的索引 insert ： 讲一个对象插入列表 pop ： 从列表删除一个元素(默认最后一个元素)，并返回这一元素 remove ： 删除第一个为指定值的元素 reverse ： 按相反的顺序排列列表中的元素 sort ： 对列表就地排序，接受两个可选参数key和values。 &gt;&gt;&gt; lst = [1, 2, 3] &gt;&gt;&gt; lst.append(4) &gt;&gt;&gt; lst [1, 2, 3, 4] &gt;&gt;&gt; lst.clear() &gt;&gt;&gt; lst [] &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b=a.copy() &gt;&gt;&gt; b [1, 2, 3] &gt;&gt;&gt; a.count(1) 1 &gt;&gt;&gt; a.extend(b) &gt;&gt;&gt; a [1, 2, 3, 1, 2, 3] &gt;&gt;&gt; a.index(2) 1 &gt;&gt;&gt; a.insert(1,4) &gt;&gt;&gt; a [1, 4, 2, 3, 1, 2, 3] &gt;&gt;&gt; a.pop() 3 &gt;&gt;&gt; a [1, 4, 2, 3, 1, 2] &gt;&gt;&gt; a.pop(0) 1 &gt;&gt;&gt; a [4, 2, 3, 1, 2] &gt;&gt;&gt; a.remove(1) &gt;&gt;&gt; a [4, 2, 3, 2] &gt;&gt;&gt; a.reverse() &gt;&gt;&gt; a [2, 3, 2, 4] &gt;&gt;&gt; a.sort() &gt;&gt;&gt; a [2, 2, 3, 4] &gt;&gt;&gt; a.sort(key=lambda x : x % 3,reverse=True) &gt;&gt;&gt; a [2, 2, 4, 3] 字符串方法 center ： 通过在字符串两边添加填充字符使字符串居中 find ：在字符串中查找子串，找到则返回子串第一个字符的索引，否则返回 -1(可指定起点值和终点值) join ： 合并序列的元素，与 split 相反 lower ： 返回字符串的小写版本 replace ： 将指定子串都替换为另一个字符串，并返回替换后的结果 split ： 与 join 相反，将字符串拆分为序列 strip ： 将字符串开头和末尾的指定字符删除 isspace ： 是否是空格 isdight ： 是否是数字 isupper : 是否是大写字母 islower ： 是否是小写字母 &gt;&gt;&gt; x=\"Hello World!\" &gt;&gt;&gt; x.center(15) ' Hello World! ' &gt;&gt;&gt; x.center(15,'*') '**Hello World!*' &gt;&gt;&gt; x.find('llo') 2 &gt;&gt;&gt; x.find('llo',3) -1 &gt;&gt;&gt; seq = ['1','2','3'] &gt;&gt;&gt; '+'.join(seq) '1+2+3' &gt;&gt;&gt; x.lower() 'hello world!' &gt;&gt;&gt; x.replace('hello','Hello') 'Hello World!' &gt;&gt;&gt; x.split() ['Hello', 'World!'] &gt;&gt;&gt; x = x.center(15,'*') &gt;&gt;&gt; x '**Hello World!*' &gt;&gt;&gt; x.strip('*') 'Hello World!' &gt;&gt;&gt; x '**Hello World!*' 字典 创建字典：通过键——值对序列或者字典推导式创建 &gt;&gt;&gt; items = [('name','Gumby'),('age',42)] &gt;&gt;&gt; d=dict(items) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; x={i:i ** 2 for i in range(3)} &gt;&gt;&gt; x {0: 0, 1: 1, 2: 4} **clear ** ：删除所有字典项 copy ： 返回一个新字典，包含的键值对与原字典相同 fromkeys ：创建一个新字典，包含指定的键，且每个键对应的值都是None，可指定对应的默认值 get ：访问不存在的建时返回None，可指定返回值 items ： 返回一个包含所有字典项的列表，其中每个元素都是（key,value）的形式 pop ： 获取与指定键相关联的值，并将该键值对从字典中删除 popitem ： 随机弹出一个字典项 setdefault ： 类似get ，当字典中不包含指定的键时，在字典中添加指定的键值对 update ： 使用一个字典中的项来更新另一个字典 keys ： 返回一个字典视图，其中包含字典中的键 values ： 返回一个字典视图，其中包含字典中的值 &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d.clear() &gt;&gt;&gt; d {} &gt;&gt;&gt; d=dict(name='Gumby',age=42) &gt;&gt;&gt; x=d.copy() &gt;&gt;&gt; x {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; {}.fromkeys(['name','age']) {'name': None, 'age': None} &gt;&gt;&gt; {}.fromkeys(['name','age'],False) {'name': False, 'age': False} &gt;&gt;&gt; x.get('score') &gt;&gt;&gt; x.items() dict_items([('name', 'Gumby'), ('age', 42)]) IndentationError: expected an indented block &gt;&gt;&gt; for key,value in x.items(): ... print(value) Gumby 42 &gt;&gt;&gt; x {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; x.pop('name') 'Gumby' &gt;&gt;&gt; x.popitem() ('age', 42) &gt;&gt;&gt; x {} &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42} &gt;&gt;&gt; d.setdefault('name','N/A') 'Gumby' &gt;&gt;&gt; d.setdefault('score','N/A') 'N/A' &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42, 'score': 'N/A'} &gt;&gt;&gt; x={'score':88} &gt;&gt;&gt; d.update(x) &gt;&gt;&gt; d {'name': 'Gumby', 'age': 42, 'score': 88} &gt;&gt;&gt; x.keys() dict_keys(['name', 'age']) &gt;&gt;&gt; x.values() dict_values(['Gumby', 42])","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"十大排序算法","slug":"排序算法","date":"2020-06-27T01:44:00.000Z","updated":"2020-06-27T02:10:11.063Z","comments":true,"path":"2020/06/27/排序算法/","link":"","permalink":"/2020/06/27/排序算法/","excerpt":"","text":"算法 算法部分的介绍在十大经典排序算法动画与解析，看我就够了！ 关于时间复杂度： 平方阶 (O(n2))(O(n^2))(O(n2)) 排序 各类简单排序：插入排序、选择排序和冒泡排序。 线性对数阶$ (O(nlog2n))$ 排序：快速排序、堆排序和归并排序； O(n+§))O(n+§))O(n+§)) 排序，§§§ 是介于 0 和 1 之间的常数。 希尔排序 线性阶 (O(n)) 排序 基数排序，此外还有桶、箱排序 关于稳定性： 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 python实现 冒泡排序 def bubblesort(arr): n=len(arr) for i in range(n): for j in range(n-i-1): if arr[j] &gt; arr[j+1]: arr[j],arr[j+1]=arr[j+1],arr[j] return arr if __name__ == '__main__': arr = [5,4,2,3,8] bubblesort(arr) print(arr) 选择排序 def selectionsort(arr): n = len(arr) for i in range(n): min =i for j in range(i, n): if arr[j] &lt; arr[i]: min =j arr[min], arr[i] = arr[i], arr[min] return arr if __name__ == '__main__': arr=[5,4,2,3,8] selectionsort(arr) print(arr) 插入排序 def insertionSort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j &gt;= 0 and key &lt; arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key if __name__ == '__main__': arr = [5,4,2,3,8] insertionSort(arr) print(arr) 希尔排序 def shellsort(arr): n = len(arr) gap = n while gap &gt; 0: gap = gap // 2 for i in range(gap, n): key = arr[i] j = i - gap while j &gt;= 0 and key &lt; arr[j]: arr[j + gap] = arr[j] j -= gap arr[j + gap] = key return arr if __name__ == '__main__': arr = [5, 4, 2, 3, 8] shellsort(arr) print(arr) 归并排序 def sorttwoarray(arr_left, arr_right): m, n, i, j = len(arr_left), len(arr_right), 0, 0 arr = [] while i &lt; m and j &lt; n: if arr_left[i] &lt; arr_right[j]: arr.append(arr_left[i]) i += 1 else: arr.append(arr_right[j]) j += 1 arr.extend(arr_left[i:m]) arr.extend(arr_right[j:n]) return arr def mergesort(arr): n = len(arr) arr_left = arr[:(n + 1) // 2] arr_right = arr[(n + 1) // 2:] if len(arr_left) == 1: return sorttwoarray(arr_left, arr_right) else: return sorttwoarray(mergesort(arr_left), mergesort(arr_right)) if __name__ == '__main__': arr = [6, 4, 3, 7, 5, 1, 2] array = mergesort(arr) print(array) 快速排序 # -*- coding: UTF-8 -*- # 《算法导论》中的快速排序 def quick_sort(array, l, r): if l &lt; r: q = partition(array, l, r) quick_sort(array, l, q - 1) quick_sort(array, q + 1, r) def partition(array, l, r): x = array[r] i = l - 1 for j in range(l, r): if array[j] &lt;= x: i += 1 array[i], array[j] = array[j], array[i] array[i + 1], array[r] = array[r], array[i + 1] return i + 1 if __name__ == '__main__': arr = [6, 4, 3, 7, 5, 1, 2] quick_sort(arr,0,6) print(arr) 堆排序 # -*- coding: UTF-8 -*- def heapify(arr, n, i): largest = i l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 if l &lt; n and arr[i] &lt; arr[l]: largest = l if r &lt; n and arr[largest] &lt; arr[r]: largest = r if largest != i: arr[i], arr[largest] = arr[largest], arr[i] # 交换 heapify(arr, n, largest) def heapSort(arr): n = len(arr) # Build a maxheap. for i in range(n, -1, -1): heapify(arr, n, i) # 一个个交换元素 for i in range(n - 1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # 交换 heapify(arr, i, 0) if __name__ == '__main__': arr = [5, 2, 7, 3, 6, 1, 4] heapSort(arr) print (arr), 计数排序 def countingsort(arr): n = len(arr) min_arr = min(arr) max_arr = max(arr) length = max_arr - min_arr + 1 newarr = [0] * length sort_arr = [] for i in arr: newarr[i - min_arr] += 1 for i in range(length): sort_arr += [i + min_arr] * newarr[i] return sort_arr if __name__ == '__main__': arr = [5, 3, 4, 7, 2, 4, 3, 4, 7] sort_arr = countingsort(arr) print(sort_arr) 桶排序 def bucketsort(arr, num_bucket): min_arr = min(arr) max_arr = max(arr) buckets = [[] for i in range(num_bucket)] size = (max_arr - min_arr + 1) / num_bucket sorted_arr = [] for i in arr: index = (i - min_arr) // size buckets[index].append(i) for i in buckets: sorted_arr += (sorted(i)) return sorted_arr if __name__ == '__main__': arr = [7, 12, 56, 23, 19, 33, 35, 42, 42, 2, 8, 22, 39, 26, 17] sort_arr = bucketsort(arr, 5) print(sort_arr) 基数排序 def radixsort(arr): d = 1 max_arr = max(arr) while max_arr / 10 != 0: d += 1 max_arr /= 10 for i in range(d): s = [[] for k in range(10)] for j in arr: s[int(j / (10 ** i)) % 10].append(j) sorted_arr = [a for b in s for a in b] return sorted_arr if __name__ == '__main__': arr = [321, 1, 10, 30, 277, 753, 127] sorted_arr = radixsort(arr) print (sorted_arr)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"排序","slug":"排序","permalink":"/tags/排序/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"终端使用socks5代理","slug":"终端socks5","date":"2020-06-26T10:32:00.000Z","updated":"2020-07-13T05:52:42.000Z","comments":true,"path":"2020/06/26/终端socks5/","link":"","permalink":"/2020/06/26/终端socks5/","excerpt":"","text":"proxychains安装 # git仓库中编译安装 $ git clone https://github.com/rofl0r/proxychains-ng.git $ cd proxychains-ng $ ./configure $ make &amp;&amp; make install $ cp ./src/proxychains.conf /etc/proxychains.conf $ cd .. &amp;&amp; rm -rf proxychains-ng # 或直接安装 $ brew install proxychains-ng 编辑proxychains配置 $ vim /etc/proxychains.conf 将sock4 127.0.0.1 9095 改为socks5 12.0.0.1 1080 (配置同ssr或v2ray客户端一致) 使用方法 在需要代理的命令前加上 proxychains4 ，如 $ proxychains4 wget http://xxx.com/xxx.zip","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"科学上网","slug":"科学上网","permalink":"/tags/科学上网/"},{"name":"代理","slug":"代理","permalink":"/tags/代理/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"screen","slug":"screen","date":"2020-05-28T10:49:00.000Z","updated":"2020-06-26T06:24:41.865Z","comments":true,"path":"2020/05/28/screen/","link":"","permalink":"/2020/05/28/screen/","excerpt":"","text":"安装 $ sudo apt-get install screen 使用 # 创建一个名为screen-name的会话 $ screen -S screen-name # 离开会话: ctrl+a+d # 恢复创建的会话 $ screen -r screen-name # 或，如果只有一个会话 $ screen -r # 查看已经创建的会话 $ screen -ls # 退出会话 $ exit 其它命令 Ctrl + a，d #暂离当前会话 Ctrl + a，c #在当前screen会话中创建一个子会话 Ctrl + a，w #子会话列表 Ctrl + a，p #上一个子会话 Ctrl + a，n #下一个子会话 Ctrl + a，0-9 #在第0窗口至第9子会话间切换 鼠标回滚 screen模式下，无法在终端中使用鼠标滚轴进行翻页，解决： ctrl + a +[ 进入回滚模式 ctrl + c 切换回之前模式","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"screen","slug":"screen","permalink":"/tags/screen/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"感知机","slug":"感知机","date":"2020-05-10T02:02:00.000Z","updated":"2020-06-26T06:41:58.121Z","comments":true,"path":"2020/05/10/感知机/","link":"","permalink":"/2020/05/10/感知机/","excerpt":"","text":"1．感知机是根据输入实例的特征向量 xxx 对其进行二类分类的线性分类模型： f(x)=(w⋅x+b)f(x)=(w \\cdot x + b) f(x)=(w⋅x+b) 感知机模型对应于输入空间（特征空间）中的分离超平面 w⋅x+b=0w \\cdot x+b=0w⋅x+b=0 2．感知机学习的策略是极小化损失函数： min⁡w,bL(w,b)=−∑xi∈Myi(w⋅xi+b)\\min _{w, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right) w,bmin​L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 对应于误分类点到分离超平面的总距离。 原始形式： 误分类点集合MMM ，损失函数L(w,b)L(w,b)L(w,b) 的梯度由 ∇wL(w,b)=−∑xi∈Myixi∇bL(w,b)=−∑xi∈Myi\\begin{array}{l} \\nabla_{w} L(w, b)=-\\sum_{x_{i} \\in M} y_{i} x_{i} \\\\ \\nabla_{b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i} \\end{array} ∇w​L(w,b)=−∑xi​∈M​yi​xi​∇b​L(w,b)=−∑xi​∈M​yi​​ 给出 随机选取一个误分类点(xi,yi)(x_i,y_i)(xi​,yi​) ，对w,bw,bw,b 进行更新（η\\etaη 为学习率）： w←w+ηyixib←b+ηyi\\begin{array}{c} w \\leftarrow w+\\eta y_{i} x_{i} \\\\ b \\leftarrow b+\\eta y_{i} \\end{array} w←w+ηyi​xi​b←b+ηyi​​ 收敛性： 设训练数据集T={(x1,y1),(x2,y2),⋯ ,(xN,yN)}T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}T={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)} 是线性可分的，其中xi∈X=Rn,yi∈Y={−1,+1},i=1,2,⋯ ,Nx_{i} \\in \\mathcal{X}=\\mathbf{R}^{n}, \\quad y_{i} \\in \\mathcal{Y}=\\{-1,+1\\}, \\quad i=1,2, \\cdots, Nxi​∈X=Rn,yi​∈Y={−1,+1},i=1,2,⋯,N。则： (1) 存在满足条件∥w^opt∥=1\\|\\hat w _{opt}\\|=1∥w^opt​∥=1的超平面w^opt⋅x^=wopt⋅x+bopt=0\\hat{w}_{opt} \\cdot \\hat{x}=w_{opt} \\cdot x+b_{opt}=0w^opt​⋅x^=wopt​⋅x+bopt​=0将训练数据集完全正确分开；且存在γ&gt;0\\gamma &gt; 0γ&gt;0，对所有i=1,2,…,Ni=1,2,…,Ni=1,2,…,N yi(w^opt⋅x^i)=yi(wopt⋅xi+bopt)≥γy_{i}\\left(\\hat{w}_{opt} \\cdot \\hat{x}_{i}\\right)=y_{i}\\left(w_{opt} \\cdot x_{i}+b_{opt}\\right) \\ge \\gamma yi​(w^opt​⋅x^i​)=yi​(wopt​⋅xi​+bopt​)≥γ (2) 令R=max⁡1⩽i⩽N∥x^i∥R=\\max _{1 \\leqslant i\\leqslant N }\\left\\|\\hat{x}_{i}\\right\\|R=max1⩽i⩽N​∥x^i​∥ ,则原始感知机算法在训练数据集上的误分类次数kkk 满足： k⩽(Rγ)2k \\leqslant\\left(\\frac{R}{\\gamma}\\right)^{2} k⩽(γR​)2 对偶形式 最后学习到的w,bw,bw,b 可以表示为 w=∑i=1Nαiyixib=∑i=1Nαiyi\\begin{array}{l} w=\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i} \\\\ b=\\sum_{i=1}^{N} \\alpha_{i} y_{i} \\end{array} w=∑i=1N​αi​yi​xi​b=∑i=1N​αi​yi​​ 如果yi(∑j=1Nαjyjxj⋅xi+b)⩽0y_{i}\\left(\\sum_{j=1}^{N} \\alpha_{j} y_{j} x_{j} \\cdot x_{i}+b\\right) \\leqslant 0yi​(∑j=1N​αj​yj​xj​⋅xi​+b)⩽0 αi←αi+ηb←b+ηyi\\begin{array}{l} \\alpha_{i} \\leftarrow \\alpha_{i}+\\eta \\\\ b \\leftarrow b+\\eta y_{i} \\end{array} αi​←αi​+ηb←b+ηyi​​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"感知机","slug":"感知机","permalink":"/tags/感知机/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"统计学习","slug":"统计学习","date":"2020-05-01T07:13:00.000Z","updated":"2020-06-26T06:44:23.473Z","comments":true,"path":"2020/05/01/统计学习/","link":"","permalink":"/2020/05/01/统计学习/","excerpt":"","text":"基本概念 假设空间可定义为条件概率的集合 F={P∣P(Y∣X)}\\mathcal{F}=\\{P|P(Y|X)\\} F={P∣P(Y∣X)} 通常是由一个参数向量决定的条件概率分布族 F={P∣Pθ(Y∣X),θ∈Rn}\\mathcal{F}=\\left\\{P\\left|P_{\\theta}(Y | X), \\theta \\in \\mathbf{R}^{n}\\right\\}\\right. F={P∣Pθ​(Y∣X),θ∈Rn} 风险函数(risk function)/期望损失(expected loss): Rexp⁡(f)=EP[L(Y,f(X))]=∫x×yL(y,f(x))P(x,y)dxdyR_{\\exp }(f)=E_{P}[L(Y, f(X))]=\\int_{x \\times y} L(y, f(x)) P(x, y) \\mathrm{d} x \\mathrm{d} y Rexp​(f)=EP​[L(Y,f(X))]=∫x×y​L(y,f(x))P(x,y)dxdy 学习的目标是选择期望风险最小的模型，由于联合分布P(X,Y)P(X,Y)P(X,Y) 未知，Rexp⁡(f)R_{\\exp}(f)Rexp​(f) 不能直接计算。所以监督学习是一个病态问题(ill-formed problem)。 经验风险： Remp(f)=1N∑i=1NL(yi,f(xi))R_{\\mathrm{emp}}(f)=\\frac{1}{N} \\sum_{i=1}^{N} L\\left(y_{i}, f\\left(x_{i}\\right)\\right) Remp​(f)=N1​i=1∑N​L(yi​,f(xi​)) 正则化:防止过拟合 min⁡f∈F1N∑t=1NL(yi,f(xi))+λJ(f)\\min _{f \\in \\mathcal{F}} \\frac{1}{N} \\sum_{t=1}^{N} L\\left(y_{i}, f\\left(x_{i}\\right)\\right)+\\lambda J(f) f∈Fmin​N1​t=1∑N​L(yi​,f(xi​))+λJ(f) 交叉验证：一般选用S折交叉验证。 首先随机地将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型. 泛化误差 利用学习到的模型f^\\hat{f}f^​ ，对未知数据预测的误差: Rexp⁡(f^)=EP[L(Y,f^(X))]=∫X×YL(y,f^(x))P(x,y)dxdyR_{\\exp }(\\hat{f})=E_{P}[L(Y, \\hat{f}(X))]=\\int_{\\mathcal{X} \\times \\mathcal{Y}} L(y, \\hat{f}(x)) P(x, y) \\mathrm{d} x \\mathrm{d} y Rexp​(f^​)=EP​[L(Y,f^​(X))]=∫X×Y​L(y,f^​(x))P(x,y)dxdy 泛化误差上界通常具有以下性质：它是样本容量的函数，当样本容量增加时，泛化上界趋于0；它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。 **定理：**对于二分类问题，关于fff 的期望风险和经验风险分别是 R(f)=E[L(Y,f(X))]R(f)=E[L(Y, f(X))] R(f)=E[L(Y,f(X))] R^(f)=1N∑i=1NL(yi,f(xi))\\hat{R}(f)=\\frac{1}{N} \\sum_{i=1}^{N} L\\left(y_{i}, f\\left(x_{i}\\right)\\right) R^(f)=N1​i=1∑N​L(yi​,f(xi​)) 当假设空间是有限个函数的集合F={f1,f2,⋯ ,fd}\\mathcal{F}=\\{f_1,f_2,\\cdots,f_d\\}F={f1​,f2​,⋯,fd​} 时,对任意一个函数f∈Ff \\in \\mathcal{F}f∈F ，至少以概率1−δ1-\\delta1−δ ,以下不等式成立： R(f)⩽R^(f)+ε(d,N,δ)R(f) \\leqslant \\hat{R}(f)+\\varepsilon(d, N, \\delta) R(f)⩽R^(f)+ε(d,N,δ) 其中 ε(d,N,δ)=12N(log⁡d+log⁡1δ)\\varepsilon(d, N, \\delta)=\\sqrt{\\frac{1}{2 N}\\left(\\log d+\\log \\frac{1}{\\delta}\\right)} ε(d,N,δ)=2N1​(logd+logδ1​)​ 不等式左端R(f)R(f)R(f)是泛化误差，右端即为泛化误差上界。在泛化误差上界中，第一项是训练误差，训练误差越小，泛化误差也越小，第2项是NNN的单调递减函数，log⁡d\\log dlogd的单调递增函数，假设空间F包含的函数越多，其值越大。 证明： 证明中需要用到 Hoeffding 不等式： 设Sn=∑i=1nXiS_n=\\sum_{i=1}^{n}X_iSn​=∑i=1n​Xi​ 是独立随机变量X1,X2,⋯ ,XnX_1,X_2,\\cdots,X_nX1​,X2​,⋯,Xn​ 之和，Xi∈[ai,bi]X_i \\in [a_i,b_i]Xi​∈[ai​,bi​] ，则对任意t&gt;0t&gt;0t&gt;0 ,以下不等式成立： P(Sn−ESn⩾t)⩽exp⁡(−2t2∑i=1n(bi−ai)2)P\\left(S_{n}-E S_{n} \\geqslant t\\right) \\leqslant \\exp \\left(\\frac{-2 t^{2}}{\\sum_{i=1}^{n}\\left(b_{i}-a_{i}\\right)^{2}}\\right) P(Sn​−ESn​⩾t)⩽exp(∑i=1n​(bi​−ai​)2−2t2​) P(ESn−Sn⩾t)⩽exp⁡(−2t2∑i=1n(bi−ai)2)P\\left(E S_{n}-S_{n} \\geqslant t\\right) \\leqslant \\exp \\left(\\frac{-2 t^{2}}{\\sum_{i=1}^{n}\\left(b_{i}-a_{i}\\right)^{2}}\\right) P(ESn​−Sn​⩾t)⩽exp(∑i=1n​(bi​−ai​)2−2t2​) 证明：对任意函数f∈Ff\\in \\mathcal{F}f∈F， R^(f)\\hat{R}(f)R^(f) 是N个独立随机变量L(Y,f(X))L(Y,f(X))L(Y,f(X)) 的样本取值，R(f)R(f)R(f) 是随机变量L(Y,f(X))L(Y,f(X))L(Y,f(X)) 的期望值。如果损失函数取值于区间[0,1][0,1][0,1] ，即对所有iii ,[ai,bi]=[0,1][a_i,b_i]=[0,1][ai​,bi​]=[0,1]， 那么由Hoeffding不等式：对ε&gt;0\\varepsilon &gt; 0ε&gt;0，以下不等式成立： P(R(f)−R^(f)⩾ε)⩽exp⁡(−2Nε2)P(R(f)-\\hat{R}(f) \\geqslant \\varepsilon) \\leqslant \\exp \\left(-2 N \\varepsilon^{2}\\right) P(R(f)−R^(f)⩾ε)⩽exp(−2Nε2) F={f1,f2,⋯ ,fd}\\mathcal{F}=\\left\\{f_{1}, f_{2}, \\cdots, f_{d}\\right\\}F={f1​,f2​,⋯,fd​} 是一个有限集合： P(∃f∈F:R(f)−R^(f)⩾ε)=P(⋃G′F{R(f)−R^(f)⩾ε})⩽∑f∈FP(R(f)−R^(f)⩾ε)⩽dexp⁡(−2Nε2)\\begin{aligned} P(\\exists f \\in \\mathcal{F}: R(f)-\\hat{R}(f) \\geqslant \\varepsilon) &amp;=P\\left(\\bigcup_{\\mathscr{G}^{\\prime} F}\\{R(f)-\\hat{R}(f) \\geqslant \\varepsilon\\}\\right) \\\\ &amp; \\leqslant \\sum_{f \\in \\mathcal{F}} P(R(f)-\\hat{R}(f) \\geqslant \\varepsilon) \\\\ &amp; \\leqslant d \\exp \\left(-2 N \\varepsilon^{2}\\right) \\end{aligned} P(∃f∈F:R(f)−R^(f)⩾ε)​=P(G′F⋃​{R(f)−R^(f)⩾ε})⩽f∈F∑​P(R(f)−R^(f)⩾ε)⩽dexp(−2Nε2)​ 即 P(R(f)−R^(f)&lt;ε)⩾1−dexp⁡(−2Nε2)P(R(f)-\\hat{R}(f)&lt;\\varepsilon) \\geqslant 1-d \\exp \\left(-2 N \\varepsilon^{2}\\right) P(R(f)−R^(f)&lt;ε)⩾1−dexp(−2Nε2) 令δ=dexp⁡(−2Nε2)\\delta=d \\exp \\left(-2 N \\varepsilon^{2}\\right)δ=dexp(−2Nε2) P(R(f)&lt;R^(f)+ε)⩾1−δP(R(f)&lt;\\hat{R}(f)+\\varepsilon) \\geqslant 1-\\delta P(R(f)&lt;R^(f)+ε)⩾1−δ 二分类问题： 许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络，Winnow等。 TP——将正类预测为正类数； FN——将正类预测为负类数； FP——将负类预测为正类数； TN——将负类预测为负类数. 精确率： P=TPTP+FPP=\\frac{TP}{TP+FP} P=TP+FPTP​ 召回率： R=TPTP+FNR=\\frac{TP}{TP+FN} R=TP+FNTP​ F1F_1F1​ 值： 2F1=1P+1R\\frac{2}{F_{1}}=\\frac{1}{P}+\\frac{1}{R} F1​2​=P1​+R1​ F1=2TP2TP+FP+FNF_{1}=\\frac{2 T P}{2 T P+F P+F N} F1​=2TP+FP+FN2TP​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"/tags/统计学习/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"Numpy","slug":"numpy基础用法总结","date":"2020-04-24T03:38:00.000Z","updated":"2020-06-26T06:37:19.505Z","comments":true,"path":"2020/04/24/numpy基础用法总结/","link":"","permalink":"/2020/04/24/numpy基础用法总结/","excerpt":"","text":"基础篇 创建数组： # 通过list创建 &gt;&gt;&gt;np.array([[1, 2], [3, 4]]) array([[1, 2], [3, 4]]) # 通过arange创建 &gt;&gt;&gt;np.arange(0,1,0.1) array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) # arange+广播 &gt;&gt;&gt;np.arange(1,60,10).reshape(-1,1)+np.arange(0,6) array([[ 1, 2, 3, 4, 5, 6], [11, 12, 13, 14, 15, 16], [21, 22, 23, 24, 25, 26], [31, 32, 33, 34, 35, 36], [41, 42, 43, 44, 45, 46], [51, 52, 53, 54, 55, 56]]) # linspace通过等差数列创建数组 &gt;&gt;&gt;np.linspace(0,1,10) array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ]) # 特殊形式数组 &gt;&gt;&gt;np.zeros((2,3),np.int) array([[0, 0, 0], [0, 0, 0]]) &gt;&gt;&gt;np.ones((2,3),np.int) array([[1, 1, 1], [1, 1, 1]]) # 长度为10，元素值为0-1的随机数数组 &gt;&gt;&gt;np.random.rand(2,3) array([[0.96064533, 0.55490284, 0.13219661], [0.3036712 , 0.95073354, 0.39364538]]) # 通过frombuffer,fromstring,fromfile和fromfunction等函数创建数组 &gt;&gt;&gt;np.fromfunction(lambda x,y:(x+1)*(y+1),(2,3)) array([[1., 2., 3.], [2., 4., 6.]]) # a和b之间的随机整数 &gt;&gt;&gt;np.random.randint(low=0,high=20,size=5) array([ 7, 19, 12, 18, 12]) 索引和切片 &gt;&gt;&gt;a=np.arange(5) &gt;&gt;&gt;a[2] 2 &gt;&gt;&gt;a[:2] array([0, 1]) &gt;&gt;&gt;a[:-1] array([0, 1, 2, 3]) # 加入步长 &gt;&gt;&gt;a[0:4:2] array([0, 2]) &gt;&gt;&gt;a[::-1] array([4, 3, 2, 1, 0]) # 布尔索引 &gt;&gt;&gt;mask=np.array([True,True,False,False,True]) &gt;&gt;&gt;a[mask] array([0, 1, 4]) &gt;&gt;&gt;a[a&gt;2] array([3, 4]) # 索引轴缺失 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;a[-1] array([3, 4, 5]) &gt;&gt;&gt;a[-1,:] array([3, 4, 5]) # 使用...补全索引轴 &gt;&gt;&gt;a=np.arange(24).reshape(2,3,4) &gt;&gt;&gt;a array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) &gt;&gt;&gt;a[0,...,1] array([1, 5, 9]) 转置及reshape reshape: # None可将一维行向量转置为列向量 &gt;&gt;&gt;a=np.random.rand(6) &gt;&gt;&gt;a array([0.25779506, 0.22348449, 0.19464385, 0.26307378, 0.76859958, 0.80357972]) &gt;&gt;&gt;a[:,None] array([[0.25779506], [0.22348449], [0.19464385], [0.26307378], [0.76859958], [0.80357972]]) # 等价于reshape &gt;&gt;&gt;a.reshape(-1,1) array([[0.25779506], [0.22348449], [0.19464385], [0.26307378], [0.76859958], [0.80357972]]) &gt;&gt;&gt;a.reshape(2,3) array([[0.25779506, 0.22348449, 0.19464385], [0.26307378, 0.76859958, 0.80357972]]) # flatten()返回一维数组 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;a.flatten() array([0, 1, 2, 3, 4, 5]) # 等价于ravel或reshape(-1) &gt;&gt;&gt;np.ravel(a) array([0, 1, 2, 3, 4, 5]) transpose: &gt;&gt;&gt;a=np.arange(24).reshape(2,3,4) &gt;&gt;&gt;a array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) &gt;&gt;&gt;a,shape (2, 3, 4) &gt;&gt;&gt;a.transpose(2,0,1) array([[[ 0, 4, 8], [12, 16, 20]], [[ 1, 5, 9], [13, 17, 21]], [[ 2, 6, 10], [14, 18, 22]], [[ 3, 7, 11], [15, 19, 23]]]) &gt;&gt;&gt;a.transpose(2,0,1).shape (4, 2, 3) Ufun numpy数学函数： 注：不提供axis时，按整个数组计算 函数 说明 np.sin(x) sin(x) np.cos(x) cos(x) np.tan(x tan(x) np.arcsin(x) arcsin(x) np.arccos(x) arccos(x) np.arctan(x) arctan(x) np.arctan2(x,y) arctan(x/y) np.deg2rad(x) 角度转弧度 np.rad2deg(x) 弧度转角度 np.prod(x,axis=None) 乘积 np.sum(x,axis=None) 求和 np.exp(x) exp(x) np.log(x) ln(x) np.sqrt(x) 开根 np.square(x) 平方 np.absolute(x) 绝对值 np.fabs(x) 绝对值 np.sign(x) 符号 np.maximum(x,y) 逐元素取最大值 np.minimum(x,y) 逐元素取最小值 np.mean(x,axis=None) 均值 np.std(x,axis=None) 标准差 np.var(x,axis=None) 方差 np.average(x,weight) (加权)平均 np.argmax(x,axis=None) 最大值索引 np.argmin(x,axis=None) 最小值索引 np.sort(x,axis=None) 从小到大排序 np.argsort(x,axis=None) 从小到大排序的索引 利用frompyfunc自定义Ufun： np.frompyfunc(func,n_in,n_out) &gt;&gt;&gt;def pow2(x): ... return x**2 &gt;&gt;&gt;fun_pow2=np.frompyfunc(pow2,1,1) &gt;&gt;&gt;a=np.arange(5) &gt;&gt;&gt;fun_pow2(a) array([0, 1, 4, 9, 16], dtype=object) 广播操作 广播是针对形状不同的数组的运算采取的操作。 当我们使用ufunc函数对两个数组进行计算时，ufunc函数会对这两个数组的对应元素进行计算，因此它要求这两个数组有相同的大小(shape相同)。如果两个数组的shape不同的话（行列规模不等），会进行如下的广播(broadcasting)处理： 1）. 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐。因此输出数组的shape是输入数组shape的各个轴上的最大值（往最大轴长上靠）。 2）. 如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错。 3）. 当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值。 &gt;&gt;&gt;np.arange(2)[:,None]+np.arange(3) array([[0, 1, 2], [1, 2, 3]]) 四则运算 +，-，*，/ ， 为逐元素四则运算** 矩阵乘法，注意要符合矩阵乘法规则: # 2*3矩阵 &gt;&gt;&gt;a=np.array([[1,2,3],[4,5,6]]) # 3*2矩阵 &gt;&gt;&gt;b=np.array([[3,4],[5,6],[7,8]]) &gt;&gt;&gt;a.dot(b) array([[34, 40], [79, 94]]) **内积：**对于两个一维数组，计算的是这两个数组对应下标元素的乘积和；对于多维数组a和b，它计算的结果数组中的每个元素都是数组a和b的最后一维的内积，因此数组a和b的最后一维的长度必须相同。 计算公式为：inner(a, b)[i,j,k,m] = sum(a[i,j,:]*b[k,m,:]) &gt;&gt;&gt;a=np.arange(12).reshape(2,3,2) &gt;&gt;&gt;b=np.arange(12,24).reshape(2,3,2) &gt;&gt;&gt;np.inner(a,b) array([[[[ 13, 15, 17], [ 19, 21, 23]], [[ 63, 73, 83], [ 93, 103, 113]], [[113, 131, 149], [167, 185, 203]]], [[[163, 189, 215], [241, 267, 293]], [[213, 247, 281], [315, 349, 383]], [[263, 305, 347], [389, 431, 473]]]]) **外积：**只按照一维数组进行计算，如果传入为多维数组，先展开再计算 &gt;&gt;&gt;np.outer([1,2,3],[4,5,6,7]) array([[ 4, 5, 6, 7], [ 8, 10, 12, 14], [12, 15, 18, 21]]) 其它一些 np.ndenumerate返回索引及数组值的迭代对象 &gt;&gt;&gt;for index, x in np.ndenumerate(c): ... print(index, x) ((0, 0), 1) ((0, 1), 2) ((1, 0), 3) ((1, 1), 4) &gt;&gt;&gt;np.ndenumerate(c) &lt;numpy.lib.index_tricks.ndenumerate at 0x7f21cc0dbb90&gt; &gt;&gt;&gt;np.ndenumerate(c).next() ((0, 0), 1) np.random.choice从一维数组或int对象随机选择元素 np.random.choice(a,size=None,replace=True,p=None) a:一维数据或int对象；replace=True：可重复选择；p：选取的概率 &gt;&gt;&gt;np.random.choice(5,3,p=[0.1,0,0.3,0.6,0]) Out[4]: array([3, 3, 2], dtype=int64) np.nonezeros返回非0元素的索引 &gt;&gt;&gt;np.nonzero([[0,1,2],[0,0,2]]) (array([0, 0, 1], dtype=int64), array([1, 2, 2], dtype=int64)) np.intersect1d求两个数组的交集： &gt;&gt;&gt; np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1]) array([1, 3]) # 利用reduce取多个交 &gt;&gt;&gt; from functools import reduce &gt;&gt;&gt; reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2])) array([3]) np.where找到矩阵中满足条件的元素的索引 &gt;&gt;&gt; x = np.arange(9.).reshape(3, 3) &gt;&gt;&gt; np.where( x &gt; 5 ) (array([2, 2, 2]), array([0, 1, 2])) &gt;&gt;&gt; x[np.where( x &gt; 3.0 )] # 返回大于3的值. array([ 4., 5., 6., 7., 8.]) &gt;&gt;&gt; np.where( x == 3.0 ) # 返回等于3的值的索引. (array([1], dtype=int64), array([0], dtype=int64)) **np.indices:**获取数组shape属性的所有索引，其shpe为(dim,shape) &gt;&gt;&gt;np.indices((2,3)).shape (2, 2, 3) &gt;&gt;&gt;np.indices((2,3))[0] array([[0, 0, 0], [1, 1, 1]]) &gt;&gt;&gt;np.indices((2,3))[1] array([[0, 1, 2], [0, 1, 2]]) # 用于索引 &gt;&gt;&gt;a=np.arange(6).reshape(2,3) &gt;&gt;&gt;b=np.indices((2,3))[0] &gt;&gt;&gt;a[b] array([[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[3, 4, 5], [3, 4, 5], [3, 4, 5]]]) &gt;&gt;&gt;c=np.indices((2,3))[1] &gt;&gt;&gt;a[:,c] array([[[0, 1, 2], [0, 1, 2]], [[3, 4, 5], [3, 4, 5]]]) # 配合布尔索引进行mask操作 &gt;&gt;&gt;a[b&gt;0] array([3, 4, 5]) &gt;&gt;&gt;b&gt;0 array([[False, False, False], [ True, True, True]]) **np.repeat:**对数组进行扩展 np.repeat(a,repeats,axis=None) &gt;&gt;&gt;a=np.array([[10,20],[30,40]]) &gt;&gt;&gt;np.repeat(a,[3,2],axis=0) array([[10, 20], [10, 20], [10, 20], [30, 40], [30, 40]]) **np.tile:**对整个数组进行复制拼接 np.tile(a,reps) &gt;&gt;&gt; a=np.array([10,20]) &gt;&gt;&gt;np.tile(a, (3,2)) array([[10, 20, 10, 20], [10, 20, 10, 20], [10, 20, 10, 20]]) **np.pad:**数组填充(padding)操作 np.pad(array,pad_width,mode,**kwags) &gt;&gt;&gt;A = np.arange(95,99).reshape(2,2) &gt;&gt;&gt;np.pad(A,((3,2),(2,3)),'constant',constant_values = (0,0)) array([[ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 95, 96, 0, 0, 0], [ 0, 0, 97, 98, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0]]) **np.flip:**沿着指定轴翻转 &gt;&gt;&gt;A = np.arange(4).reshape((2,2)) &gt;&gt;&gt;np.flip(A,0) array([[2, 3], [0, 1]]) **np.unravel_index:**返回indices的下标 np.unravel_index(indices,dims,order=‘C’) &gt;&gt;&gt;np.unravel_index([22, 41, 37], (7,6)) (array([3, 6, 6]), array([4, 5, 1])) &gt;&gt;&gt;a=np.array([[1,2,3],[4,3,2]]) &gt;&gt;&gt;np.argmax(a) 3 &gt;&gt;&gt;np.unravel_index(np.argmax(a),a.shape) (1, 0) **np.unique:**去除数组中重复数字，并进行排序后输出 &gt;&gt;&gt;a=np.array([[1,2,3],[4,3,2]]) &gt;&gt;&gt;np.unique(a) array([1, 2, 3, 4])","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"numpy","slug":"numpy","permalink":"/tags/numpy/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"矩阵求导","slug":"矩阵求导","date":"2020-04-15T23:27:00.000Z","updated":"2020-04-17T03:29:07.000Z","comments":true,"path":"2020/04/16/矩阵求导/","link":"","permalink":"/2020/04/16/矩阵求导/","excerpt":"","text":"符号与符号布局的规定 符号规定： LLL ：标量 x​x​x​ ：n​n​n​ 维向量 yyy ：mmm 维向量 XXX ：m∗nm*nm∗n 维矩阵 分母布局： 标量LLL 对向量xxx 求导得到的是nnn 维向量，其中(∂L∂x)i=∂L∂xi(\\frac{\\partial L}{\\partial x})_i=\\frac{\\partial L}{\\partial x_i}(∂x∂L​)i​=∂xi​∂L​ 标量LLL 对矩阵XXX 求导是大小为m∗nm*nm∗n 的矩阵，其中(∂L∂X)ij=∂L∂xij(\\frac{\\partial L}{\\partial X})_{ij}=\\frac{\\partial L}{\\partial x_{ij}}(∂X∂L​)ij​=∂xij​∂L​ 向量y​y​y​ 对向量xxx 求导是大小为m∗nm*nm∗n 的矩阵，其中(∂y∂x)ij=∂yj∂xi​(\\frac{\\partial y}{\\partial x})_{ij}=\\frac{\\partial y_j}{\\partial x_i}​(∂x∂y​)ij​=∂xi​∂yj​​​ 分子布局： 标量LLL 对向量xxx 求导得到的是nnn 维向量，其中(∂L∂x)i=∂L∂xi(\\frac{\\partial L}{\\partial x})_i=\\frac{\\partial L}{\\partial x_i}(∂x∂L​)i​=∂xi​∂L​ 标量LLL 对矩阵XXX 求导是大小为n∗mn*mn∗m 的矩阵，其中(∂L∂X)ij=∂L∂xji(\\frac{\\partial L}{\\partial X})_{ij}=\\frac{\\partial L}{\\partial x_{ji}}(∂X∂L​)ij​=∂xji​∂L​ 向量yyy 对向量xxx 求导是大小为n∗mn*mn∗m 的矩阵，其中(∂y∂x)ij=∂yi∂xj(\\frac{\\partial y}{\\partial x})_{ij}=\\frac{\\partial y_i}{\\partial x_j}(∂x∂y​)ij​=∂xj​∂yi​​ 注：数学界有两派人使用着自己的符号约定，从而将矩阵微积分划分成了两个派别。这两个约定都是被大家所接受的。这两个布局之间相差一个转置，大多数的转置异常问题根本上来说是由于没有统一求导布局所导致的 。 几个重要的向量对向量求导结论 利用定义验证： ∂x∂x=I\\frac{\\partial x}{\\partial x}=I∂x∂x​=I 设向量z=f(x)z=f(x)z=f(x) ，则∂Az∂x=∂f∂xAT\\frac{\\partial Az}{\\partial x}=\\frac{\\partial f}{\\partial x}A^T∂x∂Az​=∂x∂f​AT ，特别地：∂Ax∂x=AT\\frac{\\partial Ax}{\\partial x}=A^T∂x∂Ax​=AT 设向量z=g(x)z=g(x)z=g(x) ，则∂zTA∂x=∂g∂xA\\frac{\\partial z^TA}{\\partial x}=\\frac{\\partial g}{\\partial x}A∂x∂zTA​=∂x∂g​A ，特别地：∂xTA∂x=A\\frac{\\partial x^TA}{\\partial x}=A∂x∂xTA​=A 设fff 为按元素运算的函数，则∂f(x)∂x=diag(f′(x))\\frac{\\partial f(x)}{\\partial x}=diag(f&#x27;(x))∂x∂f(x)​=diag(f′(x)) 标量求导的迹方法 多元函数微积分中，有df=∑i=1n∂f∂xidxi=∂f∂xTdxd f=\\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_{i}} d x_{i}=\\frac{\\partial f}{\\partial x}^{T} d xdf=∑i=1n​∂xi​∂f​dxi​=∂x∂f​Tdx ，在矩阵导数中： dL=∑i=1m∑j=1n∂L∂XijdXij=tr⁡((∂L∂X)TdX)d L=\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} d \\mathbf{X}_{ij}=\\operatorname{tr}\\left(\\left(\\frac{\\partial L}{\\partial \\mathbf{X}}\\right)^{T} d \\mathbf{X}\\right) dL=i=1∑m​j=1∑n​∂Xij​∂L​dXij​=tr((∂X∂L​)TdX) 迹的常用性质： tr(XT)=tr(X)tr(X^T)=tr(X)tr(XT)=tr(X) tr(X+Y)=tr(X)+tr(Y)tr(X+Y)=tr(X)+tr(Y)tr(X+Y)=tr(X)+tr(Y) tr(XY)=tr(YX)tr(XY)=tr(YX)tr(XY)=tr(YX) tr⁡(ATB)=∑i,jAijBij\\operatorname{tr}\\left(A^{T} B\\right)=\\sum_{i, j} A_{i j} B_{i j}tr(ATB)=∑i,j​Aij​Bij​ tr(AT(B⊙C))=tr((A⊙B)TC)=∑i,jAijBijCijtr\\left(A^{T}(B \\odot C)\\right)=tr\\left((A \\odot B)^{T} C\\right)=\\sum_{i, j} A_{i j} B_{i j} C_{i j}tr(AT(B⊙C))=tr((A⊙B)TC)=∑i,j​Aij​Bij​Cij​ 其中⊙\\odot⊙ 为Hadamard乘积 常用全微分公式及法则 常用公式 d(X+Y)=dX+dYd(X+Y)=dX+dYd(X+Y)=dX+dY d(tr(X))=tr(dX)d(tr(X))=tr(dX)d(tr(X))=tr(dX) d(XY)=(dX)Y+XdYd(XY)=(dX)Y+XdYd(XY)=(dX)Y+XdY d(X⊙Y)=(dX)⊙Y+X⊙dYd(X \\odot Y)=(dX) \\odot Y +X \\odot dYd(X⊙Y)=(dX)⊙Y+X⊙dY dσ(X)=σ′(X)⊙dXd \\sigma(X)=\\sigma&#x27;(X)\\odot dXdσ(X)=σ′(X)⊙dX （$\\sigma $ 为逐元素运算） dX−1=−X−1(dX)X−1dX^{-1}=-X^{-1}(dX)X^{-1}dX−1=−X−1(dX)X−1 d∣X∣=∣X∣tr(X−1dX)d|X|=|X|tr(X^{-1}dX)d∣X∣=∣X∣tr(X−1dX) dln∣X∣=tr(X−1dX)dln|X|=tr(X^{-1}dX)dln∣X∣=tr(X−1dX) dXT=(dX)TdX^T=(dX)^TdXT=(dX)T 乘法法则 若x∈Rp,y=f(x)∈Rq,z=g(x)∈Rq\\mathbf{x} \\in R^{p}, \\mathbf{y}=f(\\mathbf{x}) \\in R^{q}, \\mathbf{z}=g(\\mathbf{x}) \\in R^{q}x∈Rp,y=f(x)∈Rq,z=g(x)∈Rq,则$ \\frac{\\partial \\mathbf{y}^{T} \\mathbf{z}}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\mathbf{z}+\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\mathbf{y} \\in R^{p}$ 若 x∈Rp,y=f(x)∈R,z=g(x)∈Rq\\mathbf{x} \\in R^{p}, y=f(\\mathbf{x}) \\in R, \\mathbf{z}=g(\\mathbf{x}) \\in R^{q}x∈Rp,y=f(x)∈R,z=g(x)∈Rq,则 ∂yz∂x=∂y∂xzT+∂z∂xy∈Rp×q\\frac{\\partial y \\mathbf{z}}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial \\mathbf{x}} \\mathbf{z}^{T}+\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} y \\in R^{p \\times q}∂x∂yz​=∂x∂y​zT+∂x∂z​y∈Rp×q 链式法则 向量对向量求导(包含向量对标量求导、标量对向量求导)： 设多个向量之间存在依赖关系：a⇒b⇒…⇒x⇒y⇒z\\mathbf{a} \\Rightarrow \\mathbf{b} \\Rightarrow \\ldots \\Rightarrow \\mathbf{x} \\Rightarrow \\mathbf{y} \\Rightarrow \\mathbf{z}a⇒b⇒…⇒x⇒y⇒z 计算方法：∂z∂a=∂b∂a∂c∂b…∂y∂x∂z∂y\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{a}}=\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}} \\ldots \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}∂a∂z​=∂a∂b​∂b∂c​…∂x∂y​∂y∂z​ 标量对矩阵求导： 若矩阵X∈Rp∗qX\\in R^{p*q}X∈Rp∗q ，矩阵y=g(X)∈Rs∗ty=g(X)\\in R^{s*t}y=g(X)∈Rs∗t ，标量z=f(Y)∈Rz=f(Y) \\in Rz=f(Y)∈R，则∂z∂Xij=tr⁡((∂z∂Y)T∂Y∂Xij)\\frac{\\partial z}{\\partial \\mathbf{X}_{ij}}=\\operatorname{tr}\\left(\\left(\\frac{\\partial z}{\\partial \\mathbf{Y}}\\right)^{T} \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{X}_{ij}}\\right)∂Xij​∂z​=tr((∂Y∂z​)T∂Xij​∂Y​) 由上式，若链式关系中存在有矩阵对矩阵求导，则难以直接写出标量对矩阵的导数，下面则给出常用的线性关系下的&quot;链式&quot;求导 ： 若存在关系：X⇒Y=AX+B⇒L=f(Y)\\mathbf{X} \\Rightarrow \\mathbf{Y}=\\mathbf{A} \\mathbf{X}+\\mathbf{B} \\Rightarrow L=f(\\mathbf{Y})X⇒Y=AX+B⇒L=f(Y) ，则∂L∂X=AT∂L∂Y\\frac{\\partial L}{\\partial \\mathbf{X}}=A^{T} \\frac{\\partial L}{\\partial \\mathbf{Y}}∂X∂L​=AT∂Y∂L​ 若存在关系： X⇒Y=XA+B⇒L=f(Y)\\mathbf{X} \\Rightarrow \\mathbf{Y}=\\mathbf{X} \\mathbf{A}+\\mathbf{B} \\Rightarrow L=f(\\mathbf{Y})X⇒Y=XA+B⇒L=f(Y)，则∂L∂X=∂L∂YAT\\frac{\\partial L}{\\partial \\mathbf{X}}= \\frac{\\partial L}{\\partial \\mathbf{Y}}A^T∂X∂L​=∂Y∂L​AT 链式法则不再成立，往往通过迹方法来求标量对矩阵的导数 常见技巧 一维下标求和考虑写成向量内积的形式 二位下标求和考虑写成trtrtr的形式 向量模长的平方考虑内积运算∑ixi2=xTx\\sum_{i} x_{i}^{2}=\\mathbf{x}^{T} \\mathbf{x}∑i​xi2​=xTx 矩阵的Frobenius范数考虑写成trtrtr 的形式∥X∥F2=tr(XXT)\\|\\mathbf{X}\\|_{F}^{2}=t r\\left(\\mathbf{X} \\mathbf{X}^{T}\\right)∥X∥F2​=tr(XXT) 推荐阅读： 知乎，矩阵求导术(上) 知乎，矩阵求导术(下)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"矩阵求导","slug":"矩阵求导","permalink":"/tags/矩阵求导/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"内网穿透proxyer","slug":"proxyer做内网穿透","date":"2020-04-10T01:15:00.000Z","updated":"2020-04-10T01:48:36.000Z","comments":true,"path":"2020/04/10/proxyer做内网穿透/","link":"","permalink":"/2020/04/10/proxyer做内网穿透/","excerpt":"","text":"Github地址：https://github.com/khvysofq/proxyer 服务端 安装docker： $ curl -sSL https://get.docker.com/ | sh $ systemctl start docker $ systemctl enable docker 安装docker compose $ curl -L \"https://get.daocloud.io/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose $ chmod +x /usr/local/bin/docker-compose 安装proxyer： $ wget https://raw.githubusercontent.com/khvysofq/proxyer/master/docker-compose.yml # 后面1.1.1.1改成服务器ip地址 $ export PROXYER_PUBLIC_HOST=1.1.1.1 $ docker-compose up -d 安装完成后，通过ip:6789访问服务端WEB管理面板了，进去后需要设置一个客户端认证密码。 客户端 从web管理面板下载对应系统客户端 windows系统直接运行，linux解压后运行./proxyer，按照提示浏览器进入127.0.0.1:9876 如图，内网地址填127.0.0.1:22，序列号自定义 安装ssh，已安装可跳过： $ sudo apt-get install ssh 以图片为例，远程ssh连接时，连接使用： $ ssh -p 43537 yu@66.152.179.100 设置为开机自启 (支持Ubuntu16.04，18.04好像不支持这种方式了) 编辑/etc/rc.local文件，在exit 0前添加proxyer文件的位置 如：~/proxyer","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"内网穿透","slug":"内网穿透","permalink":"/tags/内网穿透/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"Git！","slug":"Git","date":"2020-03-30T10:49:00.000Z","updated":"2020-03-30T13:08:35.000Z","comments":true,"path":"2020/03/30/Git/","link":"","permalink":"/2020/03/30/Git/","excerpt":"","text":"工作原理/流程 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 配置 在Github注册后，做一下本地的Git仓库配置： $ git config --global user.name \"ybb-ybb\" $ git config --global user.email \"21901037@mail.dlut.edu.cn\" 如果对某个仓库使用不同的用户名和邮箱，去掉--global参数即可 本地仓库 提交 # 将当前目录变成git可管理的仓库 $ git init # 将readme.md添加到缓存区 $ git add readme.md # 将所有文件添加到缓存区 $ git add . # 将缓存区文件提交到仓库 $ git commit -m \"提交信息\" # 查看是否还有文件未提交 $ git status # 查看文件修改内容 $ git diff readme.md # 查看历史记录 $ git log # 查看历史记录的简单信息 $ git log –pretty=oneline 版本回退 # 回到上一个版本 $ git reset --hard HEAD^ # 同理，回到上上个版本等，以此类推 $ git reset --hard HEAD^^ # 或 $ git reset --hard HEAD~2 # 查看版本号变化 $ git reflog # 版本号回退 $ git reset --hard {版本号} # 丢弃工作区的修改（回到暂存区的状态） $ git checkout -- filename 远程仓库 配置 本地Git仓库和Github仓库之间是通过ssh加密的，因此需要先进行一些配置： 第一步：创建SSH Key。在主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果有的话，直接跳过此如下命令，如果没有的话： ssh-keygen -t rsa –C “youremail@example.com” id_rsa是私钥，id_rsa.pub是公钥 第二步：登录github,打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”,填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容 ，Add key 推送到远程仓库 # 先创建一个github仓库，复制http或ssh地址,然后关联 $ git remote add origin {http adress} # 推送，第一次推送时添加-u参数,把master分支推送到远程 $ git push -u origin master 分支 # 创建分支 $ git branch backup # 切换分支 $ git checkout backup # 或者：创建+切换分支 $ git checkout -b backup # 查看当前分支 $ git branch # 在master分支上，将分支backup合并到master上 $ git merge backup # 删除分支 $ git branch -d backup","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"Git","slug":"Git","permalink":"/tags/Git/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"离线下载+在线播放网盘","slug":"离线下载+在线播放网盘","date":"2020-03-15T04:13:00.000Z","updated":"2020-03-15T04:39:06.210Z","comments":true,"path":"2020/03/15/离线下载+在线播放网盘/","link":"","permalink":"/2020/03/15/离线下载+在线播放网盘/","excerpt":"","text":"主要过程是用Aria2进行下载，然后上传到OneDrive云盘并用OneIndex关联云盘实现网页访问。 获取OneDrive 申请OneDrive5T账号 两个申请OneDrive云盘5T的方法： 1、申请微软的Office 365开发者计划，地址：免费获得一年的21TB OneDrive和Microsoft Office 365企业 2、使用热心大佬提供的临时邮箱申请一个，方法如下： 1)、进入注册地址https://products.office.com/en-us/student?tab=students 2)、输入如有乐享提供的临时邮箱，地址：https://51.ruyo.net/8263.html 3)、填入密码，和从临时邮箱获取的验证码 授权 授权认证： 点击右侧URL登录并授权，授权地址→ 国际版 世纪互联 授权后会获取一个localhost开头打不开的链接，这里只需要记住code，也就是链接中code=和&amp;中间的参数。 安装Aria2 以下操作都是在服务器端执行 $ cd /root $ wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubiBackup/doubi/master/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh #备用地址 $ wget -N --no-check-certificate https://www.moerats.com/usr/shell/Aria2/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh 安装后可以使用bash aria2.sh命令修改Aria2的默认下载目录、端口号、密码等 安装OneDriveUploader $ wget https://raw.githubusercontent.com/MoeClub/OneList/master/OneDriveUploader/amd64/linux/OneDriveUploader -P /usr/local/bin/ # 授权 $ chmod +x /usr/local/bin/OneDriveUploader # 初始化配置 #将moerats替换成授权步骤中获取的code参数 $ code=\"moerats\" $ OneDriveUploader -a \"${code}\" 如果提示 Init config file: /root/auth.json 类似信息，则初始化成功。 手动上传示例： # -c指定初始化文件位置，-s指定上传文件，-r指定网盘目录，不指定默认为根目录 # 将当前目录下的Download文件夹上传到OneDrive网盘Test目录中 $ OneDriveUploader -c /root/auth.json -s \"Download\" -r \"Test\" 配置Aria2的自动上传 # 新建文件 $ touch rcloneupload.sh # 修改文件内容 $ vim rcloneupload.sh 将文件内容修改为下面内容，注意替换Aria2的下载目录 #!/bin/bash GID=\"$1\"; FileNum=\"$2\"; File=\"$3\"; MaxSize=\"15728640\"; Thread=\"3\"; #默认3线程，自行修改，服务器配置不好的话，不建议太多 Block=\"20\"; #默认分块20m，自行修改 RemoteDIR=\"\"; #上传到Onedrive的路径，默认为根目录，如果要上传到MOERATS目录，\"\"里面请填成MOERATS LocalDIR=\"/www/download/\"; #Aria2下载目录，记得最后面加上/ Uploader=\"/usr/local/bin/OneDriveUploader\"; #上传的程序完整路径，默认为本文安装的目录 Config=\"/root/auth.json\"; #初始化生成的配置auth.json绝对路径，参考第3步骤生成的路径 if [[ -z $(echo \"$FileNum\" |grep -o '[0-9]*' |head -n1) ]]; then FileNum='0'; fi if [[ \"$FileNum\" -le '0' ]]; then exit 0; fi if [[ \"$#\" != '3' ]]; then exit 0; fi function LoadFile(){ if [[ ! -e \"${Uploader}\" ]]; then return; fi IFS_BAK=$IFS IFS=$'\\n' tmpFile=\"$(echo \"${File/#$LocalDIR}\" |cut -f1 -d'/')\" FileLoad=\"${LocalDIR}${tmpFile}\" if [[ ! -e \"${FileLoad}\" ]]; then return; fi ItemSize=$(du -s \"${FileLoad}\" |cut -f1 |grep -o '[0-9]*' |head -n1) if [[ -z \"$ItemSize\" ]]; then return; fi if [[ \"$ItemSize\" -ge \"$MaxSize\" ]]; then echo -ne \"\\033[33m${FileLoad} \\033[0mtoo large to spik.\\n\"; return; fi ${Uploader} -c \"${Config}\" -t \"${Thread}\" -b \"${Block}\" -s \"${FileLoad}\" -r \"${RemoteDIR}\" if [[ $? == '0' ]]; then rm -rf \"${FileLoad}\"; fi IFS=$IFS_BAK } LoadFile; 授权 $ chmod +x rcloneupload.sh bash aria2.sh修改配置文件，加上一行： on-download-complete=/root/rcloneupload.sh 重启Aria2生效 测试一下 试一下 bash /root/rcloneupload.sh 正常为无反应，如果报错： 1、安装dos2unix $ apt-get install dos2unix -y 2、转换格式 $ dos2unix /root/rcloneupload.sh Aria2离线下载的使用 谷歌浏览器插件 aria2 for chrome 进去之后AriaNg设置——&gt;添加新RPC设置，配置如下： 然后添加任务即可进行离线下载，并自动上传到OneDrive OneIndex对接网盘 OneIndex可以对接Onedrive网盘，将网盘里的内容直接显示成目录，视频可以在线播放。也可以搭建自己的在线图床/视频播放系统。 使用docker安装Oneindex $ docker run -d -p 8181:80 --restart=always baiyuetribe/oneindex 然后访问ip:8181按照提示操作即可 效果图： 如果要进入系统管理页面，访问ip:8181/?/login 可选，域名访问 如果有自己的域名，可以通过反向代理进行http访问 宝塔反代：先进入宝塔面板，点击左侧网站，添加站点，完成后进入网站设置，点击反向代理，目标URL填入http://127.0.0.1:8181，再启用反向代理即可。 宝塔界面可一键安装： $ wget -O install.sh http://download.bt.cn/install/install-ubuntu.sh &amp;&amp; sudo bash install.sh","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"OneDrive","slug":"OneDrive","permalink":"/tags/OneDrive/"},{"name":"离线下载","slug":"离线下载","permalink":"/tags/离线下载/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"tensorboardX","slug":"tensorboardX","date":"2020-03-09T13:31:11.839Z","updated":"2020-03-15T04:35:24.214Z","comments":true,"path":"2020/03/09/tensorboardX/","link":"","permalink":"/2020/03/09/tensorboardX/","excerpt":"","text":"创建 TensorBoardX的GitHub地址：传送门 首先创建一个 SummaryWriter 的示例 ： from tensorboardX import SummaryWriter # Creates writer1 object. # The log will be saved in 'runs/exp' writer1 = SummaryWriter('runs/exp') # Creates writer2 object with auto generated file name # The log directory will be something like 'runs/Aug20-17-20-33' writer2 = SummaryWriter() # Creates writer3 object with auto generated file name, the comment will be appended to the filename. # The log directory will be something like 'runs/Aug20-17-20-33-resnet' writer3 = SummaryWriter(comment='resnet') 以上展示了三种初始化 SummaryWriter 的方法： 提供一个路径，将使用该路径来保存日志 无参数，默认将使用 runs/日期时间 路径来保存日志 提供一个 comment 参数，将使用 runs/日期时间-comment 路径来保存日志 在浏览器中查看这些可视化数据： tensorboard --logdir=&lt;your_log_dir&gt; 用各种add方法记录数据 数字(scalar) add_scalar(tag, scalar_value, global_step=None, walltime=None) 参数: tag (string): 数据名称，不同名称的数据使用不同曲线展示 scalar_value (float): 数字常量值 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() 需要注意，这里的 scalar_value 一定是 float 类型，如果是 PyTorch scalar tensor，则需要调用 .item() 方法获取其数值。我们一般会使用 add_scalar 方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，直观地监控训练过程。 示例： from tensorboardX import SummaryWriter writer = SummaryWriter('runs/scalar_example') for i in range(10): writer.add_scalar('quadratic', i**2, global_step=i) writer.add_scalar('exponential', 2**i, global_step=i) 图片(image) 需要pillow库的支持 用add_image记录单个图像数据，用add_images记录多个图像数据 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') CHW为channel*hight*width 示例： from tensorboardX import SummaryWriter import cv2 as cv writer = SummaryWriter('runs/image_example') for i in range(1, 6): writer.add_image('countdown', cv.cvtColor(cv.imread('{}.jpg'.format(i)), cv.COLOR_BGR2RGB), global_step=i, dataformats='HWC') 直方图(histogram) add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 示例： from tensorboardX import SummaryWriter import numpy as np writer = SummaryWriter('runs/embedding_example') writer.add_histogram('normal_centered', np.random.normal(0, 1, 1000), global_step=1) writer.add_histogram('normal_centered', np.random.normal(0, 2, 1000), global_step=50) writer.add_histogram('normal_centered', np.random.normal(0, 3, 1000), global_step=100) &quot;DISTRIBUTIONS&quot;和&quot;HISTOGRAMS&quot;两栏都是用来观察数据分布的。其中在&quot;HISTOGRAMS&quot;中，同一数据不同 step 时候的直方图可以上下错位排布 (OFFSET) 也可重叠排布 (OVERLAY)。 运行图(graph) add_graph(model, input_to_model=None, verbose=False, **kwargs) 可以可视化神经网络的结构，参考Github官方样例 嵌入张量(embedding) 使用 add_embedding 方法可以在二维或三维空间可视化 embedding 向量。 add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) 参数： mat (torch.Tensor or numpy.array): 一个MxN矩阵，每行代表特征空间的一个数据点 metadata (list or torch.Tensor or numpy.array, optional): 一个一维列表N，mat 中每行数据的 label，大小应和 mat 行数相同 label_img (torch.Tensor, optional): 一个形如 NxCxHxW 的张量，对应 mat 每一行数据显示出的图像，N 应和 mat 行数相同 global_step (int, optional): 训练的 step tag (string, optional): 数据名称，不同名称的数据将分别展示 示例： from tensorboardX import SummaryWriter import torchvision writer = SummaryWriter('runs/embedding_example') mnist = torchvision.datasets.MNIST('mnist', download=True) writer.add_embedding( mnist.train_data.reshape((-1, 28 * 28))[:100,:], #直接将mnist前100个数据展开成一维向量作为embedding metadata=mnist.train_labels[:100], #每个embedding的label label_img = mnist.train_data[:100,:,:].reshape((-1, 1, 28, 28)).float() / 255, #每个图像 global_step=0 )","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2020-03-09T12:16:01.000Z","updated":"2020-03-15T04:21:33.074Z","comments":true,"path":"2020/03/09/Hexo-Theme-Sakura/","link":"","permalink":"/2020/03/09/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakura主题 English document 基于WordPress主题Sakura修改成Hexo的主题。 demo预览 正在开发中… 交流群 若你是使用者，加群QQ: 801511924 若你是创作者，加群QQ: 194472590 主题特性 首页大屏视频 首页随机封面 图片懒加载 valine评论 fancy-box相册 pjax支持，音乐不间断 aplayer音乐播放器 多级导航菜单（按现在大部分hexo主题来说，这也算是个特性了） 赞赏作者 如果喜欢hexo-theme-sakura主题，可以考虑资助一下哦~非常感激！ paypal | Alipay 支付宝 | WeChat Pay 微信支付 未完善的使用教程 那啥？老实说我目前也不是很有条理233333333~ 1、主题下载安装 hexo-theme-sakura建议下载压缩包格式，因为除了主题内容还有些source的配置对新手来说比较太麻烦，直接下载解压就省去这些麻烦咯。 下载好后解压到博客根目录（不是主题目录哦，重复的选择替换）。接着在命令行（cmd、bash）运行npm i安装依赖。 2、主题配置 博客根目录下的_config配置 站点 # Site title: 你的站点名 subtitle: description: 站点简介 keywords: author: 作者名 language: zh-cn timezone: 部署 deploy: type: git repo: github: 你的github仓库地址 # coding: 你的coding仓库地址 branch: master 备份 （使用hexo b发布备份到远程仓库） backup: type: git message: backup my blog of https://honjun.github.io/ repository: # 你的github仓库地址,备份分支名 （建议新建backup分支） github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup 主题目录下的_config配置 其中标明【改】的是需要修改部门，标明【选】是可改可不改，标明【非】是不用改的部分 # site name # 站点名 【改】 prefixName: さくら荘その siteName: hojun # favicon and site master avatar # 站点的favicon和头像 输入图片路径（下面的配置是都是cdn的相对路径，没有cdn请填写完整路径，建议使用jsdeliver搭建一个cdn啦，先去下载我的cdn替换下图片就行了，简单方便~）【改】 favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # 站点url 【改】 url: https://sakura.hojun.cn # 站点介绍（或者说是个人签名）【改】 description: Live your life with passion! With some drive! # 站点cdn，没有就为空 【改】 若是cdn为空，一些图片地址就要填完整地址了，比如之前avatar就要填https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # 开启pjax 【选】 pjax: 1 # 站点首页的公告信息 【改】 notice: hexo-Sakura主题已经开源，目前正在开发中... # 懒加载的加载中图片 【选】 lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # 站点菜单配置 【选】 menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer first，1: older first. 【非】 homePageSortType: -1 # Home page article shown number) 【非】 homeArticleShown: 10 # 背景图片 【选】 bgn: 8 # startdash面板 url, title, desc img 【改】 startdash: - {url: /theme-sakura/, title: Sakura, desc: 本站 hexo 主题, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: 博主的b站视频, img: /img/startdash/bilibili.jpg} - {url: /, title: hojun的万事屋, desc: 技术服务, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # 你的站点建立日期 【改】 siteBuildingTime: 07/17/2018 # 社交按钮(social) url, img PC端配置 【改】 social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # 社交按钮(msocial) url, img 移动端配置 【改】 msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # 赞赏二维码（其中wechatSQ是赞赏单页面的赞赏码图片）【改】 donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # 首页视频地址为https://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4，配置如下 【改】 movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # 多个视频用逗号隔开，随机获取。支持的格式目前已知MP4,Flv。其他的可以试下，不保证有用 name: Unbroken.mp4 # 左下角aplayer播放器配置 主要改id和server这两项，修改详见[aplayer文档] 【改】 aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valine评论配置【改】 valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ 分类页和标签页配置 分类页 标签页 配置项在\\themes\\Sakura\\languages\\zh-cn.yml里。新增一个分类或标签最好加下哦，当然嫌麻烦可以直接使用一张默认图片（可以改主题或者直接把404图片替换下，征求下意见要不要给这个在配置文件中加个开关，可以issue或群里提出来），现在是没设置的话会使用那种倒立小狗404哦。 #category # 按分类名创建 技术: #中文标题 zh: 野生技术协会 # 英文标题 en: Geek – Only for Love # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg 生活: zh: 生活 en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # 标签名即是标题 悦读: # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg 单页面封面配置 如留言板页面页面，位于source下的comment下，打开index.md如下： --- title: comment date: 2018-12-20 23:13:48 keywords: 留言板 description: comments: true # 在这里配置单页面头部图片，自定义替换哦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- 单页面配置 番组计划页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # 番组图片 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # 番组名 title: 朝花夕誓——于离别之朝束起约定之花 # 追番状态 （追番ing/已追完） status: 已追完 # 追番进度 progress: 100 # 番剧日文名称 jp: さよならの朝に約束の花をかざろう # 放送时间 time: 放送时间: 2018-02-24 SUN. # 番剧介绍 desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: 朝花夕誓——于离别之朝束起约定之花 status: 已追完 progress: 50 jp: さよならの朝に約束の花をかざろう time: 放送时间: 2018-02-24 SUN. desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 --- 友链页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: links title: links # 创建日期，可以改下 date: 2018-12-19 23:11:06 # 图片上的标题，自定义修改 keywords: 友人帐 description: # true/false 开启/关闭评论 comments: true # 页面头部图片，自定义修改 photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # 友链配置 links: # 类型分组 - group: 个人项目 # 类型简介 desc: 充分说明这家伙是条咸鱼 &lt; (￣︶￣)&gt; items: # 友链链接 - url: https://shino.cc/fgvf # 友链头像 img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # 友链站点名 name: Google # 友链介绍 下面雷同 desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 # 类型分组... - group: 小伙伴们 desc: 欢迎交换友链 ꉂ(ˊᗜˋ) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 --- 写文章配置 主题集成了个人插件hexo-tag-bili和hexo-tag-fancybox_img。其中hexo-tag-bili用来在文章或单页面中插入B站外链视频，使用语法如下： 详细使用教程详见hexo-tag-bili。 hexo-tag-fancybox_img用来在文章或单页面中图片，使用语法如下： 详细使用教程详见hexo-tag-fancybox_img 还有啥，一时想不起来… To be continued…","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"web","slug":"web","permalink":"/tags/web/"},{"name":"悦读","slug":"悦读","permalink":"/tags/悦读/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]},{"title":"neural ode","slug":"Neural ODE","date":"2020-03-04T03:38:00.000Z","updated":"2020-07-02T08:52:50.662Z","comments":true,"path":"2020/03/04/Neural ODE/","link":"","permalink":"/2020/03/04/Neural ODE/","excerpt":"","text":"原文：Neural ordinary differential equations NIPS2018最佳论文 简介 简单复习一下ResNet： 通过残差块解决反向传播过程中的梯度消失问题 ResNet、RNN、Normalizing flow 等模型都是这种形式： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) 如果采用更多的层数和更小的步长，可以优化为一个常微分方程： dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta) dtdh(t)​=f(h(t),t,θ) 这就是ODE Net的核心idea了……下面进行具体的分析 给定常微分方程，数学理论上可以对其进行解析法求解，但通常我们只关心数值解：在已知h(t0)h(t_0)h(t0​) 的情况下，求出h(t1)h(t_1)h(t1​) 。这在神经网络里对应的是正向传播。用ResNet对比一下： ResNet的正向传播： ht+1=ht+f(ht,θt)h_{t+1}=h_t+f(h_t,\\theta_t) ht+1​=ht​+f(ht​,θt​) ODE网络的正向传播： dh(t)dt=f(h(t),t,θ)∫t0t1dh(t)=∫t0t1f(h(t),t,θ)dth(t1)=h(t0)+∫t0t1f(h(t),t,θ)dt\\begin{array}{c} \\frac{d h(t)}{d t}=f(h(t), t, \\theta) \\\\ \\int_{t_{0}}^{t_{1}} d h(t)=\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\\\ h\\left(t_{1}\\right)=h\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f(h(t), t, \\theta) d t \\end{array} dtdh(t)​=f(h(t),t,θ)∫t0​t1​​dh(t)=∫t0​t1​​f(h(t),t,θ)dth(t1​)=h(t0​)+∫t0​t1​​f(h(t),t,θ)dt​ 求解这个常微分方程数值解的方法有很多，最原始的是欧拉法：固定Δt\\Delta tΔt ,通过逐步迭代来求解： h(t+Δt)=h(t)+Δt∗f(h(t),t,θ)h(t+\\Delta t)=h(t)+\\Delta t * f(h(t),t,\\theta) h(t+Δt)=h(t)+Δt∗f(h(t),t,θ) 我们看到，如果令Δt=1\\Delta t=1Δt=1 ,离散化的欧拉法就退化成残差模块的表达式，也就是说ResNet可以看成是ODENet的特殊情况。 但欧拉法只是解常微分方程最基础的解法，它每走一步都会产生误差，并且误差会层层累积起来。近百年来，在数学和物理学领域已经有更成熟的ODE Solve方法，它们不仅能保证收敛到真实解，而且能够控制误差，本文在不涉及ODE Solve内部结构的前提下(将ODE Solve作为一个黑盒来使用)，研究如何用ODE Solve帮助机器学习。 这篇文章使用了一种适应性的ODE solver，它不像ResNet那样固定步长，而是根据给定的误差容忍度自动调整步长，黑色的评估位置可以视作神经元，他的位置也会根据误差容忍度自动调整： 使用ODENet的几个好处（和原文不完全一致，详细可看原文）： 一般的神经网络利用链式法则，将梯度从最外层的函数逐层向内传播，并更新每一层的参数θ\\thetaθ ,这就需要在前向传播中需要保留所有层的激活值，并在沿计算路径反传梯度时利用这些激活值。这对内存的占用非常大，层数越多，占用的内存也越大，这限制了深度模型的训练过程。 本文给出的用ODENet反向传播的方法不存储任何中间过程，因而不管层数如何加深，只需要常数级的内存成本。 自适应的计算。传统的欧拉法会有误差逐层累积的缺陷，而ODENet可以在训练过程中实时的监测误差水平，并可以调整精度来控制模型的成本。例如：在训练时我们可以使用较高的精度使训练的模型尽可能准确，而在测试时可以使用较低的精度，减少测试成本。 应用在流模型上会极大简化变分公式的计算，在下文中详细讲解 在时间上的连续性，好理解不展开 对于ODEnet在流模型上的应用，可以看一下论文FFJORD。 反向传播 在训练连续神经网络的过程中，正向传播可以使用ODE slove。但对ODE solve求导来进行反向传播求解梯度是很困难的，本篇文章使用Pontryagin的伴随方法(adjoint method) 来求解梯度，该方法不仅在计算和内存上有更大优势，同时还能够精确地控制数值误差。 具体而言，对于： L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))(1)\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right)\\tag{1} L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ))(1) 为优化LLL ,我们需要计算他对于参数z(t0),t0,t1z(t_0),t_0,t_1z(t0​),t0​,t1​ 和θ\\thetaθ 的梯度。 第一步是确定loss的梯度如何取决于隐藏状态z(t)z(t)z(t) 的变化，这在文章中被称作伴随a(t)a(t)a(t) (adjointa(t)adjoint \\quad a(t)adjointa(t) ) a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 这个a(t)a(t)a(t) 实际等价于反向传播算法中的梯度，可以由另一个ODE给定(证明补充在后面)： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 在传统的基于链式法则的反向传播过程中，我们将后一层对前一层进行求导以传递梯度(∂L/∂z(t0)=∂L/∂z(t1)∗∂z(t1)/∂z(t0)\\partial L/\\partial z(t_0)=\\partial L/\\partial z(t_1) * \\partial z(t_1) / \\partial z(t_0)∂L/∂z(t0​)=∂L/∂z(t1​)∗∂z(t1​)/∂z(t0​))，而在ODENet中，可以再次调用ODESolve计算∂L/∂z(t0)\\partial L/\\partial z(t_0)∂L/∂z(t0​)。 对于计算相对于参数θ\\thetaθ 的梯度，公式类似： dLdθ=∫t1t0a(t)⊤∂f(z(t),t,θ)∂θdt(3)\\frac{d L}{d \\theta}=\\int_{t_{1}}^{t_{0}} a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t\\tag{3} dθdL​=∫t1​t0​​a(t)⊤∂θ∂f(z(t),t,θ)​dt(3) 这三个积分(带标号的三个)可以在同一个ODE solver过程中进行计算： 简单解释如何理解上面这个算法： 以前向传播为例，ODESolve(h(t0),f,t0,t1,θ)ODESolve(h(t_0),f,t_0,t_1,\\theta)ODESolve(h(t0​),f,t0​,t1​,θ) 表示求解常微分方程dh(t)dt=f(h(t),t,θ)\\frac{d \\mathbf{h}(t)}{d t}=f(\\mathbf{h}(t), t, \\theta)dtdh(t)​=f(h(t),t,θ) 的数值解h(t1)h(t_1)h(t1​) 。 将积分串联在一起以使用一次ODESolver解出所有量。 如果Loss不仅仅取决于最终状态，那么在用ODENet进行反向传播时，需要在这些状态中进行一系列的单独求解，每次都要调整算法中的 adjoint a(t)a(t)a(t) 。 用ODE网络替代ResNet 对图像进行两次下采样，然后分别应用六个残差块和一个ODENet进行对比： RK-Net是用Runge-Kutta积分器，直接进行反向误差的传播 LLL 表示ResNet的隐藏层数，L~\\tilde{L}L~ 表示调用ODESolve的次数。 误差控制，前向传播和反向传播的求值次数，网络深度表现在下图： a：求值次数和精度成反比 b：求值次数与时间成正比 c：求值次数与反向传播时间成正比，并且反向传播的时间大概是正向传播的一半 d：网络深度，由于ODENet是一个连续网络，没有隐藏层，因此将评估点的数量作为深度，可以看到在训练过程中网络深度逐渐增加 连续的归一化流模型 流模型的解读在前一篇博客中 流模型使用一个可逆函数fff 进行两个分布之间的映射，变换前后的两个分布满足变量代换定理： z1=f(z0)⟹log⁡p(z1)=log⁡p(z0)−log⁡∣det⁡∂f∂z0∣\\mathrm{z}_{1}=f\\left(\\mathrm{z}_{0}\\right) \\Longrightarrow \\log p\\left(\\mathrm{z}_{1}\\right)=\\log p\\left(\\mathrm{z}_{0}\\right)-\\log \\left|\\operatorname{det} \\frac{\\partial f}{\\partial \\mathrm{z}_{0}}\\right| z1​=f(z0​)⟹logp(z1​)=logp(z0​)−log∣∣∣∣​det∂z0​∂f​∣∣∣∣​ 平面归一化流(NICE之后的一篇流模型的文章，这篇论文没看……)使用的变换： z(t+1)=z(t)+uh(w⊤z(t)+b),log⁡p(z(t+1))=log⁡p(z(t))−log⁡∣1+u⊤∂h∂z∣\\mathbf{z}(t+1)=\\mathbf{z}(t)+u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\log p(\\mathbf{z}(t+1))=\\log p(\\mathbf{z}(t))-\\log \\left|1+u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}}\\right| z(t+1)=z(t)+uh(w⊤z(t)+b),logp(z(t+1))=logp(z(t))−log∣∣∣∣​1+u⊤∂z∂h​∣∣∣∣​ 在流模型中，为使∂f/∂z\\partial f/\\partial z∂f/∂z 的雅可比行列式易于计算，通常是通过精心构建函数fff 来实现。并且fff 还需要是可逆的。而在这篇文章里发现，将离散的流模型换成连续流模型，可以极大的简化计算：不需要去计算∂f/∂z\\partial f/ \\partial z∂f/∂z 的行列式，只需要计算迹，并且不需要构建fff 可逆：fff 可以是任意函数，它是天然可逆的(常微分方程决定的函数只要满足唯一性，就一定是双射的)，因此fff 理论上可以是任何网络。 核心定理（梯度变元定理）： 证明过程见附录 于是我们将平面归一化流模型连续化： dz(t)dt=uh(w⊤z(t)+b),∂log⁡p(z(t))∂t=−u⊤∂h∂z(t)\\frac{d \\mathbf{z}(t)}{d t}=u h\\left(w^{\\top} \\mathbf{z}(t)+b\\right), \\quad \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-u^{\\top} \\frac{\\partial h}{\\partial \\mathbf{z}(t)} dtdz(t)​=uh(w⊤z(t)+b),∂t∂logp(z(t))​=−u⊤∂z(t)∂h​ 不同于求行列式的值，求迹还是一个连续函数，因此如果常微分方程dz/dtdz/dtdz/dt 是由一组函数的和给出的，那么对数概率密度也可以直接用迹的和表示： dz(t)dt=∑n=1Mfn(z(t)),dlog⁡p(z(t))dt=∑n=1Mtr⁡(∂fn∂z)\\frac{d \\mathbf{z}(t)}{d t}=\\sum_{n=1}^{M} f_{n}(\\mathbf{z}(t)), \\quad \\frac{d \\log p(\\mathbf{z}(t))}{d t}=\\sum_{n=1}^{M} \\operatorname{tr}\\left(\\frac{\\partial f_{n}}{\\partial \\mathbf{z}}\\right) dtdz(t)​=n=1∑M​fn​(z(t)),dtdlogp(z(t))​=n=1∑M​tr(∂z∂fn​​) 因此对于有M个隐藏状态的连续流模型来说，计算成本仅仅是O(M)\\mathcal{O}\\left(M\\right)O(M),而平面归一化流的计算成本是O(M3)\\mathcal{O}\\left(M^{3}\\right)O(M3) 。 NF和CNF的比较： 通过ODE对时间序列建模 zt0∼p(zt0)z_{t_{0}} \\sim p\\left(z_{t_{0}}\\right) zt0​​∼p(zt0​​) zt1,zt2,…,ztN=ODESolve(zt0,f,θf,t0,…,tN)z_{t_{1}}, z_{t_{2}}, \\ldots, z_{t_{N}} =ODESolve(z_{t_0},f,\\theta_f,t_0,\\ldots,t_N) zt1​​,zt2​​,…,ztN​​=ODESolve(zt0​​,f,θf​,t0​,…,tN​) eachxti∼p(x∣zti,θX)each \\quad x_{t_i} \\sim p(x|z_{t_i},\\theta_X) eachxti​​∼p(x∣zti​​,θX​) 具体而言，在给定初始状态 z0z_0z0​ 和观测时间 t0,…tNt_0,\\ldots t_Nt0​,…tN​ 的情况下，该模型计算潜在状态 zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 和输出 xt1…xtNx_{t_1} \\ldots x_{t_N}xt1​​…xtN​​。在实验部分，初始状态z0z_0z0​由RNN编码产生，潜在状态zt1…ztNz_{t_1} \\ldots z_{t_N}zt1​​…ztN​​ 由ODESolve产生，其中的fff 用神经网络训练，然后利用VAE的方式从潜在状态中生成数据。 实验：从采样点进行螺旋线重建 均方差比较： 附录 伴随法的证明 对于z(t)z(t)z(t) 给定常微分方程： dz(t)d(t)=f(z(t),t,θ)\\frac{dz(t)}{d(t)}=f(z(t),t,\\theta) d(t)dz(t)​=f(z(t),t,θ) L(z(t1))=L(∫t0t1f(z(t),t,θ)dt)=L( ODESolve(z (t0),f,t0,t1,θ))\\left.L\\left(\\mathbf{z}\\left(t_{1}\\right)\\right)=L\\left(\\int_{t_{0}}^{t_{1}} f(\\mathbf{z}(t), t, \\theta) d t\\right)=L\\left(\\text { ODESolve(z }\\left(t_{0}\\right), f, t_{0}, t_{1}, \\theta\\right)\\right) L(z(t1​))=L(∫t0​t1​​f(z(t),t,θ)dt)=L( ODESolve(z (t0​),f,t0​,t1​,θ)) 定义便随状态： a(t)=−∂L/∂z(t)a(t) = - \\partial L / \\partial \\mathbf{z}(t) a(t)=−∂L/∂z(t) 则： da(t)dt=−a(t)⊤∂f(z(t),t,θ)∂z(2)\\frac{d a(t)}{d t}=-a(t)^{\\top} \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}\\tag{2} dtda(t)​=−a(t)⊤∂z∂f(z(t),t,θ)​(2) 证明： z(t+ε)=∫tt+εf(z(t),t,θ)dt+z(t)=Tε(z(t),t)\\mathbf{z}(t+\\varepsilon)=\\int_{t}^{t+\\varepsilon} f(\\mathbf{z}(t), t, \\theta) d t+\\mathbf{z}(t)=T_{\\varepsilon}(\\mathbf{z}(t), t) z(t+ε)=∫tt+ε​f(z(t),t,θ)dt+z(t)=Tε​(z(t),t) 然后应用链式法则，有： dL∂z(t)=dLdz(t+ε)dz(t+ε)dz(t) or a(t)=a(t+ε)∂Tε(z(t),t)∂z(t)\\frac{d L}{\\partial \\mathbf{z}(t)}=\\frac{d L}{d \\mathbf{z}(t+\\varepsilon)} \\frac{d \\mathbf{z}(t+\\varepsilon)}{d \\mathbf{z}(t)} \\quad \\text { or } \\quad \\mathbf{a}(t)=\\mathbf{a}(t+\\varepsilon) \\frac{\\partial T_{\\varepsilon}(\\mathbf{z}(t), t)}{\\partial \\mathbf{z}(t)} ∂z(t)dL​=dz(t+ε)dL​dz(t)dz(t+ε)​ or a(t)=a(t+ε)∂z(t)∂Tε​(z(t),t)​ 利用导数定义，并进行泰勒展开进行化简计算： da(t)dt=lim⁡ε→0+a(t+ε)−a(t)ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)Tε(z(t))ε=lim⁡ε→0+a(t+ε)−a(t+ε)∂∂z(t)(z(t)+εf(z(t),t,θ)+O(ε2))ε=lim⁡ε→0+a(t+ε)−a(t+ε)(I+ε∂f(z(t),t,θ)θz(t)+O(ε2))ε=lim⁡ε→0+−εa(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε2)ε=lim⁡ε→0+−a(t+ε)∂f(z(t),t,θ)∂z(t)+O(ε)=−a(t)∂f(z(t),t,θ)∂z(t)\\begin{aligned} \\frac{d \\mathbf{a}(t)}{d t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)} T_{\\varepsilon}(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial}{\\partial \\mathbf{z}(t)}\\left(\\mathbf{z}(t)+\\varepsilon f(\\mathbf{z}(t), t, \\theta)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\mathbf{a}(t+\\varepsilon)-\\mathbf{a}(t+\\varepsilon)\\left(I+\\varepsilon \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\theta \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{-\\varepsilon \\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}\\left(\\varepsilon^{2}\\right)}{\\varepsilon} \\\\ &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}}-\\mathbf{a}(t+\\varepsilon) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)}+\\mathcal{O}(\\varepsilon) \\\\ &amp;=-\\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)} \\end{aligned} dtda(t)​​=ε→0+lim​εa(t+ε)−a(t)​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​Tε​(z(t))​=ε→0+lim​εa(t+ε)−a(t+ε)∂z(t)∂​(z(t)+εf(z(t),t,θ)+O(ε2))​=ε→0+lim​εa(t+ε)−a(t+ε)(I+εθz(t)∂f(z(t),t,θ)​+O(ε2))​=ε→0+lim​ε−εa(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε2)​=ε→0+lim​−a(t+ε)∂z(t)∂f(z(t),t,θ)​+O(ε)=−a(t)∂z(t)∂f(z(t),t,θ)​​ 对于θ\\thetaθ 和ttt 定义增强状态： ddt[zθt](t)=faug⁡([z,θ,t]):=[f([z,θ,t])01],aaug:=[aaθat],aθ(t):=dLdθ(t),at(t):=dLdt(t)\\frac{d}{d t}\\left[\\begin{array}{l} \\mathrm{z} \\\\ \\theta \\\\ t \\end{array}\\right](t)=f_{\\operatorname{aug}}([\\mathrm{z}, \\theta, t]):=\\left[\\begin{array}{c} f([\\mathrm{z}, \\theta, t]) \\\\ 0 \\\\ 1 \\end{array}\\right], \\mathbf{a}_{a u g}:=\\left[\\begin{array}{l} \\mathrm{a} \\\\ \\mathrm{a}_{\\theta} \\\\ \\mathrm{a}_{t} \\end{array}\\right], \\mathrm{a}_{\\theta}(t):=\\frac{d L}{d \\theta(t)}, \\mathrm{a}_{t}(t):=\\frac{d L}{d t(t)} dtd​⎣⎡​zθt​⎦⎤​(t)=faug​([z,θ,t]):=⎣⎡​f([z,θ,t])01​⎦⎤​,aaug​:=⎣⎡​aaθ​at​​⎦⎤​,aθ​(t):=dθ(t)dL​,at​(t):=dt(t)dL​ 其中θ\\thetaθ 和ttt 无关，即dθ(t)/dt=0,dt(t)/dt=1d\\theta(t) /dt=0,dt(t)/dt=1dθ(t)/dt=0,dt(t)/dt=1 计算雅可比行列式： ∂faug∂[z,θ,t]=[∂f∂z∂f∂θ∂f∂t000000]\\frac{\\partial f_{a u g}}{\\partial[\\mathbf{z}, \\theta, t]}=\\left[\\begin{array}{ccc} \\frac{\\partial f}{\\partial z} &amp; \\frac{\\partial f}{\\partial \\theta} &amp; \\frac{\\partial f}{\\partial t} \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right] ∂[z,θ,t]∂faug​​=⎣⎡​∂z∂f​00​∂θ∂f​00​∂t∂f​00​⎦⎤​ 直接将faugf_{aug}faug​ 和aauga_{aug}aaug​ 代入上一小节的伴随法公式： daaug(t)dt=−[a(t)aθ(t)at(t)]∂faug∂[z,θ,t](t)=−[a∂f∂za∂f∂θa∂f∂t](t)\\frac{d \\mathbf{a}_{a u g}(t)}{d t}=-\\left[\\begin{array}{lllll} \\mathbf{a}(t) &amp; \\mathbf{a}_{\\theta}(t) &amp; \\mathbf{a}_{t}(t) \\end{array}\\right] \\frac{\\partial f_{\\text {aug}}}{\\partial[\\mathbf{z}, \\theta, t]}(t)=-\\left[\\begin{array}{lll} \\mathbf{a} \\frac{\\partial f}{\\partial \\mathbf{z}} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial \\theta} &amp; \\mathbf{a} \\frac{\\partial f}{\\partial t} \\end{array}\\right](t) dtdaaug​(t)​=−[a(t)​aθ​(t)​at​(t)​]∂[z,θ,t]∂faug​​(t)=−[a∂z∂f​​a∂θ∂f​​a∂t∂f​​](t) 于是得到了最终的结论： dLdθ=∫tNt0a(t)∂f(z(t),t,θ)∂θdt\\frac{d L}{d \\theta}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\theta} d t dθdL​=∫tN​t0​​a(t)∂θ∂f(z(t),t,θ)​dt dLdtN=−a(tN)∂f(z(tN),tN,θ)∂tNdLdt0=∫tNt0a(t)∂f(z(t),t,θ)∂tdt\\frac{d L}{d t_{N}}=-\\mathbf{a}\\left(t_{N}\\right) \\frac{\\partial f\\left(\\mathbf{z}\\left(t_{N}\\right), t_{N}, \\theta\\right)}{\\partial t_{N}} \\quad \\frac{d L}{d t_{0}}=\\int_{t_{N}}^{t_{0}} \\mathbf{a}(t) \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial t} d t dtN​dL​=−a(tN​)∂tN​∂f(z(tN​),tN​,θ)​dt0​dL​=∫tN​t0​​a(t)∂t∂f(z(t),t,θ)​dt 梯度变元定理的证明 给定常微分方程： dz(t)d(t)=f(z(t),t)\\frac{dz(t)}{d(t)}=f(z(t),t) d(t)dz(t)​=f(z(t),t) fff 要求对zzz Lipschitz连续，对ttt 连续。则对数概率密度满足： ∂log⁡p(z(t))∂t=−tr⁡(dfdz(t))\\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t}=-\\operatorname{tr}\\left(\\frac{d f}{d \\mathbf{z}}(t)\\right) ∂t∂logp(z(t))​=−tr(dzdf​(t)) 证明： 首先类似上面伴随法证明的过程，将z(t+ε)z(t+\\varepsilon)z(t+ε) 表示为Tε(z(t))T_{\\varepsilon}(\\mathbf{z}(t))Tε​(z(t)) fff 要求对zzz Lipschitz连续，对ttt 连续。这是为了使方程满足Picard存在定理，使得解存在且唯一。 首先是要推导出 ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ϵ∂∂zTε(z(t)))\\frac{\\partial \\log p(z(t))}{\\partial t}=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\epsilon} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(z(t))\\right) ∂t∂logp(z(t))​=−tr(ε→0+lim​∂ϵ∂​∂z∂​Tε​(z(t))) 过程： ∂log⁡p(z(t))∂t=lim⁡ε→0+log⁡p(z(t))−log⁡∣det⁡∂∂zTε(z(t))∣−log⁡p(z(t))ε=−lim⁡ε→0+log⁡∣det⁡∂∂zTε(z(t))∣ε=−lim⁡ε→0+∂∂εlog⁡∣det⁡∂∂zTε(z(t))∣∂∂εε=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣∣det⁡∂∂zTε(z(t))∣(∂log⁡(z)∂z∣z=1=1)=−(lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣)⏟bounded (lim⁡ε→0+1∣det⁡∂∂zTε(z(t))∣)=−lim⁡ε→0+∂∂ε∣det⁡∂∂zTε(z(t))∣\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log p(\\mathbf{z}(t))-\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|-\\log p(\\mathbf{z}(t))}{\\varepsilon} \\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon} \\log \\left|\\operatorname{det} \\frac{\\partial}{\\partial z} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\frac{\\partial}{\\partial \\varepsilon} \\varepsilon}\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|} \\qquad \\qquad \\quad \\left(\\left.\\frac{\\partial \\log (\\mathbf{z})}{\\partial \\mathbf{z}}\\right|_{\\mathbf{z}=1}=1\\right)\\\\ &amp;=-\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0+} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|\\right)}_{\\text {bounded }}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{1}{\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right|}\\right)\\\\ &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left|\\operatorname{det} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right| \\end{aligned} ∂t∂logp(z(t))​​=ε→0+lim​εlogp(z(t))−log∣∣​det∂z∂​Tε​(z(t))∣∣​−logp(z(t))​=−ε→0+lim​εlog∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∂ε∂​ε∂ε∂​log∣∣​det∂z∂​Tε​(z(t))∣∣​​=−ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​∂ε∂​∣∣​det∂z∂​Tε​(z(t))∣∣​​(∂z∂log(z)​∣∣∣∣​z=1​=1)=−bounded (ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​)​​(ε→0+lim​∣∣​det∂z∂​Tε​(z(t))∣∣​1​)=−ε→0+lim​∂ε∂​∣∣∣∣​det∂z∂​Tε​(z(t))∣∣∣∣​​ 第一步用到的是流模型中的公式(本质是概率密度上的雅可比公式)，后面仅用到洛必达法则、链式法则等简单技巧。 然后应用雅可比公式： ∂log⁡p(z(t))∂t=−lim⁡ε→0+tr⁡(adj⁡(∂∂zTε(z(t)))∂∂ε∂∂zTε(z(t)))=−tr⁡((lim⁡ε→0+adj⁡(∂∂zTε(z(t))))⏟=I(lim⁡ε→0+∂∂ε∂∂zTε(z(t))))=−tr⁡(lim⁡ε→0+∂∂ε∂∂zTε(z(t)))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{tr}\\left(\\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\underbrace{\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\operatorname{adj}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right)}_{=I}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}} T_{\\varepsilon}(\\mathbf{z}(t))\\right) \\end{aligned} ∂t∂logp(z(t))​​=−ε→0+lim​tr(adj(∂z∂​Tε​(z(t)))∂ε∂​∂z∂​Tε​(z(t)))=−tr⎝⎜⎜⎛​=I(ε→0+lim​adj(∂z∂​Tε​(z(t))))​​(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))⎠⎟⎟⎞​=−tr(ε→0+lim​∂ε∂​∂z∂​Tε​(z(t)))​ 雅可比公式是： ddtdet(A(t))=tr(adj(A(t))ddtA(t))\\frac{d}{dt}det(A(t))=tr(adj(A(t))\\frac{d}{dt}A(t)) dtd​det(A(t))=tr(adj(A(t))dtd​A(t)) 最后进行泰勒展开即可： ∂log⁡p(z(t))∂t=−tr⁡(lim⁡ε→0+∂∂ε∂∂z(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+∂∂ε(I+∂∂zεf(z(t),t)+O(ε2)+O(ε3)+…))=−tr⁡(lim⁡ε→0+(∂∂zf(z(t),t)+O(ε)+O(ε2)+…))=−tr⁡(∂∂zf(z(t),t))\\begin{aligned} \\frac{\\partial \\log p(\\mathbf{z}(t))}{\\partial t} &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon} \\frac{\\partial}{\\partial \\mathbf{z}}\\left(\\mathbf{z}+\\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}} \\frac{\\partial}{\\partial \\varepsilon}\\left(I+\\frac{\\partial}{\\partial \\mathbf{z}} \\varepsilon f(\\mathbf{z}(t), t)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\mathcal{O}\\left(\\varepsilon^{3}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\lim _{\\varepsilon \\rightarrow 0^{+}}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)+\\mathcal{O}(\\varepsilon)+\\mathcal{O}\\left(\\varepsilon^{2}\\right)+\\ldots\\right)\\right) \\\\ &amp;=-\\operatorname{tr}\\left(\\frac{\\partial}{\\partial \\mathbf{z}} f(\\mathbf{z}(t), t)\\right) \\end{aligned} ∂t∂logp(z(t))​​=−tr(ε→0+lim​∂ε∂​∂z∂​(z+εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​∂ε∂​(I+∂z∂​εf(z(t),t)+O(ε2)+O(ε3)+…))=−tr(ε→0+lim​(∂z∂​f(z(t),t)+O(ε)+O(ε2)+…))=−tr(∂z∂​f(z(t),t))​","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"neural ode","slug":"neural-ode","permalink":"/tags/neural-ode/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"gan,vae和flow","slug":"GAN VAE and Flow","date":"2020-02-15T13:19:00.000Z","updated":"2020-07-01T11:08:38.809Z","comments":true,"path":"2020/02/15/GAN VAE and Flow/","link":"","permalink":"/2020/02/15/GAN VAE and Flow/","excerpt":"","text":"前言 GAN，VAE和FLOW的目标是一致的——希望构建一个从隐变量ZZZ生成目标数据XXX的模型，其中先验分布P(z)P(z)P(z)通常被设置为高斯分布。我们希望找到一个变换函数f(x)f(x)f(x)，他能建立一个从zzz到xxx的映射：f:z→xf:z\\to xf:z→x，然后在P(Z)P(Z)P(Z)中随机采样一个点z′z&#x27;z′，通过映射fff，就可以找到一个新的样本点x′x&#x27;x′。 举个栗子： 如何将均匀分布U[0,1]U[0,1]U[0,1]映射成正态分布N(0,1)N(0,1)N(0,1)？ 将X∼U[0,1]X \\sim U[0,1]X∼U[0,1]经过函数Y=f(x)Y = f(x)Y=f(x)映射之后，就有Y∼N(0,1)Y\\sim N(0,1)Y∼N(0,1)了那么[x,x+dx][x,x+dx][x,x+dx]和[y,y+dy][y,y+dy][y,y+dy]两个区间上的概率应该相等，即： ρ(x)dx=12πexp⁡(−y22)dy\\rho(x) d x=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{y^{2}}{2}\\right) d y ρ(x)dx=2π​1​exp(−2y2​)dy 对其进行积分，有： ∫0xρ(t)dt=∫−∞y12πexp⁡(−t22)dt=Φ(y)\\int_{0}^{x} \\rho(t) d t=\\int_{-\\infty}^{y} \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{t^{2}}{2}\\right) d t=\\Phi(y) ∫0x​ρ(t)dt=∫−∞y​2π​1​exp(−2t2​)dt=Φ(y) y=Φ−1(∫0xρ(t)dt)=f(x)y=\\Phi^{-1}\\left(\\int_{0}^{x} \\rho(t) d t\\right)=f(x) y=Φ−1(∫0x​ρ(t)dt)=f(x) 可以看到Y=f(X)Y = f(X)Y=f(X)的解是存在的，但很复杂，无法用初等函数进行显示的表示，因此在大多数情况下，我们都是通过神经网络来拟合这个函数。 假设我们现在已经有一个映射fff，我们如何衡量映射fff构造出来的数据集f(z1),f(z2),...,f(zn)f(z_1),f(z_2),...,f(z_n)f(z1​),f(z2​),...,f(zn​)，是否和目标数据XXX分布相同？(注：KL和JS距离根据两个概率分布的表达式计算分布的相似度，而我们现在只有从构造的分布采样的数据和真实分布采样的数据，而离散化的KL和JS距离因为图像维度问题，计算量非常大)。在这里GAN采用了一个暴力的办法：训练一个判别器作为两者相似性的度量，而VAE(变分自编码器)和FLOW(流模型)在最大化最大似然。 VAE(变分自编码器) VAE的基本思路 对于连续随机变量，概率分布PPP和QQQ，KL散度(又称相对熵)的定义为： DKL(P∥Q)=∫−∞∞p(x)ln⁡p(x)q(x)dx=Ex∼P(x)[logP(x)−logQ(x)]D_{\\mathrm{KL}}(P \\| Q)=\\int_{-\\infty}^{\\infty} p(x) \\ln \\frac{p(x)}{q(x)} \\mathrm{d} x=E_{x \\sim P(x)}[logP(x)-logQ(x)] DKL​(P∥Q)=∫−∞∞​p(x)lnq(x)p(x)​dx=Ex∼P(x)​[logP(x)−logQ(x)] 给定一个概率分布DDD,已知其概率密度函数(连续分布)或概率质量函数(离散分布)为fDf_DfD​，以及一个分布参数θ\\thetaθ，我们可以从这个分布中抽出一个具有nnn个值的采样X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，利用fDf_DfD​计算出其似然函数： L(θ∣x1,…,xn)=fθ(x1,…,xn)\\mathbf{L}\\left(\\theta | x_{1}, \\ldots, x_{n}\\right)=f_{\\theta}\\left(x_{1}, \\ldots, x_{n}\\right) L(θ∣x1​,…,xn​)=fθ​(x1​,…,xn​) 若DDD是离散分布，fθf_{\\theta}fθ​即是在参数为θ\\thetaθ时观测到这一采样的概率。若其是连续分布，fθf_{\\theta}fθ​则为X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​联合分布的概率密度函数在观测值处的取值。一旦我们获得X1,X2,...,XnX_1,X_2,...,X_nX1​,X2​,...,Xn​，我们就能求得一个关于θ\\thetaθ的估计。最大似然估计会寻找关于的最可能的值（即，在所有可能的取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的值即称为的最大似然估计。由定义，最大似然估计是样本的函数。 注：下面忽略积分号和求和的差异 VAE做最大似然估计，也就是要最大化概率： P(X)=∑iP(X∣zi;θ)P(zi)P(X)=\\sum_{i} P\\left(X | z_{i} ; \\theta\\right) P\\left(z_{i}\\right) P(X)=i∑​P(X∣zi​;θ)P(zi​) 一般选择P(Z)P(Z)P(Z)服从一个高斯分布，而p(X∣z)p(X|z)p(X∣z)可以是任意分布，例如条件高斯分布或狄拉克分布，理论上讲，这个积分形式的分布可以拟合任意分布。 实际上，最大似然估计与最小化两个分布之间的KL距离是等价的: argminKL(p(x)∥q(x))=argmin∫p(x)logp(x)q(x)dx=argmax∫p(x)q(x)dx=argmaxEx∼p(x)q(x)\\begin{aligned} argmin{KL(p(x)\\|q(x))} &amp;= argmin \\int p(x)log \\frac{p(x)}{q(x)}dx \\\\ &amp;= argmax \\int p(x)q(x)dx \\\\ &amp;=argmaxE_{x \\sim p(x)}q(x) \\end{aligned} argminKL(p(x)∥q(x))​=argmin∫p(x)logq(x)p(x)​dx=argmax∫p(x)q(x)dx=argmaxEx∼p(x)​q(x)​ 但是这里的P(X)P(X)P(X)是积分形式的，很难进行计算。VAE从让人望而生畏的变分和贝叶斯理论出发，推导出了一个很接地气的公式： log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)](1)\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] \\tag{1} logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)](1) VAE并没有选择直接去优化P(X)P(X)P(X)，而是选择去优化他的一个变分下界（公式1右端）。 而VAE的自编码器性质也从这个公式里开始体现出来：我们可以将D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]视作编码器的优化，使由真实数据编码出的隐变量分布Q(z∣X)Q(z|X)Q(z∣X)去尽量近似P(z)P(z)P(z)（标准高斯分布），而将Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]视作解码器的优化，使得服从分布QQQ的隐变量zzz解码出的xxx尽可能地服从真是数据分布，而将D[Q(z∣X)∥P(z∣X)]\\mathcal{D}[Q(z | X) \\| P(z | X)]D[Q(z∣X)∥P(z∣X)]视作误差项。 但VAE也因为它并没有直接去优化P(X)P(X)P(X)，而选择去优化它的变分下界，使得他只是一个近似模型，无法保证良好的生成效果。 VAE的优化过程 首先要确定概率密度Q(z∣X)Q(z|X)Q(z∣X)的形式，一般选择正态分布，即N(μ,σ2)\\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)N(μ,σ2)，其中μ(X;θμ),σ2(X;θσ)\\mu\\left(X ; \\theta_{\\mu}\\right) , \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)μ(X;θμ​),σ2(X;θσ​)通过两个神经网络(编码器)训练出来。公式中的D[Q(z∣X)∥P(z)]\\mathcal{D}[Q(z | X) \\| P(z)]D[Q(z∣X)∥P(z)]变为D[N(μ(X;θμ),σ2(X;θσ))∥N(0,I)]D\\left[\\mathcal{N}\\left(\\mu\\left(X ; \\theta_{\\mu}\\right), \\sigma^{2}\\left(X ; \\theta_{\\sigma}\\right)\\right) \\| \\mathcal{N}(0, I)\\right]D[N(μ(X;θμ​),σ2(X;θσ​))∥N(0,I)]，这个时候就可以通过两个正态分布的KL散度的计算公式来计算这一项。 对于第一项Ez∼Q[log⁡P(X∣z)]E_{z \\sim Q}[\\log P(X | z)]Ez∼Q​[logP(X∣z)]，对于一个batch来说，可以在QQQ中采样，然后将单个样本的log⁡P(X∣z)\\log P(X|z)logP(X∣z)求和取平均数作为期望的估计。但这样出现一个问题：把Q(z∣X)Q(z|X)Q(z∣X)弄丢了，也就是每次训练的时候梯度不传进QQQ里，论文里采用了一个称为重参数化技巧(reparamenterization trick)的方法，如图： 至此，整个VAE网络就可以训练了。 公式推导部分 D[Q(z∣X)∣∣P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)]=Ez∼Q[log⁡Q(z∣X)−log⁡P(z∣X)−log⁡P(X)]+log⁡P(X)\\begin{aligned} \\mathcal{D}[Q(z|X)||P(z|X)] &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X)] \\\\ &amp;= E_{z \\sim Q}[\\log Q(z|X) - \\log P(z|X) - \\log P(X)] + \\log P(X) \\end{aligned} D[Q(z∣X)∣∣P(z∣X)]​=Ez∼Q​[logQ(z∣X)−logP(z∣X)]=Ez∼Q​[logQ(z∣X)−logP(z∣X)−logP(X)]+logP(X)​ 移项得 log⁡P(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q[log⁡P(X∣z)]−D[Q(z∣X)∥P(z)]\\log P(X)-\\mathcal{D}[Q(z | X) \\| P(z | X)]=E_{z \\sim Q}[\\log P(X | z)]-\\mathcal{D}[Q(z | X) \\| P(z)] logP(X)−D[Q(z∣X)∥P(z∣X)]=Ez∼Q​[logP(X∣z)]−D[Q(z∣X)∥P(z)] GAN 模型构建 由于大家都对GAN比较熟悉，本文直接从变分推断的角度去理解GAN。 不同于VAE将P(X∣z)P(X|z)P(X∣z)选为高斯分布，GAN的选择是： P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dzP(x | z)=\\delta(x-G(z)), \\quad P(x)=\\int P(x | z) P(z) d z P(x∣z)=δ(x−G(z)),P(x)=∫P(x∣z)P(z)dz 其中δ(x)\\delta (x)δ(x)是狄拉克函数，G(z)G(z)G(z)为生成器网络。 在VAE中z被当作是一个隐变量，但在GAN中，狄拉克函数意味着单点分布，即x和z为一一对应的关系。于是在GAN中z没有被当作隐变量处理(不需要考虑后验分布P(z∣x)P(z|x)P(z∣x)) 判别器的理解： 在GAN中引入了一个二元的隐变量y来构成联合分布，其中p~(x)\\tilde{p}(x)p~​(x) 为真实样本的分布： q(x,y)={p~(x)p1,y=1p(x)p0,y=0q(x, y)=\\left\\{\\begin{array}{l} {\\tilde{p}(x) p_{1}, y=1} \\\\ {p(x) p_{0}, y=0} \\end{array}\\right. q(x,y)={p~​(x)p1​,y=1p(x)p0​,y=0​ 这里y是图像的真实标签，当图片为真实图片时，y=1，当图片是生成图片时，y=0。 其中p0,p1p_0, p_1p0​,p1​代表权重，在下面讨论中我们直接取p0=p1=1/2p_0=p_1=1/2p0​=p1​=1/2 另一方面，我们需要使判别器的判别结果尽可能真实，设p(x,y)=p(y∣x)p~(x)p(x,y)=p(y|x)\\tilde{p}(x)p(x,y)=p(y∣x)p~​(x)，p(y∣x)p(y|x)p(y∣x)为一个条件伯努利分布(判别器的判别结果)。优化目标是KL(q(x,y)∣∣p(x,y))KL(q(x,y)||p(x,y))KL(q(x,y)∣∣p(x,y))： KL(q(x,y)∥p(x,y))=∫p~(x)p1log⁡p~(x)p1p(1∣x)p~(x)dx+∫p(x)p0log⁡p(x)p0p(0∣x)p~(x)dx∼∫p~(x)log⁡12p(1∣x)dx+∫p(x)log⁡p(x)2p(0∣x)p~(x)dx=−Ex∼p~(x)[log⁡2p(1∣x)]−Ex∼p(x)[log⁡2p(0∣x)]+KL(p(x)∣∣p~(x))\\begin{aligned} K L(q(x, y) \\| p(x, y)) &amp;=\\int \\tilde{p}(x) p_{1} \\log \\frac{\\tilde{p}(x) p_{1}}{p(1 | x) \\tilde{p}(x)} d x+\\int p(x) p_{0} \\log \\frac{p(x) p_{0}}{p(0 | x) \\tilde{p}(x)} d x \\\\ &amp; \\sim \\int \\tilde{p}(x) \\log \\frac{1}{2p(1 | x)} d x+\\int p(x) \\log \\frac{p(x)}{2p(0 | x) \\tilde{p}(x)} d x\\\\ &amp; = -E_{x \\sim \\tilde{p}(x)}[\\log 2p(1|x)]-E_{x \\sim p(x)}[\\log 2p(0|x)]+KL(p(x)||\\tilde{p}(x)) \\end{aligned} KL(q(x,y)∥p(x,y))​=∫p~​(x)p1​logp(1∣x)p~​(x)p~​(x)p1​​dx+∫p(x)p0​logp(0∣x)p~​(x)p(x)p0​​dx∼∫p~​(x)log2p(1∣x)1​dx+∫p(x)log2p(0∣x)p~​(x)p(x)​dx=−Ex∼p~​(x)​[log2p(1∣x)]−Ex∼p(x)​[log2p(0∣x)]+KL(p(x)∣∣p~​(x))​ 一旦成功优化，就有q(x,y)→p(x,y)q(x,y)\\to p(x,y)q(x,y)→p(x,y)，对于x求边缘概率分布，有： 12p~(x)+12p(x)→p(1∣x)p~(x)+p(0∣x)p~(x)=p~(x)\\frac{1}{2}\\tilde{p}(x)+\\frac{1}{2}p(x)\\to p(1|x)\\tilde{p}(x)+p(0|x)\\tilde{p}(x)=\\tilde{p}(x) 21​p~​(x)+21​p(x)→p(1∣x)p~​(x)+p(0∣x)p~​(x)=p~​(x) 即： p(x)→p~(x)p(x)\\to \\tilde{p}(x) p(x)→p~​(x) 这就完成了对模型的构建。 目标优化 现在我们有优化目标：p(1∣x)p(1|x)p(1∣x)和G(z)G(z)G(z)，分别是判别器(p(y∣x)p(y|x)p(y∣x)服从条件伯努利分布，可以直接由p(1∣x)p(1|x)p(1∣x)确定)和生成器(p(x)p(x)p(x)由G(z)G(z)G(z)决定)。类似EM算法，我们进行交替优化：先固定G(z)G(z)G(z),这也意味着p(x)p(x)p(x)固定了，然后优化p(y∣x)p(y|x)p(y∣x)，优化目标为： D=arg⁡min⁡D{−Ex∼p~(x)[log⁡2D(x)]−Ex∼p(x)[log⁡2(1−D(x))]}D=\\underset{D}{\\arg \\min }\\{-E_{x \\sim \\tilde{p}(x)}[\\log 2D(x)]-\\mathbb{E}_{x \\sim p(x)}[\\log 2(1-D(x))]\\} D=Dargmin​{−Ex∼p~​(x)​[log2D(x)]−Ex∼p(x)​[log2(1−D(x))]} 当生成器固定时，p(x)p(x)p(x) 也固定，分别记p~(x),p(x),D(x)\\tilde{p}(x),p(x),D(x)p~​(x),p(x),D(x) 为a,b,ta,b,ta,b,t ,忽略常数项，优化目标变为： D=argmax∫p~(x)log⁡t+p(x)log⁡(1−t)dxD=argmax \\int \\tilde{p}(x)\\log t + p(x) \\log(1-t)dx D=argmax∫p~​(x)logt+p(x)log(1−t)dx 求导算出其理论最优解为t=D(x)=p~(x)p~(x)+p0(x)t=D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}t=D(x)=p~​(x)+p0(x)p~​(x)​ 然后固定D(x)D(x)D(x)来优化G(x)G(x)G(x)，相关loss为： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dxG=\\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x G=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx 假设D(x)D(x)D(x)有足够的拟合能力，当D(x)=p~(x)p~(x)+p0(x)D(x)=\\frac{\\tilde{p}(x)}{\\tilde{p}(x)+p^0(x)}D(x)=p~​(x)+p0(x)p~​(x)​时，有 KL(q(x,y)∥p0(x,y))=∫p~(x)log⁡12D(x)dx+∫p0(x)log⁡p0(x)2(1−D(x))p~(x)dx=∫p~(x)log⁡p~(x)+p0(x)2p~(x)+p0(x)log⁡p~(x)+p0(x)2p~(x)=KL(p~(x)+p0(x)∣∣2p~(x))\\begin{aligned} K L(q(x, y) \\| p^0(x, y)) &amp;= \\int \\tilde{p}(x) \\log \\frac{1}{2D(x)} d x+\\int p^0(x) \\log \\frac{p^0(x)}{2(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\int\\tilde{p}(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}+p^0(x) \\log \\frac{\\tilde{p}(x)+p^0(x)}{2\\tilde{p}(x)}\\\\ &amp;= KL(\\tilde{p}(x) +p^0(x)||2\\tilde{p}(x) ) \\end{aligned} KL(q(x,y)∥p0(x,y))​=∫p~​(x)log2D(x)1​dx+∫p0(x)log2(1−D(x))p~​(x)p0(x)​dx=∫p~​(x)log2p~​(x)p~​(x)+p0(x)​+p0(x)log2p~​(x)p~​(x)+p0(x)​=KL(p~​(x)+p0(x)∣∣2p~​(x))​ 在优化判别器时，p0(x)p^0(x)p0(x)应该为上一阶段生成器优化的p(x)p(x)p(x) 。将这个D(x)D(x)D(x)代入生成器的相关loss： G=arg⁡min⁡G∫p(x)log⁡p0p(x)(1−D(x))p~(x)dx=arg⁡min⁡G∫p(x)log⁡p(x)2D(x)p0(x)dx=arg⁡min⁡G[−Ex∼p(x)2D(x)+KL(p(x)∣∣p0(x))]=arg⁡min⁡G[−Ex∼p(x)2D(G(z))+KL(p(x)∣∣p0(x))]\\begin{aligned} G &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{p_0 p(x)}{(1-D(x)) \\tilde{p}(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min } \\int p(x) \\log \\frac{ p(x)}{2D(x) p^0(x)} d x\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(x)+KL(p(x)||p^0(x))]\\\\ &amp;= \\underset{G}{\\arg \\min }[-E_{x \\sim p(x)}2D(G(z))+KL(p(x)||p^0(x))] \\end{aligned} G​=Gargmin​∫p(x)log(1−D(x))p~​(x)p0​p(x)​dx=Gargmin​∫p(x)log2D(x)p0(x)p(x)​dx=Gargmin​[−Ex∼p(x)​2D(x)+KL(p(x)∣∣p0(x))]=Gargmin​[−Ex∼p(x)​2D(G(z))+KL(p(x)∣∣p0(x))]​ 可以看到，此时的第一项−Ex∼p(x)2D(G(z))-E_{x \\sim p(x)}2D(G(z))−Ex∼p(x)​2D(G(z))就是标准的GAN所采用的loss之一。而我们知道，目前标准的GAN生成器的loss都不包含KL(p(x)∣∣p0(x))KL(p(x)||p^0(x))KL(p(x)∣∣p0(x))，这实际上造成了loss的不完备。 第二个loss是在限制要求新的生成器跟旧的生成器生成结果不能差别太大 ，也就是生成器不能剧烈变化。在loss不完备的情况下，假设有一个优化算法总能找到G(z)G(z)G(z)的理论最优解、并且G(z)G(z)G(z)具有无限的拟合能力，那么G(z)G(z)G(z)只需要生成唯一一个使得D(x)D(x)D(x)最大的样本（不管输入的zzz是什么），这就是模型坍缩。模型塌缩的视频(需要梯子)。 然后对第二项进行估算，得到一个可以在实验中使用的正则项： 记po(x)=qθ−Δθ(x),p(x)=qθ(x)p^{o}(x)=q_{\\theta-\\Delta \\theta}(x), \\quad p(x)=q_{\\theta}(x)po(x)=qθ−Δθ​(x),p(x)=qθ​(x)，其中Δθ\\Delta \\thetaΔθ为生成器的参数变化，对qo(x)=qθ−Δθ(x)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)qo(x)=qθ−Δθ​(x)做泰勒展开，有： qo(x)=qθ−Δθ(x)=qθ(x)−Δθ⋅∇θqθ(x)+O((Δθ)2)q^{o}(x)=q_{\\theta-\\Delta \\theta}(x)=q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)+O\\left((\\Delta \\theta)^{2}\\right) qo(x)=qθ−Δθ​(x)=qθ​(x)−Δθ⋅∇θ​qθ​(x)+O((Δθ)2) KL(q(x)∥qo(x))≈∫qθ(x)log⁡qθ(x)qθ(x)−Δθ⋅∇θqθ(x)dx=−∫qθ(x)log⁡[1−Δθ⋅∇θqθ(x)qθ(x)]dx≈−∫qθ(x)[−Δθ⋅∇θqθ(x)qθ(x)−(Δθ⋅∇θqθ(x)qθ(x))2]dx=Δθ⋅∇θ∫qθ(x)dx+(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx=(Δθ)2⋅∫(∇θqθ(x))22qθ(x)dx≈(Δθ⋅c)2\\begin{aligned} K L\\left(q(x) \\| q^{o}(x)\\right) &amp; \\approx \\int q_{\\theta}(x) \\log \\frac{q_{\\theta}(x)}{q_{\\theta}(x)-\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)} d x \\\\ &amp;=-\\int q_{\\theta}(x) \\log \\left[1-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right] d x \\\\ &amp; \\approx-\\int q_{\\theta}(x)\\left[-\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}-\\left(\\frac{\\Delta \\theta \\cdot \\nabla_{\\theta} q_{\\theta}(x)}{q_{\\theta}(x)}\\right)^{2}\\right] d x \\\\ &amp;=\\Delta \\theta \\cdot \\nabla_{\\theta} \\int q_{\\theta}(x) d x+(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp;=(\\Delta \\theta)^{2} \\cdot \\int \\frac{\\left(\\nabla_{\\theta} q_{\\theta}(x)\\right)^{2}}{2 q_{\\theta}(x)} d x \\\\ &amp; \\approx(\\Delta \\theta \\cdot c)^{2} \\end{aligned} KL(q(x)∥qo(x))​≈∫qθ​(x)logqθ​(x)−Δθ⋅∇θ​qθ​(x)qθ​(x)​dx=−∫qθ​(x)log[1−qθ​(x)Δθ⋅∇θ​qθ​(x)​]dx≈−∫qθ​(x)[−qθ​(x)Δθ⋅∇θ​qθ​(x)​−(qθ​(x)Δθ⋅∇θ​qθ​(x)​)2]dx=Δθ⋅∇θ​∫qθ​(x)dx+(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx=(Δθ)2⋅∫2qθ​(x)(∇θ​qθ​(x))2​dx≈(Δθ⋅c)2​ 上式中应用了log⁡(1+x)\\log(1+x)log(1+x)的泰勒展开式以及求导和积分可互换、可积分的假设。上面的粗略估计表明，生成器的参数不能变化太大。而我们用的是基于梯度下降的优化算法，所以Δθ\\Delta \\thetaΔθ正比于梯度，因此标准GAN训练时的很多trick，比如梯度裁剪、用Adam优化器、用BN，都可以解释得通了，它们都是为了稳定梯度，使得Δθ\\Delta \\thetaΔθ不至于过大，同时，G(z)G(z)G(z)的迭代次数也不能过多，因为过多同样会导致Δθ\\Delta \\thetaΔθ过大。 正则项 考虑如何添加正则项以改进GAN的稳定性： 直接对KL(q(x)∥qo(x))K L\\left(q(x) \\| q^{o}(x)\\right)KL(q(x)∥qo(x))进行估算是很困难的，但是我们上面提到q(z∣x)q(z|x)q(z∣x)和qo(z∣x)q^o(z|x)qo(z∣x)是狄拉克分布，而狄拉克分布可以看作方差为0的高斯分布，于是考虑用KL(q(x,z)∥qo(x,z))K L\\left(q(x,z) \\| q^{o}(x,z)\\right)KL(q(x,z)∥qo(x,z))进行估算： KL(q(x,z)∥q~(x,z))=∬q(x∣z)q(z)log⁡q(x∣z)q(z)q~(x∣z)q(z)dxdz=∬δ(x−G(z))q(z)log⁡δ(x−G(z))δ(x−Go(z))dxdz=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\iint q(x | z) q(z) \\log \\frac{q(x | z) q(z)}{\\tilde{q}(x | z) q(z)} d x d z \\\\ &amp;=\\iint \\delta(x-G(z)) q(z) \\log \\frac{\\delta(x-G(z))}{\\delta\\left(x-G^{o}(z)\\right)} d x d z \\\\ &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∬q(x∣z)q(z)logq~​(x∣z)q(z)q(x∣z)q(z)​dxdz=∬δ(x−G(z))q(z)logδ(x−Go(z))δ(x−G(z))​dxdz=∫q(z)logδ(G(z)−Go(z))δ(0)​dz​ 将狄拉克分布可以看作方差为0的高斯分布,并代入： δ(x)=lim⁡σ→01(2πσ2)d/2exp⁡(−x22σ2)\\delta(x)=\\lim _{\\sigma \\rightarrow 0} \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{d / 2}} \\exp \\left(-\\frac{x^{2}}{2 \\sigma^{2}}\\right) δ(x)=σ→0lim​(2πσ2)d/21​exp(−2σ2x2​) KL(q(x,z)∥q~(x,z))=∫q(z)log⁡δ(0)δ(G(z)−Go(z))dz=lim⁡σ→0∫q(x)log⁡[1/exp⁡(−(G(z)−G0(z))22σ2)]dx=lim⁡σ→0∫q(x)(−(G(z)−G0(z))22σ2)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz\\begin{aligned} K L(q(x, z) \\| \\tilde{q}(x, z)) &amp;=\\int q(z) \\log \\frac{\\delta(0)}{\\delta\\left(G(z)-G^{o}(z)\\right)} d z \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\log \\left[ 1/{\\exp \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)} \\right]dx \\\\ &amp;= \\lim _{\\sigma \\rightarrow 0} \\int q(x) \\left(-\\frac{(G(z)-G^0(z))^{2}}{2 \\sigma^{2}}\\right)dx \\\\ &amp; \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z \\end{aligned} KL(q(x,z)∥q~​(x,z))​=∫q(z)logδ(G(z)−Go(z))δ(0)​dz=σ→0lim​∫q(x)log[1/exp(−2σ2(G(z)−G0(z))2​)]dx=σ→0lim​∫q(x)(−2σ2(G(z)−G0(z))2​)dx∼λ∫q(z)∥G(z)−Go(z)∥2dz​ 于是有 KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dzK L\\left(q(x) \\| q^{o}(x)\\right) \\sim \\lambda \\int q(z)\\left\\|G(z)-G^{o}(z)\\right\\|^{2} d z KL(q(x)∥qo(x))∼λ∫q(z)∥G(z)−Go(z)∥2dz 从而完整的生成器loss可以选择为 Ez∼q(z)[−log⁡D(G(z))+λ∥G(z)−Go(z)∥2]\\mathbb{E}_{z \\sim q(z)}\\left[-\\log D(G(z))+\\lambda\\left\\|G(z)-G^{o}(z)\\right\\|^{2}\\right] Ez∼q(z)​[−logD(G(z))+λ∥G(z)−Go(z)∥2] 实验结果 FLOW 基本思路：直接硬算积分式 ∫zp(x∣z)p(z)dz\\int_{z} p(x | z) p(z) d z ∫z​p(x∣z)p(z)dz 流模型有一个非常与众不同的特点是，它的转换通常是可逆的。也就是说，流模型不 仅能找到从 A 分布变化到 B 分布的网络通路，并且该通路也能让 B 变化到 A，简言之流模 型找到的是一条 A、B 分布间的双工通路。当然，这样的可逆性是具有代价的——A、B 的 数据维度必须是一致的。 A、B 分布间的转换并不是轻易能做到的，流模型为实现这一点经历了三个步骤：最初 的 NICE 实现了从 A 分布到高斯分布的可逆求解；后来 RealNVP 实现了从 A 分布到条件非 高斯分布的可逆求解；而最新的 GLOW，实现了从 A 分布到 B 分布的可逆求解，其中 B 分 布可以是与 A 分布同样复杂的分布，这意味着给定两堆图片，GLOW 能够实现这两堆图片 间的任意转换。 NICE 两个一维分布之间的转化参考前言中的栗子，下面考虑高维分布： 类似一维分布，两个分布在映射前后的相同区域应该有相同的概率。 p(x′)∣det⁡(Jf)∣=π(z′)p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|=\\pi\\left(z^{\\prime}\\right) p(x′)∣det(Jf​)∣=π(z′) 其中JfJ_fJf​为雅可比行列式，函数fff将zzz上的分布变换到xxx上的分布。 根据雅可比行列式的逆运算，同样有： p(x′)=π(z′)∣det⁡(Jf−1)∣p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right| p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​ 至此，我们得到了一个比较重要的结论：如果 zzz 与 xxx 分别满足两种分布，并且 zzz 通过 函数 fff 能够转变为 xxx，那么 zzz 与 xxx 中的任意一组对应采样点 𝑧′𝑧′z′ 与 𝑥′𝑥′x′ 之间的关系为： {π(z′)=p(x′)∣det⁡(Jf)∣p(x′)=π(z′)∣det⁡(Jf−1)∣\\left\\{\\begin{array}{c} {\\pi\\left(z^{\\prime}\\right)=p\\left(x^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|} \\\\ {p\\left(x^{\\prime}\\right)=\\pi\\left(z^{\\prime}\\right)\\left|\\operatorname{det}\\left(J_{f^{-1}}\\right)\\right|} \\end{array}\\right. {π(z′)=p(x′)∣det(Jf​)∣p(x′)=π(z′)∣∣​det(Jf−1​)∣∣​​ 从这个公式引入了Flow_based_model 的基本思路：设计一个神经网络，将分布 xxx 映射到分布 zzz ，具体来说，流模型选择 q(z)q(z)q(z) 为高斯分布，q(x∣z)q(x|z)q(x∣z) 为狄拉克分布 δ(x−g(z)\\delta(x-g(z)δ(x−g(z) ，其中ggg 是可逆的： x=g(z)⇔z=f(x)x=g(z) \\Leftrightarrow z=f(x) x=g(z)⇔z=f(x) 要从理论上实现可逆，需要 xxx 和 zzz 的维数相同，将 zzz 的分布代入，则有： q(z)=1(2π)D/2exp⁡(−12∥z∥2)q(z)=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|z\\|^{2}\\right) q(z)=(2π)D/21​exp(−21​∥z∥2) q(x)=1(2π)D/2exp⁡(−12∥f(x)∥2)∣det⁡[∂f∂x]∣(2)q(\\boldsymbol{x})=\\frac{1}{(2 \\pi)^{D / 2}} \\exp \\left(-\\frac{1}{2}\\|\\boldsymbol{f}(\\boldsymbol{x})\\|^{2}\\right)\\left|\\operatorname{det}\\left[\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}\\right]\\right|\\tag{2} q(x)=(2π)D/21​exp(−21​∥f(x)∥2)∣∣∣∣​det[∂x∂f​]∣∣∣∣​(2) 公式(2)(2)(2)对 fff 提出了三个基本要求： 可逆，且逆函数容易计算。 对应的雅可比行列式方便计算 拟合能力强 这样的话，就有 log⁡q(x)=−D2log⁡(2π)−12∥f(x)∥2+log⁡∣det⁡[∂f∂x]∣\\log q(x)=-\\frac{D}{2} \\log (2 \\pi)-\\frac{1}{2}\\|f(x)\\|^{2}+\\log \\left|\\operatorname{det}\\left[\\frac{\\partial f}{\\partial x}\\right]\\right| logq(x)=−2D​log(2π)−21​∥f(x)∥2+log∣∣∣∣​det[∂x∂f​]∣∣∣∣​ 这个优化目标是可计算的，并且因为 fff 可逆，那么我们在zzz 中取样，就可以生成相应的 xxx x=f−1(z)=g(z)x=f^{-1}(z)=g(z) x=f−1(z)=g(z) 为了满足这三个条件，NICE和REAL NVP、GLOW都采用了模块化思想，将 fff 设计成一组函数的复合，其中每个函数都满足要求一和要求二，经过复合之后函数也容易满足要求三。 f=fL∘…∘f2∘f1f=f_{L} \\circ \\ldots \\circ f_{2} \\circ f_{1} f=fL​∘…∘f2​∘f1​ 相对而言，雅可比行列式的计算要比函数求逆更加复杂，考虑第二个要求，我们知道三角行列式最容易计算，所以我们要想办法让变换 fff 的雅可比矩阵为三角阵。NICE的做法是：将 DDD 的 xxx 分为两部分 x1,x2x_1,x_2x1​,x2​，然后取下述变换： h1=x1h2=x2+m(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{x}_{2}+\\boldsymbol{m}\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=x2​+m(x1​)​ 其中 mmm 为任意函数，这个变换称为“加性耦合层” ，这个变换的雅可比矩阵 [∂h∂x][\\frac{\\partial h}{\\partial x}][∂x∂h​] 是一个三角阵，且对角线元素全部为1，用分块矩阵表示为： [∂h∂x]=(IdO∂m∂x1ID−d)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathrm{I}_{d}} &amp; {\\mathrm{O}} \\\\ \\frac{\\partial m}{\\partial x_1} &amp; I_{D-d} \\end{array}\\right) [∂x∂h​]=(Id​∂x1​∂m​​OID−d​​) 同时这个变换也是可逆的，其逆变换为 x1=xhx2=h2−m(h1)\\begin{array}{l} {\\boldsymbol{x}_{1}=\\boldsymbol{x}_{h}} \\\\ {\\boldsymbol{x}_{2}=\\boldsymbol{h}_{2}-\\boldsymbol{m}\\left(\\boldsymbol{h}_{1}\\right)} \\end{array} x1​=xh​x2​=h2​−m(h1​)​ 满足了要求一和要求二，同时这个雅可比行列式的值为1，行列式的值的物理含义是体积，所以这个变换暗含了变换前后的体积不变性。我们注意到：该变换的第一部分是平凡的（恒等变换），因此需要对调I1和I2两组维度，再输入加和耦合层，并将这个过程重复若干次， 以达到信息充分混合的目的，如图： 因为该变换需要满足 zzz 和 xxx 的维度相同，这会产生很严重的唯独浪费问题，NICE在最后一层里引入了一个尺度变换对维度进行缩放： z=s⊗h(n)z=s \\otimes h^{(n)} z=s⊗h(n) 其中s=(s1,s2,...,sD)s=(s_1,s_2,...,s_D)s=(s1​,s2​,...,sD​)也是一个要优化的参数向量，这个 sss 向量能够识别每个维度的重要程度， sss 越小，这个维度越不重要，起到压缩流形的作用。这个尺度变换层的雅可比行列式就不是一了，而是： [∂z∂h(n)]=diag⁡(s)\\left[\\frac{\\partial z}{\\partial \\boldsymbol{h}^{(n)}}\\right]=\\operatorname{diag}(\\boldsymbol{s}) [∂h(n)∂z​]=diag(s) 他的行列式的值为 ∏isi\\prod_{i} s_{i}∏i​si​,于是最后的对数似然为： log⁡q(x)∼−12∥s⊗f(x)∥2+∑ilog⁡si\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2}\\|\\boldsymbol{s} \\otimes \\boldsymbol{f}(\\boldsymbol{x})\\|^{2}+\\sum_{i} \\log \\boldsymbol{s}_{i} logq(x)∼−21​∥s⊗f(x)∥2+i∑​logsi​ 这个尺度变换实际上是将先验分布 q(z)q(z)q(z) 的方差也作为训练参数，方差越小，说明这个维度的“弥散”越小，若方差为0，这一维的特征就恒为均值，于是流行减小一维。 我们写出带方差的正态分布： q(z)=1(2π)D/2∏i=1Dσiexp⁡(−12∑i=1Dzi2σi2)q(z)=\\frac{1}{(2 \\pi)^{D / 2} \\prod_{i=1}^{D} \\sigma_{i}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{z_{i}^{2}}{\\sigma_{i}^{2}}\\right) q(z)=(2π)D/2∏i=1D​σi​1​exp(−21​i=1∑D​σi2​zi2​​) 将 z=f(x)z=f(x)z=f(x) 代入，并取对数，类似得： log⁡q(x)∼−12∑i=1Dfi2(x)σi2−∑i=1Dlog⁡σi\\log q(\\boldsymbol{x}) \\sim-\\frac{1}{2} \\sum_{i=1}^{D} \\frac{\\boldsymbol{f}_{i}^{2}(\\boldsymbol{x})}{\\boldsymbol{\\sigma}_{i}^{2}}-\\sum_{i=1}^{D} \\log \\boldsymbol{\\sigma}_{i} logq(x)∼−21​i=1∑D​σi2​fi2​(x)​−i=1∑D​logσi​ 与之前那个公式对比，就有 si=1/σis_i=1/\\sigma_isi​=1/σi​ ，所以尺度变换层等价于将先验分布的方差作为训练参数，若方差足够小，则维度减一，暗含了降维的可能。 REALNVP NICE构思巧妙，但在实验部分只是采取了简单的加性耦合层和将全连接层进行简单的堆叠，并没有使用卷积。REALNVP一般化了耦合层，并在耦合模型中引入了卷积层，使得模型可以更好地处理图像问题。论文里还引入了一个多尺度结构来处理维度浪费问题。 将加性耦合层换成仿射耦合层： h1=x1h2=s(x1)⊗x2+t(x1)(x1)\\begin{array}{l} {\\boldsymbol{h}_{1}=\\boldsymbol{x}_{1}} \\\\ {\\boldsymbol{h}_{2}=\\boldsymbol{s}\\left(\\boldsymbol{x}_{1}\\right) \\otimes \\boldsymbol{x}_{2}+t\\left(\\boldsymbol{x}_{1}\\right)\\left(\\boldsymbol{x}_{1}\\right)} \\end{array} h1​=x1​h2​=s(x1​)⊗x2​+t(x1​)(x1​)​ 仿射耦合层的雅可比行列式仍然是一个对角阵 [∂h∂x]=(IdO[∂m∂x1]s)\\left[\\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{x}}\\right]=\\left(\\begin{array}{cc} {\\mathbb{I}_{d}} &amp; {\\mathbb{O}} \\\\ {\\left[\\frac{\\partial m}{\\partial x_{1}}\\right]} &amp; {s} \\end{array}\\right) [∂x∂h​]=(Id​[∂x1​∂m​]​Os​) 雅可比行列式的值不再是1，没有保持变换前后的体积不变。 在NICE中，通过交错的方式来混合信息流(直接反转原来的向量)，在REALNVP中发现：随机打乱维度可以使信息混合的更加充分。 引入卷积层：使用卷积的条件是具有局部相关性，因此指定向量的打乱和重排都是在channel维度上进行，在height和width维度上进行卷积。对通道的分割论文里还提出棋盘式分割的策略，但较为复杂，对模型的提升也不大，因此在GLOW中被舍弃了。 一般的图像通道数只有三层，MNIST等灰度图只有一层，因此REALNVP引入了squeeze操作来增加通道数。 其思想很简单：直接 reshape，但 reshape 时局部地进行。具体来说，假设原来图像为 h×w×c 大小，前两个轴是空间维度，然后沿着空间维度分为一个个 2×2×c 的块（ 2 可以自定义），然后将每个块直接 reshape 为 1×1×4c，最后变成了 h/2×w/2×4c。 REALNVP中还引入了一个多尺度结构： 最终的输出 z1,z3,z5z_1,z_3,z_5z1​,z3​,z5​ 怎么取？ p(z1,z3,z5)=p(z1∣z3,z5)p(z3∣z5)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{3}, z_{5}\\right) p\\left(z_{3} | z_{5}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z3​,z5​)p(z3​∣z5​)p(z5​) 由于 z3,z5z_3,z_5z3​,z5​ 是由 z2z_2z2​ 完全决定的，z5z_5z5​ 也是由 z4z_4z4​ 完全决定的，因此条件部分可以改为： p(z1,z3,z5)=p(z1∣z2)p(z3∣z4)p(z5)p\\left(z_{1}, z_{3}, z_{5}\\right)=p\\left(z_{1} | z_{2}\\right) p\\left(z_{3} | z_{4}\\right) p\\left(z_{5}\\right) p(z1​,z3​,z5​)=p(z1​∣z2​)p(z3​∣z4​)p(z5​) RealNVP 和 Glow 假设右端三个概率分布都是正态分布，类似VAE， p(z1∣z2)p(z_1|z_2)p(z1​∣z2​) 的均值方差由 z2z_2z2​ 算出来，p(z3∣z4)p(z_3|z_4)p(z3​∣z4​) 的均值方差由 z4z_4z4​ 算出来，p(z5)p(z_5)p(z5​) 的均值方差直接学习出来。这相当于做了变量代换： z^1=z1−μ(z2)σ(z2),z^3=z3−μ(z4)σ(z4),z^5=z5−μσ\\hat{z}_{1}=\\frac{z_{1}-\\mu\\left(z_{2}\\right)}{\\sigma\\left(z_{2}\\right)}, \\quad \\hat{z}_{3}=\\frac{z_{3}-\\mu\\left(z_{4}\\right)}{\\sigma\\left(z_{4}\\right)}, \\quad \\hat{z}_{5}=\\frac{z_{5}-\\mu}{\\sigma} z^1​=σ(z2​)z1​−μ(z2​)​,z^3​=σ(z4​)z3​−μ(z4​)​,z^5​=σz5​−μ​ 然后认为 [z^1,z^3,z^5][\\hat{z}_1,\\hat{z}_3,\\hat{z}_5][z^1​,z^3​,z^5​]服从标准正态分布。类似NICE，这三个变换会导致一个非1的雅可比行列式，也就是往loss中加入 Σi=1Dlog⁡σi\\Sigma_{i=1}^{D} \\log \\sigma_{i}Σi=1D​logσi​ 这一项。 多尺度结构相当于抛弃了 p(z)p(z)p(z) 是标准正态分布的直接假设，而采用了一个组合式的条件分布，这样尽管输入输出的总维度依然一样，但是不同层次的输出地位已经不对等了，模型可以通过控制每个条件分布的方差来抑制维度浪费问题（极端情况下，方差为 0，那么高斯分布坍缩为狄拉克分布，维度就降低 1），条件分布相比于独立分布具有更大的灵活性。而如果单纯从 loss 的角度看，多尺度结构为模型提供了一个强有力的正则项。 GLOW 效果好的令人惊叹的生成模型： 改变图像属性 采样展示 潜在空间的插值 总体来说，GLOW引入1*1可逆卷积来代替通道维度的打乱和重排操作，并对 REALNVP 的原始模型做了简化和规范。 向量之间的元素置换操作可以用简单的行变换矩阵来操作： (badc)=(0100100000010010)(abcd)\\left(\\begin{array}{l} {b} \\\\ {a} \\\\ {d} \\\\ {c} \\end{array}\\right)=\\left(\\begin{array}{llll} {0} &amp; {1} &amp; {0} &amp; {0} \\\\ {1} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {1} \\\\ {0} &amp; {0} &amp; {1} &amp; {0} \\end{array}\\right)\\left(\\begin{array}{l} {a} \\\\ {b} \\\\ {c} \\\\ {d} \\end{array}\\right) ⎝⎜⎜⎛​badc​⎠⎟⎟⎞​=⎝⎜⎜⎛​0100​1000​0001​0010​⎠⎟⎟⎞​⎝⎜⎜⎛​abcd​⎠⎟⎟⎞​ GLOW中用一个更一般的矩阵 WWW 来代替这个置换矩阵 h=xWh=xW h=xW 这个变换的雅可比矩阵就是det(W)det(W)det(W)，因此需要将 −log∣det(W)∣-log|det(W)|−log∣det(W)∣ 加入到loss中，WWW 的初始选择要求可逆，不引入loss，因此选为随即正交阵。 这个变换引入了 det(W)det(W)det(W) 的计算问题，GLOW中逆用LU分解克服了这个问题，若 W=PLUW=PLUW=PLU (其中P是一个置换矩阵),则 log⁡∣det⁡W∣=∑log⁡∣diag⁡(U)∣\\log |\\operatorname{det} W|=\\sum \\log |\\operatorname{diag}(U)| log∣detW∣=∑log∣diag(U)∣ 这就是GLOW中给出的技巧：先随机生成一个正交矩阵，然后做 LULULU 分解，得到 P,L,UP,L,UP,L,U，固定 P，也固定 U 的对角线的正负号，然后约束 L 为对角线全 1 的下三角阵，U 为上三角阵，优化训练 L,U 的其余参数。 也可以理解为：在NICE和REALNVP中都是通过交换、打乱、重排等操作混合各个通道的信息，但通道信息的混合可直接通过1*1卷积实现。 整个GLOW模型如下： 对比 比较反转、打乱和1*1逆卷积的loss： 缺点 模型庞大，参数量极大，NICE模型在MNIST数据集上的训练参数就大概有两千万个。 再贴两个Glow模型在Gayhub Github上的issue感受下： 256*256的高清人脸生成，用一块GPU训练的话，大概要一年…… 一图对比GAN，VAE和FLOW 参考文献 Variational Inference: A Unified Framework of Generative Models and Some Revelations Tutorial on Variational Autoencoders 用变分推断统一理解生成模型（VAE、GAN、AAE、ALI） NICE: Non-linear Independent Components Estimation NOTE_FLOW Glow: Generative Flow with Invertible 1×1 Convolutions 细水长flow之NICE：流模型的基本概念与实现 RealNVP与Glow：流模型的传承与升华 Density estimation using Real NVP","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"gan","slug":"gan","permalink":"/tags/gan/"},{"name":"vae","slug":"vae","permalink":"/tags/vae/"},{"name":"flow","slug":"flow","permalink":"/tags/flow/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"gan,vae和flow","slug":"FFJORD","date":"2020-02-15T13:19:00.000Z","updated":"2020-07-02T13:39:09.290Z","comments":true,"path":"2020/02/15/FFJORD/","link":"","permalink":"/2020/02/15/FFJORD/","excerpt":"","text":"介绍 流模型中使用变量代换定理","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"gan","slug":"gan","permalink":"/tags/gan/"},{"name":"vae","slug":"vae","permalink":"/tags/vae/"},{"name":"flow","slug":"flow","permalink":"/tags/flow/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"shell脚本","slug":"shell脚本","date":"2020-02-10T05:32:00.000Z","updated":"2020-03-15T04:34:47.342Z","comments":true,"path":"2020/02/10/shell脚本/","link":"","permalink":"/2020/02/10/shell脚本/","excerpt":"","text":"# 指定解释器 #！/bin/bash # 向窗口输出文本 echo \"Hello world!\" printf \"Hello world!\" # for循环示例,使用变量要加$符号 for file in `ls /etc` do echo \"${file}\" done # 双引号和单引号 # 双引号中可以有变量，单引号中的变量是无效的 # if-else语句 if condition1 then command1 elif condition2 then commed2 else command3 fi","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"shell","slug":"shell","permalink":"/tags/shell/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"emacs基本操作","slug":"Emacs笔记","date":"2020-02-10T03:16:00.000Z","updated":"2020-03-15T04:18:17.714Z","comments":true,"path":"2020/02/10/Emacs笔记/","link":"","permalink":"/2020/02/10/Emacs笔记/","excerpt":"","text":"Emacs基本操作 C = Ctrl, M = Alt 光标移动 C-v 向下翻页 M-v 向上翻页 C-b 向左(back) C-f 向右(forward) C-n 向下(next) C-p 向上(previous) M-b 上一个单词 M-f 下一个单词 C-a 行首 C-e 行尾 M-a 句首 M-e 句尾 M-&lt; 文件头 M-&gt; 文件尾 M-g g 跳到某一行 选择区域 C-@ 标记 删除剪切复制粘贴 C-d 向后删除(delele) C-k 删掉光标后至行尾 M-w 复制区域 C-w 剪切/删除区域 C-y 粘贴 M-y 滚动选择粘贴内容 查找替换 C-s 向前查找 C-s C-r 向后查找 M-% 替换 文件操作 C-x C-f 打开文件(find) C-x C-s 保存文件(save) C-x C-w 另存为(write) C-k 关闭文件 窗口操作 C-x b 切换文件 C-x 1 关闭其它窗口 C-x 2/C-x 3 打开其它窗口 C-x o 跳到另一个窗口(other) 其它 C-/ 撤销 M-$ 拼写检查 M-x 输入命令 C-g 取消命令","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"emacs","slug":"emacs","permalink":"/tags/emacs/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"科学上网","slug":"科学上网","date":"2020-02-09T03:19:00.000Z","updated":"2020-03-15T04:38:26.058Z","comments":true,"path":"2020/02/09/科学上网/","link":"","permalink":"/2020/02/09/科学上网/","excerpt":"","text":"插件篇 如果只是想访问谷歌服务，可以直接在谷歌浏览器里安装谷歌上网助手插件。 这个插件只能访问谷歌旗下网站，YouTube等不能访问。 还有一个不错的谷歌浏览器插件VeePN，也能实现科学上网 机场篇 免费机场 付费机场 付费的速度会比免费机场快，而且解锁奈非等 VPS篇 喜欢自己折腾的可以看，需要有VPS，以我的为例: ssh进行端口动态转发 # 本地1080端口和服务器动态转发，可添加参数-v打印一些数据流 $ ssh -D 1080 root@66.152.179.100 # 查看端口是否打开 $ netstat -nat ssh -D容易把VPS IP弄没……可以临时使用，长期使用换v2ray V2ray 在服务器端安装V2ray: 可以用一键脚本安装： bash &lt;(curl -s -L https://git.io/v2ray.sh) 也可以用Docker安装： 1.配置Docker环境： $ apt-get update $ apt-get -y install ca-certificates wget $ curl -sSL https://get.docker.com/ | sh 2.配置Portainer管理界面： $ docker run -d -p 9000:9000 --label owner=portainer \\ --restart=always --name=ui \\ --label owner=portainer \\ -v /var/run/docker.sock:/var/run/docker.sock \\ lihaixin/portainer -l owner=portainer 3.配置加速服务： $ wget https://github.com/chiakge/Linux-NetSpeed/raw/master/tcp.sh $ chmod +x tcp.sh $ ./tcp.sh 4.登入Web管理界面&lt;ip&gt;:9000 创建V2ray模板并启用 进入容器logs日志查看VMESS，复制下来，接下来会用 客户端使用V2ray： 下载对应客户端: Windows:·v2rayN Mac:v2rayU Linux:v2rayL 上面复制了vmess，然后点击v2ray客户端——&gt;从剪贴板批量导入URL 配置浏览器代理设置 在浏览器中安装Proxy SwitchyOmega扩展，新建PAC情景模式，在PAC网址中粘贴https://raw.githubusercontent.com/breakwa11/gfw_whitelist/master/proxy.pac 立即更新，应用更改。 或者新建情景模式——&gt;代理服务器——&gt;协议选择Socks5，代理服务器填127.0.0.1，代理端口为ssr或者V2ray的本地代理端口，一般默认为1080 浏览器切换到代理模式后，正常访问外网。 注意事项 谷歌上网助手和我的V2ray配置文件都选用了本地1080端口进行监听，在浏览器中同时开启Pro SwitchyOmega插件和谷歌上网助手插件可能会发生端口冲突。 解决方案:两个插件不要同时开，或者修改v2ray配置文件的本地端口。","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"},{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"},{"name":"科学上网","slug":"科学上网","permalink":"/tags/科学上网/"},{"name":"翻墙","slug":"翻墙","permalink":"/tags/翻墙/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用tensorboard和visdom","slug":"服务器使用tensorboard和visdom","date":"2020-01-28T03:27:21.351Z","updated":"2020-03-15T04:36:53.050Z","comments":true,"path":"2020/01/28/服务器使用tensorboard和visdom/","link":"","permalink":"/2020/01/28/服务器使用tensorboard和visdom/","excerpt":"","text":"服务器使用tensorboard和visdom 以tensorboard为例： 创建容器时开放6006端口 # 运行容器时将服务器docker容器的6006端口暴漏到自己主机ip下的16006端口(可自己指定) $ docker run -p &lt;ip&gt;:16006:6006 -it -v /data:/workspace/data --runtime=nvidia --net=host --name=temp /bin/bash 或者 # 在连接ssh时，将docker容器中的6006端口重新定向到自己机器上 $ ssh -p 1001 -L 16006:&lt;ip&gt;:6006 root@10.7.60.40","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"},{"name":"实验室","slug":"实验室","permalink":"/tags/实验室/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"服务器使用教程","slug":"服务器使用说明","date":"2020-01-24T03:38:00.000Z","updated":"2020-08-17T12:42:43.202Z","comments":true,"path":"2020/01/24/服务器使用说明/","link":"","permalink":"/2020/01/24/服务器使用说明/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Please enter the password to read the blog. Incorrect Password! No content to display! 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","categories":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"/tags/服务器/"}],"keywords":[{"name":"实验室","slug":"实验室","permalink":"/categories/实验室/"}]},{"title":"docker基本使用","slug":"docker使用","date":"2020-01-19T12:48:00.000Z","updated":"2020-03-15T04:16:56.482Z","comments":true,"path":"2020/01/19/docker使用/","link":"","permalink":"/2020/01/19/docker使用/","excerpt":"","text":"Docker基本使用 docker和nvidia-docker 安装 略 安装后查看安装信息和测试docker是否安装正确 $ sudo docker --version $ sudo docker run hello-world # 验证nvidia-docker $ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi docker hub 链接:dockerhub 在dockerhub上搜索自己需要的镜像 选择自己要用的镜像，点进去，点Tags会出现不同的版本 复制命令，在终端中执行，就可以将镜像下载到本地 docker基本命令 # 查看主机下有多少镜像 $ sudo docker images # 删除镜像 $ sudo docker rmi &lt;image_id&gt; # 查看主机下的容器,不加-a表示查看运行中的容器 $ sudo docker ps -a # 启动、重启和停止容器 $ sudo docker start/restart/stop &lt;container_id&gt; # 进入容器 $ sudo docker attach &lt;container_id&gt; # 删除容器(需要先停止容器) $ sudo docker rm &lt;container_id&gt; ctrl+d退出并停止容器，ctrl+p+q退出但不停止容器 根据镜像新建并启动容器 $ sudo docker run -it [options] &lt;image_id&gt; bash 说明：-it为为容器分配一个输入终端，以交互式模式运行,bash为调用镜像里的bash 可选选项： --name 为容器指定名字 -p 端口映射 -v 给容器挂载存储卷 --net 指定容器网络 --runtime=nvidia 可调用gpu 例如： $ sudo docker run -it -p 8022:22 --name=tensorflow --runtime=nvidia -v /home/yu/code:/home --net=host ufoym/deepo bash 删除所有镜像和容器 $ sudo docker rmi `sudo docker images -q` $ sudo docker rm `sudo docker ps -a -q` 导出容器和保存加载镜像 容器导出为镜像 # 容器导出为镜像 $ docker commit &lt;continer_name&gt; &lt;image_name&gt; # 镜像导出为文件 $ docker save image_name&gt; /dir/name.tar # 文件导入为镜像 $ docker load &lt; /dir/filename docker和宿主机的文件拷贝 $ docker cp &lt;container_name&gt;:/dir/filename /hostdir/ $ docker cp /hostdir/filename &lt;container_name&gt;:/dir 不管容器有没有启动，拷贝命令都会生效","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"docker","slug":"docker","permalink":"/tags/docker/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"OpenPAI安装记录","slug":"OpenPai安装记录","date":"2020-01-16T02:48:00.000Z","updated":"2020-03-15T04:33:53.506Z","comments":true,"path":"2020/01/16/OpenPai安装记录/","link":"","permalink":"/2020/01/16/OpenPai安装记录/","excerpt":"","text":"OpenPAI安装记录 环境准备 一台master主机和多台worker主机，一台维护机 所有节点不要安装CUDA驱动，具有统一的登录账户和密码 开启ssh功能和ntp功能(互相访问，时间同步) 部署过程 安装docker-ce $ sudo apt-get -y install docker.io $ sudo docker pull docker.io/openpai/dev-box:v0.14.0 运行dev-box $ sudo docker run -itd \\ -e COLUMNS=$COLUMNS -e LINES=$LINES -e TERM=$TERM \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /pathConfiguration:/cluster-configuration \\ -v /hadoop-binary:/hadoop-binary \\ --pid=host \\ --privileged=true \\ --net=host \\ --name=dev-box \\ docker.io/openpai/dev-box:v0.14.0 登录dev-box $ sudo docker exec -it dev-box /bin/bash $ cd /pai/deployment/quick-start/ 修改配置信息 $ cp quick-start-example.yaml quick-start.yaml $ vim quick-start.yaml 修改内容： machines: - &lt;ip-of-master&gt; - &lt;ip-of-worker1&gt; - &lt;ip-of-worder2&gt; ssh-username: &lt;username&gt; ssh-password: &lt;password&gt; 生成OepnPai配置文件 $ cd /pai $ python paictl.py config generate -i /pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f $ cd ~/pai-config/ 修改kubernetes-configuration.yaml 将docker-registry替换为国内镜像库 docker-registry: docker.io/mirrorgooglecontainers 修改layout.yaml 修改自己机器的配置信息 machine-sku: GENERIC: mem: 256G gpu: type: TITAN V count: 1 cpu: vcore: 4 os: ubuntu16.04 Worker1: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 Worker2: mem: 256G gpu: type: GeForce RTX 2080Ti count: 4 cpu: vcore: 4 os: ubuntu16.04 修改services-configuration.yaml 解除common和data-path两个字段的注释，将data-path赋值到真实位置，作为服务数据存储路径 cluster: common: # cluster-id: pai-example # # # HDFS, zookeeper data path on your cluster machine. data-path: \"/data\" tag字段修改为真实版本 v014.0 可修改cluster-id,后面会用到 修改rest-server下的用户名和密码，作为登录平台的账户密码 指定显卡驱动版本，不指定的话默认安装384.11，这个驱动是不支持图灵核心显卡的，安装到后面会出现’nvidia-drm’ not found 错误，驱动版本只能从注释里的版本选择 drivers: set-nvidia-runtime: false # You can set drivers version here. If this value is miss, default value will be 384.111 # Current supported version list # 384.111 # 390.25 # 410.73 version: \"410.73\" 部署Kubernetes http::9090查看进度 $ cd /pai $ python paictl.py cluster k8s-bootup -p ~/pai-config 更改配置文件到kubernetes $ cd /pai python paictl.py config push -p ~/pai-config/ -c ~/.kube/config 若报错，卸载openpai组件和ks组件，检查之前的配置文件，重新安装 $ python paictl.py service [delete|start|stop] -c ~/.kube/config [-n name] # 卸载openpai组件 $ python paictl.py service delete -c ~/.kube/config # 卸载k8s组件 $ python paictl.py cluster k8s-clean -p ~/pai-config/ 启动Openpai $ python paictl.py service start -c ~/.kube/config 界面 http://&lt;master-ip&gt;:9090 http://&lt;master-ip&gt;:80 Reference https://github.com/kangapp/openPAI https://github.com/microsoft/pai https://zhuanlan.zhihu.com/p/64061072","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"openpai","slug":"openpai","permalink":"/tags/openpai/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"NVIDIA,CUDA,CUDNN,Anaconda","slug":"Ubuntu命令记录","date":"2020-01-11T12:32:00.000Z","updated":"2020-03-15T04:35:55.578Z","comments":true,"path":"2020/01/11/Ubuntu命令记录/","link":"","permalink":"/2020/01/11/Ubuntu命令记录/","excerpt":"","text":"NVIDIA,CUDA,CUDNN ppa安装NVIDIA驱动 $ sudo add-apt-repository ppa:graphics-drivers/ppa $ sudo apt-get update $ ubuntu-drivers devices $ sudo apt-get install nvidia-driver-xxx 自动安装NVIDIA驱动 # 卸载残余驱动 sudo apt-get --purge remove \"*nvidia*\" # 查看推荐驱动版本 ubuntu-drivers devices # 自动安装 sudo ubuntu-drivers autoinstall .deb安装CUDA 下载deb文件 $ sudo dpkg -i cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb $ sudo apt-get update $ sudo apt-get install cuda 安装CUDNN 下载符合自己cuda版本的cudnn 安装cudnn 安装过程实际是将cudnn的头文件复制到CUDA的头文件目录里 $ sudo cp cuda/include/* /usr/local/cuda-10.0/include/ $ sudo cp cuda/lib64/* /usr/local/cuda-10.0/lib64/ # 添加可执行权限 $ sudo chmod +x /usr/local/cuda-10.0/include/cudnn.h $ sudo chmod +x /usr/local/cuda-10.0/lib64/libcudnn* 检验 $ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 指定运行程序使用的GPU 在程序中添加 import os os.environ['CUDA_VISIBLE_DEVICES]='0' 或者在终端中 $ CUDA_VISIBLE_DEVICES=0 python main.py 命令行安装Anaconda $ wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh $ bash Anaconda3-5.0.1-Linux-x86_64.sh # 添加环境变量，可选 $ echo 'export PATH=\"~/anaconda3/bin:$PATH\"' &gt;&gt; ~/.bashrc $ source .bashrc","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}],"tags":[{"name":"nvidia","slug":"nvidia","permalink":"/tags/nvidia/"},{"name":"cuda","slug":"cuda","permalink":"/tags/cuda/"},{"name":"cudnn","slug":"cudnn","permalink":"/tags/cudnn/"},{"name":"anaconda","slug":"anaconda","permalink":"/tags/anaconda/"}],"keywords":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"/categories/瞎折腾/"}]},{"title":"安利","slug":"安利区","date":"2020-01-11T02:48:00.000Z","updated":"2020-03-15T04:36:19.214Z","comments":true,"path":"2020/01/11/安利区/","link":"","permalink":"/2020/01/11/安利区/","excerpt":"","text":"一些软件 softdownloader: 一款布局清爽的下载工具 Fences: 桌面管理工具 copytranslator: 一款翻译软件 天若OCR文字识别 Snipaste: 一款简洁的截图和贴图软件 Mathpix: 每个月50次识别次数，快速识别图片中的公式，转化为 LaTeX\\LaTeXLATE​X 格式 Windows 的内置 Liunx 系统，支持 Linux 命令 TexStudio + Texlive 编辑LaTeX\\LaTeXLATE​X公式 Axmath: 个人感觉优于mathtype, Office中的公式编辑器 亿寻:百度云破解限速 Google浏览器插件 momentum: 浏览器壁纸标签页 LastPass: 密码记录插件 SwitchyOmega: 浏览器代理插件 OneTab: 标签页管理 一键管理扩展: 管理所有插件 AdblockPlus: 强烈安利，拦截广告 Read Viewer: 网页阅读模式 划词翻译 Tampermonkey:传说中的油猴 又在Github上找到个操作系统软件集合: Windows linux Mac","categories":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}],"tags":[{"name":"安利","slug":"安利","permalink":"/tags/安利/"}],"keywords":[{"name":"安利","slug":"安利","permalink":"/categories/安利/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-01-11T02:16:02.000Z","updated":"2020-03-15T04:19:40.002Z","comments":true,"path":"2020/01/11/hello-world/","link":"","permalink":"/2020/01/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","categories":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"其它","slug":"其它","permalink":"/categories/其它/"}]}]}